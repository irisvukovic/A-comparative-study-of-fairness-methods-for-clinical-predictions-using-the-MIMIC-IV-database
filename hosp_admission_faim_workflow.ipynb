{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mbd7qjEMqTnm"
   },
   "source": [
    "#ShapleyVIC edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4WZwE82cjTd",
    "outputId": "d8292d4a-70ab-4e8a-8828-70332cba70af"
   },
   "outputs": [],
   "source": [
    "import ShapleyVIC, os\n",
    "print(ShapleyVIC.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnxTqBKSmYuW",
    "outputId": "a264797c-7917-44d4-92cf-5b707209926c"
   },
   "outputs": [],
   "source": [
    "# see folders in ShapleyVIC\n",
    "!ls /usr/local/lib/python3.12/dist-packages/ShapleyVIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dT1D9C1hojk3"
   },
   "outputs": [],
   "source": [
    "!grep -r \"sample_w\" /usr/local/lib/python3.12/dist-packages/ShapleyVIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDhTpl42hebe",
    "outputId": "3747b2d9-4030-4d36-a535-e6f41b1bb6a5"
   },
   "outputs": [],
   "source": [
    "file_path = \"/usr/local/lib/python3.12/dist-packages/ShapleyVIC/model.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# --- First replacement: add sample_w to __init__ ---\n",
    "text = text.replace(\n",
    "    'class models:\\n    def __init__(self, x, y, output_dir, outcome_type=\"binary\", ordinal_link=\"logit\", criterion=\"loss\", epsilon = 0.05, x_names_cat=None, save_data=True):',\n",
    "    'class models:\\n    def __init__(self, x, y, output_dir, outcome_type=\"binary\", ordinal_link=\"logit\",\\n                 criterion=\"loss\", epsilon=0.05, x_names_cat=None, save_data=True,\\n                 sample_w=None):'\n",
    ")\n",
    "\n",
    "# --- Second replacement: add sample_w logic for binary/continuous ---\n",
    "text = text.replace(\n",
    "    '''if outcome_type == \"binary\":\n",
    "            x_with_constant = sm.add_constant(x_dm)\n",
    "            m0 = sm.GLM(y, x_with_constant, family=sm.families.Binomial())\n",
    "            m = m0.fit()\n",
    "        elif outcome_type == \"continuous\":\n",
    "            x_with_constant = sm.add_constant(x_dm)\n",
    "            m0 = sm.OLS(y, x_with_constant)\n",
    "            m = m0.fit()''',\n",
    "    '''if outcome_type == \"binary\":\n",
    "            x_with_constant = sm.add_constant(x_dm)\n",
    "            if sample_w is not None:\n",
    "                m0 = sm.GLM(y, x_with_constant, family=sm.families.Binomial(), freq_weights=sample_w)\n",
    "            else:\n",
    "                m0 = sm.GLM(y, x_with_constant, family=sm.families.Binomial())\n",
    "            m = m0.fit()\n",
    "        elif outcome_type == \"continuous\":\n",
    "            x_with_constant = sm.add_constant(x_dm)\n",
    "            if sample_w is not None:\n",
    "                m0 = sm.WLS(y, x_with_constant, weights=sample_w)\n",
    "            else:\n",
    "                m0 = sm.OLS(y, x_with_constant)\n",
    "            m = m0.fit()'''\n",
    ")\n",
    "\n",
    "# Save back to file\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(\"File updated successfully with sample_w support!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPB5QJ7u02TP",
    "outputId": "ba6be081-24d8-4a5c-8e21-8aaa66371bec"
   },
   "outputs": [],
   "source": [
    "!grep -r \"sample_w\" /usr/local/lib/python3.12/dist-packages/ShapleyVIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeVk96X1ofrn"
   },
   "source": [
    "#pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xan63BhKuAQe",
    "outputId": "ce42ca3b-b281-4caf-bfcf-5d6ee2fa7ea4"
   },
   "outputs": [],
   "source": [
    "! pip install git+\"https://github.com/nliulab/ShapleyVIC#egg=ShapleyVIC&subdirectory=python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-jlMicVtrO0",
    "outputId": "5c278541-bed3-4d72-8c52-d97970603bd3"
   },
   "outputs": [],
   "source": [
    "! pip install 'aif360[inFairness]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYySnHLUu2Ee",
    "outputId": "1879766f-b860-4a12-d17c-9e1bafd8f7d7"
   },
   "outputs": [],
   "source": [
    "! pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pbV03SD0ikx",
    "outputId": "3e7c1e39-c22e-4e35-b64a-8555af6d2ca8"
   },
   "outputs": [],
   "source": [
    "! pip install patchworklib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewa00AjZCGeJ"
   },
   "source": [
    "#FAIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "gOhvSx6Ppjvv",
    "outputId": "973f733d-70c8-4535-ddf7-009fe8a60f37"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import collections\n",
    "from pandas.io import gbq\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "import plotnine as pn\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy import integrate\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "import ShapleyVIC\n",
    "from ShapleyVIC import model, _util\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.postprocessing.eq_odds_postprocessing import EqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import (\n",
    "    CalibratedEqOddsPostprocessing,\n",
    ")\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import (\n",
    "    RejectOptionClassification,\n",
    ")\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fairlearn.metrics import (\n",
    "    equalized_odds_difference,\n",
    "    demographic_parity_difference,\n",
    "    true_negative_rate,\n",
    "    selection_rate,\n",
    "    MetricFrame,\n",
    ")\n",
    "import warnings\n",
    "import patchworklib as pw\n",
    "import inspect\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import shap\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5prZZ2x9pWAP"
   },
   "source": [
    "##functions and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RRNOvD_pYYR"
   },
   "outputs": [],
   "source": [
    "def rgb01_hex(col):\n",
    "    col_hex = [round(i * 255) for i in col]\n",
    "    col_hex = \"#%02x%02x%02x\" % tuple(col_hex)\n",
    "    return col_hex\n",
    "\n",
    "\n",
    "def compute_area(fairness_metrics):\n",
    "    n_metric = len(fairness_metrics)\n",
    "    tmp = fairness_metrics.values.flatten().tolist()\n",
    "    tmp_1 = tmp[1:] + tmp[:1]\n",
    "\n",
    "    if n_metric > 2:\n",
    "        theta_c = 2 * np.pi / n_metric\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
    "    elif n_metric == 2:\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
    "    else:\n",
    "        area = np.abs(tmp[0])\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def plot_perf_metric(\n",
    "    perf_metric, eligible, x_range, select=None, plot_selected=False, x_breaks=None\n",
    "):\n",
    "    \"\"\" Plot performance metrics of sampled models\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        perf_metric : numpy.array or pandas.Series\n",
    "            Numeric vector of performance metrics for all sampled models\n",
    "        eligible : numpy.array or pandas.Series\n",
    "            Boolean vector of the same length of 'perf_metric', indicating \\\n",
    "                whether each sample is eligible.\n",
    "        x_range : list\n",
    "            Numeric vector indicating the range of eligible values for \\\n",
    "                performance metrics.\n",
    "            Will be indicated by dotted vertical lines in plots.\n",
    "        select : list or numpy.array, optional (default: None)\n",
    "            Numeric vector of indexes of 'perf_metric' to be selected\n",
    "        plot_selected : bool, optional (default: False)\n",
    "            Whether performance metrics of selected models should be plotted in \\\n",
    "                a secondary figure.\n",
    "        x_breaks : list, optional (default: None)\n",
    "            If selected models are to be plotted, the breaks to use in the \\\n",
    "                histogram\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        plot : plotnine.ggplot\n",
    "            Histogram(s) of model performance made using ggplot\n",
    "    \"\"\"\n",
    "    m = len(perf_metric)\n",
    "    perf_df = pd.DataFrame(perf_metric, columns=[\"perf_metric\"], index=None)\n",
    "    plot = (\n",
    "        pn.ggplot(perf_df, pn.aes(x=\"perf_metric\"))\n",
    "        + pn.geoms.geom_histogram(\n",
    "            breaks=np.linspace(np.min(perf_metric), np.max(perf_metric), 40)\n",
    "        )\n",
    "        + pn.geoms.geom_vline(xintercept=x_range, linetype=\"dashed\", size=0.7)\n",
    "        + pn.labels.labs(\n",
    "            x=\"Ratio of loss to minimum loss\",\n",
    "            title=\"\"\"Loss of {m:d} sampled models\n",
    "                \\n{n_elg:d} ({per_elg:.1f}%) sampled models are eligible\"\"\".format(\n",
    "                m=m, n_elg=np.sum(eligible), per_elg=np.sum(eligible) * 100 / m\n",
    "            ),\n",
    "        )\n",
    "        + pn.themes.theme_bw()\n",
    "        + pn.themes.theme(\n",
    "            title=pn.themes.element_text(ha=\"left\"),\n",
    "            axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
    "            axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
    "        )\n",
    "    )\n",
    "    if plot_selected:\n",
    "        if select is None:\n",
    "            print(\"'select' vector is not specified!\\nUsing all models instead\")\n",
    "            select = [i for i in range(len(perf_df))]\n",
    "        try:\n",
    "            perf_select = perf_df.iloc[select]\n",
    "        except:\n",
    "            print(\n",
    "                \"Invalid indexes detected in 'select' vector!\\nUsing all models instead\"\n",
    "            )\n",
    "            select = [i for i in range(len(perf_df))]\n",
    "            perf_select = perf_df.iloc[select]\n",
    "        plot2 = (\n",
    "            pn.ggplot(perf_select, pn.aes(x=\"perf_metric\"))\n",
    "            + pn.geoms.geom_histogram(breaks=x_breaks)\n",
    "            + pn.labels.labs(\n",
    "                x=\"Ratio of loss to minimum loss\",\n",
    "                title=\"{n_select:d} selected models\".format(n_select=len(select)),\n",
    "            )\n",
    "            + pn.themes.theme_bw()\n",
    "            + pn.themes.theme(\n",
    "                title=pn.themes.element_text(ha=\"left\"),\n",
    "                axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
    "                axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
    "            )\n",
    "        )\n",
    "        return (plot, plot2)\n",
    "    else:\n",
    "        return plot\n",
    "\n",
    "\n",
    "def plot_distribution(df, s=4):\n",
    "    num_metrics = df.shape[1] - 2\n",
    "    labels = df.sen_var_exclusion.unique()\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == \"\":\n",
    "            labels[i] = \"No exclusion\"\n",
    "        elif len(labels[i].split(\"_\")) == 2:\n",
    "            labels[i] = f\"Exclusion of {' and '.join(labels[i].split('_'))}\"\n",
    "        elif len(labels[i].split(\"_\")) > 2:\n",
    "            sens = labels[i].split(\"_\")\n",
    "            labels[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
    "        else:\n",
    "            labels[i] = f\"Exclusion of {labels[i]}\"\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(s * num_metrics, s))\n",
    "    for i, x in enumerate(df.columns[:-2]):\n",
    "        ax = axes[i]\n",
    "        # sns.jointplot(data=df, x=x, y=\"auc\", hue=\"sen_var_exclusion\",  ax=ax, legend=False)\n",
    "        sns.histplot(\n",
    "            data=df, x=x, hue=\"sen_var_exclusion\", bins=50, ax=ax, legend=False\n",
    "        )  # layout=(1, num_metrics), figsize=(4, 4), color=\"#595959\",\n",
    "        ax.set_title(x)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Count\" if i == 0 else \"\")\n",
    "\n",
    "    plt.legend(\n",
    "        loc=\"center left\",\n",
    "        title=\"\",\n",
    "        labels=labels[::-1],\n",
    "        ncol=1,\n",
    "        bbox_to_anchor=(1.04, 0.5),\n",
    "        borderaxespad=0,\n",
    "    )\n",
    "    # plt.tight_layout() bbox_transform=fig.transFigure,\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_scatter(df, perf, sen_var_exclusion, title, c1=20, c2=0.15, **kwargs):\n",
    "    ### basic settings ###\n",
    "    np.random.seed(0)\n",
    "    if \"figsize\" not in kwargs.keys():\n",
    "        fig_h = 400\n",
    "        figsize = [fig_h * df.shape[1] * 2.45 / 3, fig_h]\n",
    "    else:\n",
    "        figsize = kwargs[\"figsize\"]\n",
    "    caption_size = figsize[1] / c1  # control font size / figure size\n",
    "    fig_caption_ratio = 0.8\n",
    "    fig_font_size = caption_size * fig_caption_ratio\n",
    "\n",
    "    font_family = \"Arial\"\n",
    "    highlight_color = \"#D4AF37\"\n",
    "    fig_font_unit = c2  # control the relative position of elements\n",
    "    caption_font_unit = fig_font_unit * fig_caption_ratio\n",
    "    d = fig_font_unit / 8\n",
    "    legend_pos_y = 1 + fig_font_unit\n",
    "    subtitle_pos = [legend_pos_y + d, legend_pos_y + d + caption_font_unit]\n",
    "    xlab_pos_y = -fig_font_unit * 2\n",
    "\n",
    "    area_list = []\n",
    "    for i, id in enumerate(df.index):\n",
    "        values = df.loc[id, :]\n",
    "        area_list.append(1 / compute_area(values))\n",
    "    ranking = np.argsort(np.argsort(area_list)[::-1])\n",
    "\n",
    "    # jittering for display\n",
    "    jitter_control = np.zeros(len(ranking))\n",
    "    for idx in range(len(ranking)):\n",
    "        if ranking[idx] == 0:\n",
    "            jitter_control[idx] = 0\n",
    "        elif ranking[idx] <= 10 and ranking[idx] != 0:\n",
    "            jitter_control[idx] = 0.01 * np.random.uniform(0, 1)\n",
    "        elif ranking[idx] <= 10**2 and ranking[idx] > 10:\n",
    "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
    "        elif ranking[idx] <= 10**3 and ranking[idx] > 10**2:\n",
    "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
    "        else:\n",
    "            jitter_control[idx] = 0.02 * np.random.uniform(-1, 1)\n",
    "\n",
    "    ### plot ###\n",
    "    best_id = df.index[np.where(ranking == 0)][0]\n",
    "    worst_id = df.index[np.argmin(area_list)]\n",
    "    meduim_id = df.index[np.argsort(area_list)[int(len(area_list) / 2)]]\n",
    "\n",
    "    num_metrics = df.shape[1]\n",
    "    num_models = df.shape[0]\n",
    "\n",
    "    fig = make_subplots(cols=num_metrics, rows=1, horizontal_spacing=0.13)\n",
    "    cmap = sns.light_palette(\"steelblue\", as_cmap=False, n_colors=df.shape[0])\n",
    "    cmap = cmap[::-1]\n",
    "    colors = [rgb01_hex(cmap[x]) if x != 0 else highlight_color for x in ranking]\n",
    "    sizes = [10 if x != 0 else 20 for x in ranking]\n",
    "\n",
    "    shapes = sen_var_exclusion.copy().tolist()\n",
    "    cases = sen_var_exclusion.unique()\n",
    "    shapes_candidates = [\"square\", \"circle\", \"triangle-up\", \"star\"][: len(cases)]\n",
    "    for i, case in enumerate(cases):\n",
    "        for j, v in enumerate(sen_var_exclusion):\n",
    "            if v == case:\n",
    "                shapes[j] = shapes_candidates[i]\n",
    "\n",
    "        if cases[i] == \"\":\n",
    "            cases[i] = \"No exclusion\"\n",
    "        elif len(cases[i].split(\"_\")) == 2:\n",
    "            cases[i] = f\"Exclusion of {' and '.join(cases[i].split('_'))}\"\n",
    "        elif len(cases[i].split(\"_\")) > 2:\n",
    "            sens = cases[i].split(\"_\")\n",
    "            cases[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
    "        else:\n",
    "            cases[i] = f\"Exclusion of {cases[i]}\"\n",
    "\n",
    "    fair_index_df = pd.DataFrame(\n",
    "        {\n",
    "            \"model id\": df.index,\n",
    "            \"fair_index\": area_list,\n",
    "            \"ranking\": ranking,\n",
    "            \"eod\": df[\"Equalized Odds\"],\n",
    "            \"colors\": colors,\n",
    "            \"shapes\": shapes,\n",
    "            \"sizes\": sizes,\n",
    "            \"cases\": sen_var_exclusion,\n",
    "            \"jitter\": jitter_control,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add scatter plots to the subplots\n",
    "    for k, s in enumerate(shapes_candidates):\n",
    "        for i in range(num_metrics):\n",
    "            # index of sen_var_exclusion(shape) == s\n",
    "            s_idx = [idx for idx, x in enumerate(shapes) if x == s]\n",
    "            x = df.iloc[s_idx, i].values\n",
    "            js = fair_index_df.loc[fair_index_df.shapes == s, \"jitter\"].values\n",
    "            jittered_x = x + js\n",
    "\n",
    "            col = fair_index_df.loc[fair_index_df.shapes == s, \"colors\"]\n",
    "            size = fair_index_df.loc[fair_index_df.shapes == s, \"sizes\"]\n",
    "            fair_index = fair_index_df.loc[fair_index_df.shapes == s, \"fair_index\"]\n",
    "            ids = fair_index_df.loc[fair_index_df.shapes == s, \"model id\"]\n",
    "            rank_text = fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
    "            r = (\n",
    "                fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
    "                .apply(lambda x: math.log10(x + 1))\n",
    "                .values\n",
    "            )\n",
    "            sen_case = fair_index_df.loc[fair_index_df.shapes == s, \"cases\"]\n",
    "\n",
    "            hovertext = [\n",
    "                f\"Fairness index: {f:.3f}. Ranking: {x}. Model id: {i}\"\n",
    "                for f, x, i in zip(fair_index, rank_text, ids)\n",
    "            ]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=r,\n",
    "                    y=jittered_x,\n",
    "                    customdata=hovertext,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        color=col,\n",
    "                        symbol=s,\n",
    "                        size=size,\n",
    "                        line=dict(color=col, width=1),\n",
    "                        opacity=0.8,\n",
    "                    ),\n",
    "                    hovertemplate=\"%{customdata}.\",\n",
    "                    hoverlabel=None,\n",
    "                    hoverinfo=\"name+z\",\n",
    "                    name=cases[k],\n",
    "                ),\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "            )\n",
    "\n",
    "            if i == int((df.shape[1] + 0.5) / 2):\n",
    "                fig.update_xaxes(\n",
    "                    title_text=None,\n",
    "                    tickvals=[0, 1, 2, 3],\n",
    "                    ticktext=[1, 10, 100, 1000],\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                    tickangle=0,\n",
    "                )\n",
    "            else:\n",
    "                fig.update_xaxes(\n",
    "                    title_text=None,\n",
    "                    tickvals=[0, 1, 2, 3],\n",
    "                    ticktext=[1, 10, 100, 1000],\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                    tickangle=0,\n",
    "                )\n",
    "            fig.update_yaxes(\n",
    "                title_text=df.columns[i],\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "                showticksuffix=\"none\",\n",
    "                titlefont={\"size\": caption_size},\n",
    "            )\n",
    "\n",
    "            fig.add_vline(\n",
    "                x=0,\n",
    "                line_width=2,\n",
    "                line_dash=\"dot\",\n",
    "                line_color=highlight_color,\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "            )\n",
    "\n",
    "            min_metric = df.loc[ranking == 0, df.columns[i]].values[0]\n",
    "            max_metric = df.loc[ranking == num_models - 1, df.columns[i]].values[0]\n",
    "            meduim_metric = df.loc[\n",
    "                ranking == int(num_models / 2), df.columns[i]\n",
    "            ].values[0]\n",
    "\n",
    "            # add annotations\n",
    "            anno_size = caption_size * 0.7\n",
    "            if k == 0:\n",
    "                fig.add_hline(\n",
    "                    y=min_metric,\n",
    "                    line_width=2,\n",
    "                    line_dash=\"dot\",\n",
    "                    line_color=highlight_color,\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                )\n",
    "\n",
    "                # position_y = np.mean(df.iloc[:, i])\n",
    "                min_annotation = {\n",
    "                    \"x\": 0,\n",
    "                    \"y\": min_metric,\n",
    "                    \"text\": f\"Model ID {best_id}<br> Rank No.1\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                }\n",
    "                fig.add_annotation(min_annotation, col=i + 1, row=1)\n",
    "            if meduim_id in ids:\n",
    "                medium_annotation = {\n",
    "                    \"x\": math.log10(int(num_models / 2) + 1),\n",
    "                    \"y\": meduim_metric + jitter_control[meduim_id],\n",
    "                    \"text\": f\"Model ID {meduim_id}<br> Rank No.{int(num_models/2)}\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                }\n",
    "                fig.add_annotation(medium_annotation, col=i + 1, row=1)\n",
    "            if worst_id in ids:\n",
    "                max_annotation = {\n",
    "                    \"x\": math.log10(num_models + 1),\n",
    "                    \"y\": max_metric + jitter_control[worst_id],\n",
    "                    \"text\": f\"Model ID {worst_id}<br> Rank No.{num_models}\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                    \"align\": \"left\",\n",
    "                }\n",
    "                fig.add_annotation(max_annotation, col=i + 1, row=1)\n",
    "\n",
    "    colorbar_trace = go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode=\"markers\",\n",
    "        hoverinfo=\"none\",\n",
    "        marker=dict(\n",
    "            colorscale=[\n",
    "                rgb01_hex(np.array((243, 244, 245)) / 255),\n",
    "                \"steelblue\",\n",
    "            ],  # \"magma\",\n",
    "            showscale=True,\n",
    "            cmin=0,\n",
    "            cmax=2,\n",
    "            colorbar=dict(\n",
    "                title=None,\n",
    "                thickness=10,\n",
    "                tickvals=[0, 2],\n",
    "                ticktext=[\"Low\", \"High\"],\n",
    "                outlinewidth=0,\n",
    "                orientation=\"v\",\n",
    "                x=1,\n",
    "                y=0.5,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    fig.add_trace(colorbar_trace)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        font=dict(family=\"Arial\", size=fig_font_size),\n",
    "        hovermode=\"closest\",\n",
    "        width=figsize[0],\n",
    "        height=figsize[1],\n",
    "        showlegend=True,\n",
    "        template=\"simple_white\",\n",
    "        legend=dict(x=0, y=legend_pos_y, orientation=\"h\"),\n",
    "    )\n",
    "\n",
    "    rectangle = {\n",
    "        \"type\": \"rect\",\n",
    "        \"x0\": -0.1,\n",
    "        \"y0\": subtitle_pos[0],\n",
    "        \"x1\": 1.1,\n",
    "        \"y1\": subtitle_pos[1],\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"fillcolor\": \"steelblue\",\n",
    "        \"opacity\": 0.1,\n",
    "    }  # 'line': {'color': 'red', 'width': 2},\n",
    "    fig.add_shape(rectangle)\n",
    "    subtitle_annotation = {\n",
    "        \"x\": -0.1,\n",
    "        \"y\": subtitle_pos[1],\n",
    "        \"text\": f\"<i> The FAIM model (i.e., fairness-aware model) is with model ID {best_id}, out of {num_models} nearly-optimal models.</i>\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": caption_size * 1.1},\n",
    "        \"align\": \"left\",\n",
    "    }\n",
    "    xaxis_annotation = {\n",
    "        \"x\": 0.5,\n",
    "        \"y\": xlab_pos_y,\n",
    "        \"text\": \"Model Rank\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": caption_size},\n",
    "    }\n",
    "    colorbar_title = {\n",
    "        \"x\": 1.05,\n",
    "        \"y\": 0.5,\n",
    "        \"text\": \"Fairness Ranking Index (FRI)\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": anno_size * 0.9},\n",
    "        \"textangle\": 90,\n",
    "    }\n",
    "    fig.add_annotation(subtitle_annotation)\n",
    "    fig.add_annotation(xaxis_annotation)\n",
    "    fig.add_annotation(colorbar_title)\n",
    "\n",
    "    for i, trace in enumerate(fig.data):\n",
    "        if i % num_metrics == 1:\n",
    "            trace.update(showlegend=True)\n",
    "        else:\n",
    "            trace.update(showlegend=False)\n",
    "    # fig.show()\n",
    "\n",
    "    return fig, fair_index_df\n",
    "\n",
    "\n",
    "def plot_radar(df, thresh_show, title, **kwargs):\n",
    "    fig = go.Figure()\n",
    "    # fig = sp.make_subplots(rows=1, cols=2)\n",
    "    cmap = sns.diverging_palette(200, 20, sep=10, s=50, as_cmap=False, n=df.shape[0])\n",
    "    theta = df.columns.tolist()\n",
    "    theta += theta[:1]\n",
    "    area_list = []\n",
    "\n",
    "    for i, id in enumerate(df.index):\n",
    "        values = df.loc[id, :]\n",
    "        area_list.append(compute_area(values))\n",
    "        values = values.values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        info = [\n",
    "            f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
    "        ]\n",
    "        fig.add_trace(\n",
    "            go.Scatterpolar(\n",
    "                r=values,\n",
    "                theta=theta,\n",
    "                fill=\"toself\" if id == \"FAIReg\" else \"none\",\n",
    "                text=\"\\n\".join(info),\n",
    "                name=f\"{id}\",\n",
    "                line=dict(color=rgb01_hex(cmap[i]), dash=\"dot\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ranking = np.argsort(np.argsort(area_list))\n",
    "    best_id = df.index[np.where(ranking == 0)][0]\n",
    "    print(\n",
    "        f\"The best model is No.{best_id} with metrics on validation set:\\n {df.loc[best_id, :]}\"\n",
    "    )\n",
    "    values = df.loc[best_id, :].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    info = [\n",
    "        f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
    "    ]\n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=theta,\n",
    "            fill=\"toself\",\n",
    "            text=\"\\n\".join(info),\n",
    "            name=f\"model {best_id}\",\n",
    "            line=dict(color=\"royalblue\", dash=\"solid\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        # title = title,\n",
    "        font=dict(family=\"Arial\", size=16),\n",
    "        polar=dict(\n",
    "            # bgcolor = \"#1e2130\",\n",
    "            radialaxis=dict(\n",
    "                showgrid=True,\n",
    "                gridwidth=1,\n",
    "                gridcolor=\"lightgray\",\n",
    "                visible=True,\n",
    "                range=[0, thresh_show],\n",
    "            )\n",
    "        ),\n",
    "        legend=dict(x=0.25, y=-0.1, orientation=\"h\"),\n",
    "        showlegend=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_bar(\n",
    "    shap_values, feature_names, original_feature_names, coef=None, title=None, **kwargs\n",
    "):\n",
    "    \"\"\"Plot the bar chart of feature importance\"\"\"\n",
    "    if \"color\" not in kwargs.keys():\n",
    "        color = \"steelblue\"\n",
    "    else:\n",
    "        color = kwargs[\"color\"]\n",
    "\n",
    "    def get_prefix(v):\n",
    "        if \"_\" in v and (v not in original_feature_names):\n",
    "            tmp = [\"_\".join(v.split(\"_\")[:i]) for i in range(len(v.split(\"_\")))]\n",
    "            return [s for s in tmp if s in original_feature_names][0]\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    if shap_values is not None:\n",
    "\n",
    "        grouped_df = pd.DataFrame({\"values\": shap_values}, index=feature_names).groupby(\n",
    "            by=get_prefix, axis=0\n",
    "        )\n",
    "        df = {k: np.mean(np.abs(g.values)) for k, g in grouped_df}\n",
    "        df = pd.DataFrame.from_dict(df, orient=\"index\").reset_index()\n",
    "        df.columns = [\"Var\", \"Value\"]\n",
    "\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in df.Value],\n",
    "                        \"order\": np.abs(df.Value),\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    elif coef is not None:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"Var\": coef.index,\n",
    "                \"Value\": coef.values,\n",
    "                \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in coef.values],\n",
    "                \"order\": np.abs(coef.values),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Either shap_value or coef should be provided\")\n",
    "\n",
    "    df = df.loc[df[\"Var\"] != \"const\", :]\n",
    "    df = df.sort_values(by=\"order\", ascending=True)\n",
    "    df[\"Var\"] = pd.Categorical(df[\"Var\"], categories=df[\"Var\"].tolist(), ordered=True)\n",
    "\n",
    "    common_theme = theme(\n",
    "        text=element_text(size=24),\n",
    "        panel_grid_major_y=element_line(colour=\"lightgrey\"),\n",
    "        panel_grid_minor=element_blank(),\n",
    "        panel_background=element_blank(),\n",
    "        axis_line_x=element_line(colour=\"black\"),\n",
    "        axis_ticks_major_y=element_blank(),\n",
    "    )\n",
    "\n",
    "    x_lab = \"Feature importance\"\n",
    "\n",
    "    p = (\n",
    "        ggplot(data=df, mapping=aes(x=\"Var\", y=\"Value\", fill=\"color\"))\n",
    "        + geom_hline(yintercept=0, color=\"grey\")\n",
    "        + geom_bar(stat=\"identity\")\n",
    "        + common_theme\n",
    "        + coord_flip()\n",
    "        + labs(x=\"\", y=x_lab, title=title)\n",
    "        + theme(legend_position=\"none\")\n",
    "        + scale_fill_manual(values=[color])\n",
    "    )\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "\n",
    "# metrics\n",
    "def get_ci_auc(y_true, y_pred, alpha=0.05, type=\"auc\"):\n",
    "    \"\"\"Calculate the confidence interval for the AUC (Area Under the Curve) score\n",
    "    or PR (Precision-Recall) score using bootstrapping.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted scores or probabilities.\n",
    "        alpha (float, optional): Significance level for the confidence interval. Default is 0.05.\n",
    "        type (str, optional): Type of score to calculate: 'auc' (default) or 'pr' (precision-recall).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "\n",
    "    n_bootstraps = 1000\n",
    "    bootstrapped_scores = []\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(y_pred) - 1, len(y_pred))\n",
    "\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "\n",
    "        if type == \"pr\":\n",
    "            precision, recall, thresholds = precision_recall_curve(\n",
    "                y_true[indices], y_pred[indices]\n",
    "            )\n",
    "            score = auc(recall, precision)\n",
    "        else:\n",
    "            score = roc_auc_score(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    # 95% c.i.\n",
    "    confidence_lower = sorted_scores[int(alpha / 2 * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(1 - alpha / 2 * len(sorted_scores))]\n",
    "\n",
    "    return confidence_lower, np.median(sorted_scores), confidence_upper\n",
    "\n",
    "\n",
    "def find_optimal_cutoff(target, predicted, method=\"auc\"):\n",
    "    \"\"\"Find the optimal probability cutoff point for a classification model related to event rate.\n",
    "\n",
    "    Args:\n",
    "        target (array-like): True labels.\n",
    "        predicted (array-like): Predicted scores or probabilities.\n",
    "        method (str, optional): Method for finding the optimal cutoff. Default is 'auc'.\n",
    "\n",
    "    Returns:\n",
    "        list: List of optimal cutoff values.\n",
    "    \"\"\"\n",
    "    if method == \"auc\":\n",
    "        fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "        i = np.arange(len(tpr))\n",
    "        roc = pd.DataFrame(\n",
    "            {\n",
    "                \"tf\": pd.Series(tpr + (1 - fpr), index=i),\n",
    "                \"threshold\": pd.Series(threshold, index=i),\n",
    "            }\n",
    "        )\n",
    "        roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[::-1][:1]]\n",
    "    elif method == \"pr-auc\":\n",
    "        precision, recall, threshold = precision_recall_curve(target, predicted)\n",
    "        i = np.arange(len(precision))\n",
    "        prc = pd.DataFrame(\n",
    "            {\n",
    "                \"tf\": pd.Series(tpr - (1 - fpr), index=i),\n",
    "                \"threshold\": pd.Series(threshold, index=i),\n",
    "            }\n",
    "        )\n",
    "        prc_t = prc.iloc[(prc.tf - 0).abs().argsort()[:1]]\n",
    "\n",
    "    return list(roc_t[\"threshold\"])\n",
    "\n",
    "\n",
    "def get_cal_fairness(df):\n",
    "    def absolute_difference(x):\n",
    "        return np.abs(spline1(x) - spline0(x))\n",
    "\n",
    "    df.groupby(\"group\").apply(lambda x: np.max(x[\"p_obs\"]))\n",
    "    # for g in df_calib.group.unique():\n",
    "\n",
    "    gs = df.group.unique()\n",
    "    pairs = list(combinations(gs, 2))\n",
    "\n",
    "    x_min_thresh = np.min(df.groupby(\"group\").apply(lambda x: np.min(x[\"p_pred\"])))\n",
    "    x_max_thresh = np.max(df.groupby(\"group\").apply(lambda x: np.max(x[\"p_pred\"])))\n",
    "    num_points = 100\n",
    "    diff_cal = []\n",
    "\n",
    "    for p in pairs:\n",
    "        p0 = df.loc[df.group == p[0], [\"p_obs\", \"p_pred\"]].sort_values(\n",
    "            by=\"p_pred\", ascending=True\n",
    "        )\n",
    "        p1 = df.loc[df.group == p[1], [\"p_obs\", \"p_pred\"]].sort_values(\n",
    "            by=\"p_pred\", ascending=True\n",
    "        )\n",
    "\n",
    "        x0 = p0[\"p_pred\"]\n",
    "        y0 = p0[\"p_obs\"]\n",
    "        spline0 = CubicSpline(x0, y0)\n",
    "\n",
    "        x1 = p1[\"p_pred\"]\n",
    "        y1 = p1[\"p_obs\"]\n",
    "        spline1 = CubicSpline(x1, y1)\n",
    "\n",
    "        x_sample = np.linspace(x_min_thresh, x_max_thresh, num_points)\n",
    "        y_sample = absolute_difference(x_sample)\n",
    "        area = integrate.simpson(y_sample, x_sample)\n",
    "        diff_cal.append(area)\n",
    "\n",
    "    cal_metric = np.mean(diff_cal)\n",
    "    # print(f\"Calibration metric: {cal_metric:.2f}\")\n",
    "    return cal_metric\n",
    "\n",
    "\n",
    "## small functions\n",
    "def col_gap(col_train, col_test, x_with_constant):\n",
    "    if len(col_train) != len(col_test):\n",
    "        col_gap = [i not in col_test for i in col_train]\n",
    "        x_with_constant[col_train[col_gap]] = 0\n",
    "        x_with_constant = x_with_constant.loc[:, col_train]\n",
    "\n",
    "    return x_with_constant\n",
    "\n",
    "\n",
    "def generate_subsets(input_list):\n",
    "    subsets = []\n",
    "    n = len(input_list)\n",
    "    for subset_size in range(n + 1):\n",
    "        for subset in itertools.combinations(input_list, subset_size):\n",
    "            subsets.append(list(subset))\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHDw9mDnovxk"
   },
   "source": [
    "##fairness_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sE_0l1VCRnK7"
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "\n",
    "class FairBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dat_train,\n",
    "        selected_vars,\n",
    "        selected_vars_cat,\n",
    "        y_name,\n",
    "        sen_name,\n",
    "        sen_var_ref,\n",
    "        without_sen=False,\n",
    "        weighted=True,\n",
    "        weights={\"tnr\": 0.5, \"tpr\": 0.5},\n",
    "        class_weight=\"balanced\",\n",
    "    ):\n",
    "        \"\"\"Initialize the fairness base class\n",
    "\n",
    "        Args:\n",
    "            dat_train (data frame): training data\n",
    "            selected_vars (list): selected variables including sensitive variables\n",
    "            selected_vars_cat (list): selected categorical variables\n",
    "            y_name (str): the name of the label\n",
    "            sen_name (list): the name of the sensitive variable\n",
    "            sen_var_ref (dict): the reference level of the sensitive variables\n",
    "            without_sen (bool, optional): directly exclude the sensitive variables. Defaults to False.\n",
    "            weighted (bool, optional): compute the weighted version of metrics \"tnr\" and \"tpr\". Defaults to True.\n",
    "            weights (dict, optional): the weightage for \"tnr\" and \"tpr\", summing up to 1. Defaults to {\"tnr\": 0.5, \"tpr\": 0.5}.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dat_train = dat_train\n",
    "        self.vars = selected_vars\n",
    "        self.vars_cat = selected_vars_cat\n",
    "        self.y_name = y_name\n",
    "\n",
    "        if not isinstance(sen_name, list):\n",
    "            self.sen_name = [self.sen_name]\n",
    "        for s in sen_name:\n",
    "            if sen_var_ref[s] not in dat_train[s].unique():\n",
    "                raise ValueError(\n",
    "                    f\"Please provide the right reference level of sensitive variables {s}!\"\n",
    "                )\n",
    "            if s not in self.vars:\n",
    "                self.vars.append(s)\n",
    "        else:\n",
    "            self.sen_name = sen_name\n",
    "            self.sen_var_ref = sen_var_ref\n",
    "\n",
    "        self.without_sen = without_sen\n",
    "        self.weighted = weighted\n",
    "        self.weights = weights\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"Compute class weights for balanced training\n",
    "\n",
    "        Args:\n",
    "            y: target labels\n",
    "\n",
    "        Returns:\n",
    "            array of weights for each sample\n",
    "        \"\"\"\n",
    "        if self.class_weight == \"balanced\":\n",
    "            classes, counts = np.unique(y, return_counts=True)\n",
    "            n_samples = len(y)\n",
    "            n_classes = len(classes)\n",
    "            class_weights = n_samples / (n_classes * counts)\n",
    "            weights = np.array([class_weights[c] for c in y])\n",
    "            return weights\n",
    "        else:\n",
    "            return np.ones(len(y))\n",
    "\n",
    "    def data_process(\n",
    "        self, dat, selected_vars=None, selected_vars_cat=None, without_sen=None\n",
    "    ):\n",
    "        \"\"\"Data preprocess\n",
    "\n",
    "        Args:\n",
    "            dat (data frame): data\n",
    "            selected_vars (list, optional): selected variables (can include sensitive variables). This needs to be provided if the case considered is beyond completely inclusion or exclusion of sensitive variables. Defaults to None.\n",
    "            selected_vars_cat (list, optional): selected categorical variables, subset of selected variables. Defaults to None.\n",
    "            without_sen (bool, optional): directly exclude the sensitive variables. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            x_1: predictors with one-coding and with constant\n",
    "            sen_var: the vector of sensitive variable. combined by \"_\", if there are several sensitive variables\n",
    "            y: the vector of the label\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def combine_sen(dat, sen):\n",
    "            new_sen = [\"_\".join(v) for v in zip(*[dat[s].astype(\"str\") for s in sen])]\n",
    "            return new_sen\n",
    "\n",
    "        if selected_vars is None:\n",
    "            selected_vars = self.vars\n",
    "            selected_vars_cat = self.vars_cat\n",
    "\n",
    "        x = dat.drop(\n",
    "            columns=[\n",
    "                c for c in dat.columns if c == self.y_name or c not in selected_vars\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            self.without_sen != \"auto\"\n",
    "            and (without_sen is None and self.without_sen)\n",
    "            or without_sen\n",
    "        ):\n",
    "            x = x.drop(columns=self.sen_name)\n",
    "\n",
    "        if len(self.sen_name) > 1:\n",
    "            sen_var = combine_sen(dat, self.sen_name)\n",
    "        else:\n",
    "            sen_var = dat[self.sen_name[0]]\n",
    "\n",
    "        y = dat[self.y_name]\n",
    "\n",
    "        x_dm, x_groups = _util.model_matrix(x=x, x_names_cat=selected_vars_cat)\n",
    "        x_1 = sm.add_constant(x_dm).astype(\"float\")\n",
    "        return x_1, sen_var, y\n",
    "\n",
    "    def data_prepare(self, dat_expl=None):\n",
    "        \"\"\"Shape the data to AIF360 format\n",
    "\n",
    "        Args:\n",
    "            dat_expl (_type_, optional): validation data needed for post-processing methods. Defaults to None.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.method == \"Unawareness\":\n",
    "            x_with_constant, _, y_train = self.data_process(\n",
    "                self.dat_train if dat_expl is None else dat_expl, without_sen=True\n",
    "            )\n",
    "            return x_with_constant\n",
    "\n",
    "        if self.method_type == \"pre\":\n",
    "            x_with_constant, _, y_train = self.data_process(self.dat_train)\n",
    "            x_with_constant_sen_bin = copy.deepcopy(x_with_constant)\n",
    "            for s in self.sen_name:\n",
    "                x_with_constant_sen_bin[s] = [\n",
    "                    0 if i == self.sen_var_ref[s] else 1 for i in self.dat_train[s]\n",
    "                ]\n",
    "            # print(x_with_constant_expl_sen_bin.columns.head(), flush=True)\n",
    "\n",
    "            pre_train_df = pd.concat([x_with_constant_sen_bin, y_train], axis=1)\n",
    "            pre_train = BinaryLabelDataset(\n",
    "                favorable_label=1,\n",
    "                df=pre_train_df,\n",
    "                label_names=[self.y_name],\n",
    "                protected_attribute_names=self.sen_name,\n",
    "            )\n",
    "            return pre_train\n",
    "\n",
    "        elif self.method_type == \"post\":\n",
    "            if dat_expl is None:\n",
    "                raise ValueError(\"Please provide validation data.\")\n",
    "            else:\n",
    "                x_with_constant_expl, sen_var, y_expl = self.data_process(dat_expl)\n",
    "                x_with_constant_expl_sen_bin = copy.deepcopy(x_with_constant_expl)\n",
    "\n",
    "                for s in self.sen_name:\n",
    "                    x_with_constant_expl_sen_bin[s] = [\n",
    "                        0 if i == self.sen_var_ref[s] else 1 for i in dat_expl[s]\n",
    "                    ]\n",
    "\n",
    "                prob_expl_ori = self.lr_results.predict(x_with_constant_expl)\n",
    "                ori_thresh = find_optimal_cutoff(y_expl, prob_expl_ori)[0]\n",
    "                pred_expl_ori = prob_expl_ori > ori_thresh\n",
    "\n",
    "                post_expl_df = pd.concat([x_with_constant_expl_sen_bin, y_expl], axis=1)\n",
    "                post_expl = BinaryLabelDataset(\n",
    "                    favorable_label=1,\n",
    "                    df=post_expl_df,\n",
    "                    label_names=[self.y_name],\n",
    "                    protected_attribute_names=self.sen_name,\n",
    "                )\n",
    "                post_expl_pred = post_expl.copy(deepcopy=True)\n",
    "                post_expl_pred.scores = prob_expl_ori.values.reshape(-1, 1)\n",
    "                post_expl_pred.labels = pred_expl_ori.values.reshape(-1, 1)\n",
    "                return post_expl, post_expl_pred\n",
    "\n",
    "    def model(self, method_type=None, method=None, dat_expl=None, **kwargs):\n",
    "        \"\"\"Fit the model\n",
    "\n",
    "        Args:\n",
    "            method_type (str, optional): the type of bias mitigation method (pre/in/post). Defaults to None.\n",
    "            method (str, optional): the name of bias mitigation method. Defaults to None.\n",
    "            dat_expl (_type_, optional): validation data needed for post-processing methods. Defaults to None.\n",
    "\n",
    "            Methods:\n",
    "            +------------------+--------------------------------+\n",
    "            | Method type      | Specific methods               |\n",
    "            +==================+================================+\n",
    "            | None             | \"OriginalLR\", \"Unawareness\"   |\n",
    "            +------------------+--------------------------------+\n",
    "            | \"pre\"            | \"Reweigh\"                      |\n",
    "            +------------------+--------------------------------+\n",
    "            | \"in\"             | \"Reductions\"                   |\n",
    "            +------------------+--------------------------------+\n",
    "            | \"post\"           | \"EqOdds\", \"CalEqOdds\", \"ROC\"  |\n",
    "            +------------------+--------------------------------+\n",
    "\n",
    "        Returns:\n",
    "            model results that can be used for prediction\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.method_type = method_type\n",
    "\n",
    "        if isinstance(self.sen_name, list):\n",
    "            privileged_groups = [{s: 0 for s in self.sen_name}]\n",
    "            unprivileged_groups = [{s: 1 for s in self.sen_name}]\n",
    "        else:\n",
    "            privileged_groups = [{self.sen_name: 0}]\n",
    "            unprivileged_groups = [{self.sen_name: 1}]\n",
    "\n",
    "        x_with_constant_nosen, _, y_train = self.data_process(\n",
    "            self.dat_train, without_sen=True\n",
    "        )\n",
    "        x_with_constant, _, y_train = self.data_process(self.dat_train)\n",
    "\n",
    "        # original LR\n",
    "        if self.class_weight == \"balanced\":\n",
    "            sample_weights = self.compute_class_weights(self.dat_train[self.y_name])\n",
    "            lr_model = sm.GLM(\n",
    "                self.dat_train[self.y_name], x_with_constant, family=sm.families.Binomial(),\n",
    "                freq_weights=sample_weights\n",
    "            )\n",
    "        else:\n",
    "            lr_model = sm.GLM(\n",
    "                self.dat_train[self.y_name], x_with_constant, family=sm.families.Binomial()\n",
    "            )\n",
    "        self.lr_results = lr_model.fit()\n",
    "\n",
    "        if method_type == None:\n",
    "            if method == \"OriginalLR\":\n",
    "                return self.lr_results\n",
    "\n",
    "            elif method == \"Unawareness\":\n",
    "                un_model = sm.GLM(\n",
    "                    self.dat_train[self.y_name],\n",
    "                    x_with_constant_nosen,\n",
    "                    family=sm.families.Binomial(),\n",
    "                )\n",
    "                un_results = un_model.fit()\n",
    "                return un_results\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Please confirm the method: 'OriginalLR' if no bias mitigation is needed; 'Unawareness' if simply excluding the sensitive variabl is enough.\"\n",
    "                )\n",
    "\n",
    "        elif method_type == \"pre\":\n",
    "            pre_train = self.data_prepare()\n",
    "\n",
    "            if method == \"Reweigh\":\n",
    "                reweigh_model = Reweighing(\n",
    "                    privileged_groups=privileged_groups,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                )\n",
    "                rw_train = reweigh_model.fit_transform(pre_train)\n",
    "                rw_model = sm.GLM(\n",
    "                    self.dat_train[self.y_name],\n",
    "                    x_with_constant,\n",
    "                    family=sm.families.Binomial(),\n",
    "                    freq_weights=rw_train.instance_weights,\n",
    "                )\n",
    "                plt.hist(rw_train.instance_weights)\n",
    "                rw_results = rw_model.fit()\n",
    "                return rw_model, rw_results, rw_train.instance_weights\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Please specify the type of pre-process bias mitigation method among ['Reweigh']!\"\n",
    "                )\n",
    "\n",
    "        elif method_type == \"in\":\n",
    "            if method == \"Reductions\":\n",
    "\n",
    "                constraint = EqualizedOdds(difference_bound=0.01)\n",
    "                np.random.seed(\n",
    "                    0\n",
    "                )  # set seed for consistent results with ExponentiatedGradient\n",
    "                lr_model_sk = LogisticRegression(max_iter=5000, penalty=None)\n",
    "                mitigator = ExponentiatedGradient(lr_model_sk, constraint)\n",
    "\n",
    "                mitigator.fit(\n",
    "                    x_with_constant,\n",
    "                    y_train,\n",
    "                    sensitive_features=self.dat_train[self.sen_name],\n",
    "                )\n",
    "                return mitigator\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Please specify the type of in-process bias mitigation method among ['Reductions']!\"\n",
    "                )\n",
    "\n",
    "        elif method_type == \"post\":\n",
    "            post_expl, post_expl_pred = self.data_prepare(dat_expl=dat_expl)\n",
    "\n",
    "            if method == \"EqOdds\":\n",
    "                eq_model = EqOddsPostprocessing(\n",
    "                    privileged_groups=privileged_groups,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                    seed=seed,\n",
    "                )\n",
    "                eq_results = eq_model.fit(post_expl, post_expl_pred)\n",
    "                return eq_results\n",
    "\n",
    "            elif method == \"CalEqOdds\":\n",
    "                if \"cost_constraint\" in kwargs:\n",
    "                    cost_constraint = kwargs[\"cost_constraint\"]\n",
    "                else:\n",
    "                    cost_constraint = \"weighted\"  # \"fnr\", \"fpr\", \"weighted\"\n",
    "                cal_eq_model = CalibratedEqOddsPostprocessing(\n",
    "                    privileged_groups=privileged_groups,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                    cost_constraint=cost_constraint,\n",
    "                    seed=seed,\n",
    "                )\n",
    "                cal_eq_results = cal_eq_model.fit(post_expl, post_expl_pred)\n",
    "                return cal_eq_results\n",
    "\n",
    "            elif method == \"ROC\":\n",
    "                ub = 0.05 if \"ub\" not in kwargs else kwargs[\"ub\"]\n",
    "                lb = -0.05 if \"lb\" not in kwargs else kwargs[\"lb\"]\n",
    "                metric_name = (\n",
    "                    \"Equal opportunity difference\"\n",
    "                    if \"metric_name\" not in kwargs\n",
    "                    else kwargs[\"metric_name\"]\n",
    "                )\n",
    "                # allowed_metrics = [\"Statistical parity difference\", \"Average odds difference\", \"Equal opportunity difference\"]\n",
    "                ROC_model = RejectOptionClassification(\n",
    "                    privileged_groups=privileged_groups,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                    low_class_thresh=0.01,\n",
    "                    high_class_thresh=0.99,\n",
    "                    num_class_thresh=100,\n",
    "                    num_ROC_margin=50,\n",
    "                    metric_name=metric_name,\n",
    "                    metric_ub=ub,\n",
    "                    metric_lb=lb,\n",
    "                )\n",
    "                ROC_results = ROC_model.fit(post_expl, post_expl_pred)\n",
    "                return ROC_results\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Please specify the type of post-process bias mitigation method among ['EqOdds', 'CalEqOdds', 'ROC']!\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Please specify the type of bias mitigation method (pre/in/post)!\"\n",
    "            )\n",
    "\n",
    "    def test(self, dat_test, model=None, params=None, thresh=None, **kwargs):\n",
    "        \"\"\"Test the fairness of the model\n",
    "\n",
    "        Args:\n",
    "            dat_test (data frame): test data\n",
    "            model (_type_, optional): the fitted model. Defaults to None.\n",
    "            params (_type_, optional): coefficients for the model. Defaults to None.\n",
    "            thresh (_type_, optional): threshold for the predictions. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pred_test / prob_test (array): predicted labels / predicted probabilities\n",
    "            fairmetrics (data frame): fairness metrics\n",
    "            fairsummary (data frame): fairness summary for each subgroup\n",
    "        \"\"\"\n",
    "        if \"without_sen\" in kwargs.keys():\n",
    "            without_sen = kwargs[\"without_sen\"]\n",
    "        else:\n",
    "            without_sen = self.without_sen\n",
    "        x_with_constant_test, sen_var, y_test = self.data_process(dat_test)\n",
    "        prob_test = None\n",
    "        thresh = None\n",
    "\n",
    "        if model is not None:\n",
    "            if self.method_type == \"post\":\n",
    "                _, post_test_pred = self.data_prepare(dat_expl=dat_test)\n",
    "                pred_test = model.predict(post_test_pred).labels.reshape(-1)\n",
    "            else:\n",
    "                if self.method_type == None and self.method == \"Unawareness\":\n",
    "                    x_with_constant_test = self.data_prepare(dat_expl=dat_test)\n",
    "\n",
    "                if self.method == \"Reductions\":\n",
    "                    pred_test = model.predict(x_with_constant_test)\n",
    "                else:\n",
    "                    prob_test = model.predict(x_with_constant_test)\n",
    "                    thresh = find_optimal_cutoff(y_test, prob_test)[0]\n",
    "                    pred_test = prob_test > thresh\n",
    "                    # print(prob_test)\n",
    "        else:\n",
    "            raise ValueError(\"Please provide the right model!\")\n",
    "\n",
    "        fe = FAIMEvaluator(\n",
    "            y_true=y_test,\n",
    "            y_pred=prob_test,\n",
    "            y_pred_bin=pred_test,\n",
    "            sen_var=sen_var,\n",
    "            weighted=self.weighted,\n",
    "            weights=self.weights,\n",
    "        )\n",
    "        fairmetrics = fe.fairmetrics\n",
    "        fairsummary = fe.fairsummary\n",
    "        clametrics = fe.clametrics\n",
    "\n",
    "        return pred_test if prob_test is None else prob_test, fairmetrics, clametrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmuQmuFto1c1"
   },
   "source": [
    "##fairness_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt0L8vOiyVsC"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "my_fairness_bases = {\n",
    "    \"tpr\": recall_score,\n",
    "    \"tnr\": true_negative_rate,\n",
    "    \"sr\": selection_rate,\n",
    "    \"acc\": accuracy_score,\n",
    "    \"conf_mat\": confusion_matrix,\n",
    "}\n",
    "# the situation for each group should not be bad; tnr -> fpr\n",
    "my_bases_bound = {\"tpr\": 0.6, \"tnr\": 0.6, \"sr\": 0, \"acc\": 0.6, \"conf_mat\": pd.NA}\n",
    "\n",
    "\n",
    "def fairarea(fairness_metrics):\n",
    "    n_metric = len(fairness_metrics)\n",
    "    tmp = fairness_metrics.values.flatten().tolist()\n",
    "    tmp_1 = tmp[1:] + tmp[:1]\n",
    "\n",
    "    if n_metric > 2:\n",
    "        theta_c = 2 * np.pi / n_metric\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
    "    elif n_metric == 2:\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
    "    else:\n",
    "        area = np.abs(tmp[0])\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "class FAIMEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        y_pred_bin,\n",
    "        sen_var,\n",
    "        fair_only=False,\n",
    "        cla_metrics=[\"auc\"],\n",
    "        weighted=False,\n",
    "        weights=None,\n",
    "        bases=my_fairness_bases,\n",
    "        bound=my_bases_bound,\n",
    "    ):\n",
    "        \"\"\"Initialize the fairness evaluator.\n",
    "\n",
    "        Args:\n",
    "            y_true: true labels\n",
    "            y_pred: predicted scores or probabilities\n",
    "            y_pred_bin: predicted binary labels\n",
    "            sen_var: the vector of sensitive variables\n",
    "            fair_only (bool, optional): whether to compute fairness metrics only. Defaults to False.\n",
    "            cla_metrics (list, optional): classification metrics. Defaults to [\"auc\"].\n",
    "            weighted (bool, optional): whether to create a customized fairness metric based on weighted combining of 'tnr' and 'tpr'. Defaults to False.\n",
    "            weights (_type_, optional): the weights for weighted combining of 'tnr' and 'tpr'. Required when `weighted` is True. Defaults to None.\n",
    "            bases (_type_, optional): the bases for fairness metrics. Defaults to my_fairness_bases (see above).\n",
    "            bound (_type_, optional): the bound for base metrics. Defaults to my_bases_bound.\n",
    "        \"\"\"\n",
    "        # super().__init__(y_obs=y_true, y_pred=y_pred, y_pred_bin=y_pred_bin, sens_var=pd.Series(sen_var), y_pos=True)\n",
    "\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.y_pred_bin = pd.Series(y_pred_bin)\n",
    "        self.sen_var = pd.Series(sen_var)\n",
    "        self.cla_metrics = cla_metrics\n",
    "\n",
    "        self.my_fairness_bases = bases\n",
    "        self.my_bases_bound = bound\n",
    "\n",
    "        if weighted:\n",
    "            if weights is None or not isinstance(weights, dict):\n",
    "                raise TypeError(\n",
    "                    \"The weights need to be specified and the type should be dict!\"\n",
    "                )\n",
    "            elif len(weights) != 2 or np.sum(list(weights.values())) != 1:\n",
    "                raise ValueError(\n",
    "                    \"The weights should be a dict containing two values respectively for 'tpr' and 'tnr'. In addition, the sum of weights should be equal to 1!\"\n",
    "                )\n",
    "            else:\n",
    "                self.weighted = weighted\n",
    "                self.weights = weights\n",
    "\n",
    "        self._fairsummary_generation()\n",
    "        self._fairmetrics_generation()\n",
    "        if not fair_only:\n",
    "            if cla_metrics is None:\n",
    "                raise ValueError(\"The classification metrics should be specified!\")\n",
    "            self._clametric_generation()\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_sen(y_obs, sen_var, sens_var_ref):\n",
    "        # print(\"Checking the sensitive variable...\")\n",
    "        return {\"sens_var\": sen_var, \"sens_var_ref\": pd.unique(sen_var)[0]}\n",
    "\n",
    "    def _fairsummary_generation(self):\n",
    "        \"\"\"Computation primary metrics (e.g., TPR, TNR, etc.) among subgroups\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        self.fairsummary = MetricFrame(\n",
    "            y_true=self.y_true,\n",
    "            y_pred=self.y_pred_bin,\n",
    "            metrics=self.my_fairness_bases,\n",
    "            sensitive_features=self.sen_var,\n",
    "        )\n",
    "\n",
    "        # Create performance metrics table with additional metrics\n",
    "        by_group = self.fairsummary.by_group\n",
    "\n",
    "        tpr = by_group[\"tpr\"]\n",
    "        tnr = by_group[\"tnr\"]\n",
    "        sr = by_group[\"sr\"]\n",
    "        acc = by_group[\"acc\"]\n",
    "\n",
    "        # Calculate FPR, FNR, and BER\n",
    "        fpr = 1 - tnr\n",
    "        fnr = 1 - tpr\n",
    "        ber = (fpr + fnr) / 2\n",
    "\n",
    "        # Create comprehensive performance table\n",
    "        self.performance_table = pd.DataFrame({\n",
    "            \"TPR\": tpr,\n",
    "            \"FPR\": fpr,\n",
    "            \"TNR\": tnr,\n",
    "            \"FNR\": fnr,\n",
    "            \"SR\": sr,\n",
    "            \"Accuracy\": acc,\n",
    "            \"BER\": ber\n",
    "        })\n",
    "\n",
    "    def _fairmetrics_generation(self):\n",
    "        \"\"\"Generate fairness metrics and disparity tables.\"\"\"\n",
    "\n",
    "        # ----- machine learning performance-based fairness metrics -----\n",
    "        bases = self.my_fairness_bases.keys()\n",
    "        fairmetrics = {}\n",
    "        qc = {}\n",
    "        diff_ = self.fairsummary.difference()\n",
    "        for b in list(bases)[:-1]:\n",
    "            qc[b] = self.fairsummary.overall[b] > self.my_bases_bound[b]\n",
    "\n",
    "        fairmetrics[\"Equal Opportunity\"] = diff_[\"tpr\"]\n",
    "        fairmetrics[\"Equalized Odds\"] = np.max([diff_[\"tpr\"], diff_[\"tnr\"]])\n",
    "        fairmetrics[\"Statistical Parity\"] = diff_[\"sr\"]\n",
    "        fairmetrics[\"Accuracy Equality\"] = diff_[\"acc\"]\n",
    "\n",
    "        # MODIFIED: Calculate true BER Equality as Range(BER)\n",
    "        by_group = self.fairsummary.by_group\n",
    "        fpr_values = 1 - by_group[\"tnr\"]\n",
    "        fnr_values = 1 - by_group[\"tpr\"]\n",
    "        ber_values = 0.5 * (fpr_values + fnr_values)\n",
    "\n",
    "        if self.weighted:\n",
    "            # True BER Equality = Range of BER across groups\n",
    "            fairmetrics[\"BER Equality\"] = ber_values.max() - ber_values.min()\n",
    "        else:\n",
    "            fairmetrics[\"BER Equality\"] = ber_values.max() - ber_values.min()\n",
    "\n",
    "        self.fairmetrics = pd.DataFrame([fairmetrics])\n",
    "\n",
    "        # Create fairness disparity summary table\n",
    "        diff_ = self.fairsummary.difference()\n",
    "\n",
    "        # Get TPR, FPR disparities\n",
    "        tpr_diff = diff_[\"tpr\"]\n",
    "        fpr_diff = abs(diff_[\"tnr\"])  # FPR diff = TNR diff\n",
    "\n",
    "        # Equalized Odds = max of TPR and FPR differences\n",
    "        equalized_odds = max(tpr_diff, fpr_diff)\n",
    "\n",
    "        # Equal Opportunity = TPR difference\n",
    "        equal_opportunity = tpr_diff\n",
    "\n",
    "        # BER Equality (now correctly calculated)\n",
    "        ber_equality = fairmetrics[\"BER Equality\"]\n",
    "\n",
    "        # Helper function to determine if we're looking at intersectional groups\n",
    "        def is_intersectional(group_name):\n",
    "            \"\"\"Check if group contains multiple attributes (e.g., 'Male_White')\"\"\"\n",
    "            return '_' in str(group_name)\n",
    "\n",
    "        # MODIFIED: Helper function to check if reference group exists\n",
    "        def get_reference_group(groups):\n",
    "            \"\"\"Find reference group based on sensitive variable type\"\"\"\n",
    "            # For intersectional (Sex_Race)\n",
    "            for group in groups:\n",
    "                if '@Male_@White' in str(group):\n",
    "                    return group\n",
    "\n",
    "            # For race only\n",
    "            for group in groups:\n",
    "                if '@White' in str(group) and '@Male' not in str(group):\n",
    "                    return group\n",
    "\n",
    "            # For sex only\n",
    "            for group in groups:\n",
    "                if '@Male' in str(group) and '@White' not in str(group):\n",
    "                    return group\n",
    "\n",
    "            return None\n",
    "\n",
    "        # Determine min type for Equalized Odds (which metric drives the disparity)\n",
    "        if tpr_diff >= fpr_diff:\n",
    "            eq_odds_min_type = \"TPR\"\n",
    "            eq_odds_values = by_group[\"tpr\"]\n",
    "        else:\n",
    "            eq_odds_min_type = \"FPR\"\n",
    "            eq_odds_values = 1 - by_group[\"tnr\"]\n",
    "\n",
    "        # Build disparity table with 3 rows\n",
    "        disparity_data = []\n",
    "\n",
    "        # Determine the appropriate column name based on group structure\n",
    "        groups = by_group.index.tolist()\n",
    "        is_intersect = is_intersectional(groups[0]) if len(groups) > 0 else False\n",
    "        group_col_name = \"Intersection\" if is_intersect else \"Group\"\n",
    "\n",
    "        # Check if reference group exists\n",
    "        reference_group = get_reference_group(groups)\n",
    "\n",
    "        # Row 1: Equalized Odds (max of TPR/FPR)\n",
    "        row1 = {\n",
    "            \"Metric\": \"Equalized Odds (max of TPR/FPR)\",\n",
    "            \"Min Value\": eq_odds_values.min(),\n",
    "            f\"Min {group_col_name}\": eq_odds_values.idxmin(),\n",
    "            \"Min Type\": eq_odds_min_type,\n",
    "            \"Max Value\": eq_odds_values.max(),\n",
    "            f\"Max {group_col_name}\": eq_odds_values.idxmax(),\n",
    "            \"Gap\": equalized_odds\n",
    "        }\n",
    "\n",
    "        # Add reference group if it exists and is not already min/max\n",
    "        if reference_group and reference_group not in [eq_odds_values.idxmin(), eq_odds_values.idxmax()]:\n",
    "            row1[f\"Reference {group_col_name}\"] = reference_group\n",
    "            row1[\"Reference Value\"] = eq_odds_values[reference_group]\n",
    "            row1[\"Reference Gap\"] = abs(eq_odds_values[reference_group] - eq_odds_values.min())\n",
    "        else:\n",
    "            row1[f\"Reference {group_col_name}\"] = None\n",
    "            row1[\"Reference Value\"] = None\n",
    "            row1[\"Reference Gap\"] = None\n",
    "\n",
    "        disparity_data.append(row1)\n",
    "\n",
    "        # Row 2: Equal Opportunity (TPR)\n",
    "        tpr_values = by_group[\"tpr\"]\n",
    "        row2 = {\n",
    "            \"Metric\": \"Equal Opportunity (TPR)\",\n",
    "            \"Min Value\": tpr_values.min(),\n",
    "            f\"Min {group_col_name}\": tpr_values.idxmin(),\n",
    "            \"Min Type\": \"TPR\",\n",
    "            \"Max Value\": tpr_values.max(),\n",
    "            f\"Max {group_col_name}\": tpr_values.idxmax(),\n",
    "            \"Gap\": equal_opportunity\n",
    "        }\n",
    "\n",
    "        # Add reference group if it exists and is not already min/max\n",
    "        if reference_group and reference_group not in [tpr_values.idxmin(), tpr_values.idxmax()]:\n",
    "            row2[f\"Reference {group_col_name}\"] = reference_group\n",
    "            row2[\"Reference Value\"] = tpr_values[reference_group]\n",
    "            row2[\"Reference Gap\"] = abs(tpr_values[reference_group] - tpr_values.min())\n",
    "        else:\n",
    "            row2[f\"Reference {group_col_name}\"] = None\n",
    "            row2[\"Reference Value\"] = None\n",
    "            row2[\"Reference Gap\"] = None\n",
    "\n",
    "        disparity_data.append(row2)\n",
    "\n",
    "        # Row 3: BER Equality (CORRECTED)\n",
    "        row3 = {\n",
    "            \"Metric\": \"BER Equality\",\n",
    "            \"Min Value\": ber_values.min(),\n",
    "            f\"Min {group_col_name}\": ber_values.idxmin(),\n",
    "            \"Min Type\": \"BER\",\n",
    "            \"Max Value\": ber_values.max(),\n",
    "            f\"Max {group_col_name}\": ber_values.idxmax(),\n",
    "            \"Gap\": ber_equality\n",
    "        }\n",
    "\n",
    "        # Add reference group if it exists and is not already min/max\n",
    "        if reference_group and reference_group not in [ber_values.idxmin(), ber_values.idxmax()]:\n",
    "            row3[f\"Reference {group_col_name}\"] = reference_group\n",
    "            row3[\"Reference Value\"] = ber_values[reference_group]\n",
    "            row3[\"Reference Gap\"] = abs(ber_values[reference_group] - ber_values.min())\n",
    "        else:\n",
    "            row3[f\"Reference {group_col_name}\"] = None\n",
    "            row3[\"Reference Value\"] = None\n",
    "            row3[\"Reference Gap\"] = None\n",
    "\n",
    "        disparity_data.append(row3)\n",
    "\n",
    "        self.disparity_table = pd.DataFrame(disparity_data)\n",
    "\n",
    "        # Round numeric columns to 4 decimals\n",
    "        numeric_cols = [\"Min Value\", \"Max Value\", \"Gap\"]\n",
    "        if \"Reference Value\" in self.disparity_table.columns:\n",
    "            numeric_cols.append(\"Reference Value\")\n",
    "        if \"Reference Gap\" in self.disparity_table.columns:\n",
    "            numeric_cols.append(\"Reference Gap\")\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if col in self.disparity_table.columns:\n",
    "                self.disparity_table[col] = self.disparity_table[col].round(4)\n",
    "\n",
    "        self.qc = pd.DataFrame([qc])\n",
    "\n",
    "    def _clametric_generation(self):\n",
    "        clametrics = {}\n",
    "        pred = self.y_pred if self.y_pred is not None else self.y_pred_bin\n",
    "\n",
    "        # Add sensitivity (TPR) and specificity (TNR)\n",
    "        clametrics[\"sensitivity\"] = recall_score(self.y_true, self.y_pred_bin)\n",
    "        clametrics[\"specificity\"] = true_negative_rate(self.y_true, self.y_pred_bin)\n",
    "\n",
    "        if \"auc\" in self.cla_metrics:\n",
    "            # clametrics[\"auc\"] = roc_auc_score(self.y_true, self.y_pred)\n",
    "            clametrics[\"auc_low\"], clametrics[\"auc\"], clametrics[\"auc_high\"] = (\n",
    "                get_ci_auc(self.y_true, pred, alpha=0.05, type=\"auc\")\n",
    "            )\n",
    "\n",
    "        self.clametrics = pd.DataFrame([clametrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJcE3tUlpA6m"
   },
   "source": [
    "##fairness_modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djWDrJNBRvnn"
   },
   "outputs": [],
   "source": [
    "class FAIMGenerator(FairBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dat_train,\n",
    "        selected_vars,\n",
    "        selected_vars_cat,\n",
    "        y_name,\n",
    "        sen_name,\n",
    "        sen_var_ref,\n",
    "        output_dir,\n",
    "        criterion=\"loss\",\n",
    "        epsilon=0.05,\n",
    "        m=800,\n",
    "        n_final=350,\n",
    "        without_sen=False,\n",
    "        pre=False,\n",
    "        pre_method=\"Reweigh\",\n",
    "        post=False,\n",
    "        post_method=\"equalizedodds\",\n",
    "        class_weight=\"balanced\",\n",
    "    ):\n",
    "        \"\"\"Initialize the class of FAIM\n",
    "\n",
    "        Args:\n",
    "            dat_train (data frame): the training data\n",
    "            selected_vars (list): the selected variables that include sensitive variables\n",
    "            selected_vars_cat (list): the selected categorical variables that include sensitive variables\n",
    "            y_name (str): the name of the label, e.g. \"y\", \"label\", etc.\n",
    "            sen_name (list): the name of the sensitive variables\n",
    "            sen_var_ref (dict): the reference values of the sensitive variables e.g. {\"gender\": \"F\"}\n",
    "            output_dir: the output directory to store the nearly optimal models results\n",
    "            criterion (str, optional): the criterion to generate nearly optimal models. Defaults to \"loss\".\n",
    "            epsilon (float, optional): the control factor of \"nearly optimality\", i.e. the gap to the optimal model. Defaults to 0.05.\n",
    "            without_sen (bool, optional): directly exclude the sensitive variables. Defaults to False.\n",
    "            pre (bool, optional): whether to use pre-process bias mitigation methods before FAIM. Defaults to False.\n",
    "            pre_method (str, optional): specific pre-process method. Defaults to \"Reweigh\".\n",
    "            post (bool, optional): whether to use post-process bias mitigation methods after FAIM. Defaults to False.\n",
    "            post_method (str, optional): specific post-process method. Defaults to \"EqOdds\".\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(\n",
    "            dat_train,\n",
    "            selected_vars,\n",
    "            selected_vars_cat,\n",
    "            y_name,\n",
    "            sen_name,\n",
    "            sen_var_ref,\n",
    "            without_sen,\n",
    "            class_weight=class_weight,\n",
    "        )\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.output_dir = output_dir\n",
    "        self.epsilon = epsilon\n",
    "        self.m = m\n",
    "        self.n_final = n_final\n",
    "\n",
    "        self.pre = pre\n",
    "        if pre:\n",
    "            self.pre_method = pre_method\n",
    "            self.rw_model, self.rw_results, self.rw_weights = self.pre_mitigate()\n",
    "            plt.hist(self.rw_weights)\n",
    "        if post:\n",
    "            self.post = post\n",
    "            self.post_method = post_method\n",
    "\n",
    "        self.optim_obj = self.optimal_model(selected_vars, selected_vars_cat)\n",
    "        self.optim_results = self.optim_obj.model_optim\n",
    "        self.optim_model = self.optim_obj.model_optim.model\n",
    "\n",
    "        self.dat_expl = None\n",
    "        self.dat_test = None\n",
    "\n",
    "    # def __reduce__(self):\n",
    "    #     return (self.__class__, (self.coefs, self.best_coef, self.best_optim_base_obj, self.best_sen_exclusion, self.fairmetrics_df))\n",
    "\n",
    "    def pre_mitigate(self):\n",
    "        \"\"\"Pre-process bias mitigation methods\"\"\"\n",
    "        rw_model, rw_results, instance_weights = self.model(\n",
    "            method_type=\"pre\", method=self.pre_method\n",
    "        )\n",
    "\n",
    "        return rw_model, rw_results, instance_weights\n",
    "\n",
    "    def optimal_model(self, selected_vars, selected_vars_cat):\n",
    "        \"\"\"Generate the optimal model\"\"\"\n",
    "        x = self.dat_train.drop(\n",
    "            columns=[\n",
    "                c\n",
    "                for c in self.dat_train.columns\n",
    "                if c == self.y_name or c not in selected_vars\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # ===== HANDLE THREE CASES =====\n",
    "\n",
    "        # Case 1: Using pre-processing fairness intervention (Reweighing)\n",
    "        if self.pre and self.pre_method == \"Reweigh\":\n",
    "            sample_weights = self.rw_weights  # Use Reweighing weights\n",
    "            print(\" Using Reweighing pre-processing weights (fairness intervention)\")\n",
    "\n",
    "        # Case 2: Using balanced class weights (for imbalance, not fairness)\n",
    "        elif self.class_weight == \"balanced\":\n",
    "            y_train = self.dat_train[self.y_name]\n",
    "            n_samples = len(y_train)\n",
    "            classes, counts = np.unique(y_train, return_counts=True)\n",
    "            class_weight_dict = {\n",
    "                cls: n_samples / (len(classes) * cnt)\n",
    "                for cls, cnt in zip(classes, counts)\n",
    "            }\n",
    "            sample_weights = np.array([class_weight_dict[cls] for cls in y_train])\n",
    "            print(f\" Using balanced class weights (for imbalance): {class_weight_dict}\")\n",
    "\n",
    "        # Case 3: No weighting\n",
    "        else:\n",
    "            sample_weights = None\n",
    "            print(\" No sample weighting\")\n",
    "\n",
    "        model_object = model.models(\n",
    "            x=x,\n",
    "            y=self.dat_train[self.y_name],\n",
    "            x_names_cat=selected_vars_cat,\n",
    "            output_dir=self.output_dir,\n",
    "            criterion=self.criterion,\n",
    "            sample_w=sample_weights\n",
    "        )\n",
    "\n",
    "        return model_object\n",
    "\n",
    "    def nearly_optimal_model(self, optim_base_obj, m=200, n_final=50, epsilon=None):\n",
    "        \"\"\"Generate the nearly optimal models\n",
    "\n",
    "        Args:\n",
    "            optim_base_obj (object): the object of the optimal model\n",
    "            m (int, optional): the number of models to be generated. Defaults to 800.\n",
    "            n_final (int, optional): the number of nearly optimal models to be generated. Defaults to 350.\n",
    "\n",
    "        Returns:\n",
    "            coefs (data frame): the coefficients of the nearly optimal models\n",
    "            plots (plot): the plot of the status nearly optimal models\n",
    "        \"\"\"\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        u1, u2 = optim_base_obj.init_hyper_params(m=m)\n",
    "        optim_base_obj.draw_models(\n",
    "            u1=u1,\n",
    "            u2=u2,\n",
    "            m=self.m,\n",
    "            n_final=self.n_final,\n",
    "            random_state=1234\n",
    "        )\n",
    "        coefs = pd.read_csv(\n",
    "            os.path.join(self.output_dir, \"models_near_optim.csv\"), index_col=0\n",
    "        )\n",
    "        return coefs, optim_base_obj.models_plot\n",
    "\n",
    "    def fairness_compute(\n",
    "        self,\n",
    "        dat_expl,\n",
    "        optim_base_obj,\n",
    "        coefs,\n",
    "        weighted=True,\n",
    "        weights={\"tnr\": 0.5, \"tpr\": 0.5},\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Compute the fairness metrics of the nearly optimal models\n",
    "\n",
    "        Args:\n",
    "            dat_expl (data frame): the data frame of the validation data\n",
    "            optim_base_obj (object): the object of the optimal model\n",
    "            coefs (data frame): the coefficients of the nearly optimal models\n",
    "            weighted (bool, optional): whether to use weighted fairness metrics. Defaults to True.\n",
    "            weights (dict, optional): the weights of the weighted fairness metrics. Defaults to {\"tnr\": 0.5, \"tpr\": 0.5}.\n",
    "            **kwargs: the other parameters of the fairness computation\n",
    "\n",
    "        Returns:\n",
    "            fairmetrics_df (data frame): the fairness metrics of the nearly optimal models\n",
    "            qc_df (data frame): the quality control results of the nearly optimal models\n",
    "        \"\"\"\n",
    "        if weighted:\n",
    "            self.weighted = weighted\n",
    "            self.weights = weights\n",
    "        self.dat_expl = dat_expl\n",
    "\n",
    "        optim_base_results = optim_base_obj.model_optim\n",
    "        optim_base_model = optim_base_obj.model_optim.model\n",
    "\n",
    "        fairmetrics_df = []\n",
    "        qc_df = []\n",
    "        by_group_list = []\n",
    "\n",
    "        for i in range(coefs.shape[0]):\n",
    "            coef = coefs.drop(columns=[\"perf_metric\"]).iloc[i, :]\n",
    "            x_with_constant, sen_var, y_expl = self.data_process(dat_expl, **kwargs)\n",
    "\n",
    "            # sen_var = dat_expl[self.sen_name]\n",
    "            optim_base_results.params = coef\n",
    "\n",
    "            col_train = optim_base_results.params.index\n",
    "            col_test = x_with_constant.columns\n",
    "            x_with_constant = col_gap(col_train, col_test, x_with_constant)\n",
    "\n",
    "            prob_expl = optim_base_model.predict(params=coef, exog=x_with_constant)\n",
    "            thresh = find_optimal_cutoff(y_expl, prob_expl)[0]\n",
    "            pred_expl = prob_expl > thresh\n",
    "\n",
    "            fe = FAIMEvaluator(\n",
    "                y_true=y_expl,\n",
    "                y_pred=prob_expl,\n",
    "                y_pred_bin=pred_expl,\n",
    "                sen_var=sen_var,\n",
    "                fair_only=True,\n",
    "                weighted=weighted,\n",
    "                weights=weights,\n",
    "            )\n",
    "            fairmetrics = fe.fairmetrics\n",
    "            qc = fe.qc\n",
    "\n",
    "            fairmetrics_df.append(fairmetrics)\n",
    "            qc_df.append(qc)\n",
    "            by_group_list.append(fe.fairsummary)\n",
    "\n",
    "        fairmetrics_df = pd.concat(fairmetrics_df).reset_index(drop=True)\n",
    "        qc_df = pd.concat(qc_df)\n",
    "\n",
    "        return fairmetrics_df, qc_df\n",
    "        # self.thresh_list = thresh_list\n",
    "\n",
    "    def compare(self, dat_expl, optim_base_results, selected_vars, selected_vars_cat):\n",
    "        \"\"\"Compare the cases of exclusion of sensitive variables with the original optimal model i.e. no exclusion of sensitive variables\n",
    "\n",
    "        Args:\n",
    "            dat_expl (data frame): the data frame of the validation data\n",
    "            optim_base_results (object): the object of the optimal model regarding the specific case of exclusion of sensitive variables\n",
    "            selected_vars (list): the selected variables that can include sensitive variables\n",
    "            selected_vars_cat (list): the selected categorical variables that can include sensitive variables\n",
    "\n",
    "        Returns:\n",
    "            bool: whether the case of exclusion of sensitive variables will be expanded to the nearly optimal models\n",
    "\n",
    "        \"\"\"\n",
    "        x_with_constant_ori, sen_var, y_expl = self.data_process(\n",
    "            dat_expl, selected_vars=self.vars, selected_vars_cat=self.vars_cat\n",
    "        )\n",
    "        x_with_constant_base, sen_var, y_expl = self.data_process(\n",
    "            dat_expl, selected_vars=selected_vars, selected_vars_cat=selected_vars_cat\n",
    "        )\n",
    "        pred_ori = self.optim_results.predict(x_with_constant_ori)\n",
    "        pred_base = optim_base_results.predict(x_with_constant_base)\n",
    "\n",
    "        if self.criterion == \"auc\":\n",
    "            auc_ori = roc_auc_score(y_expl, pred_ori)\n",
    "            auc_base = roc_auc_score(y_expl, pred_base)\n",
    "\n",
    "            # return auc_base > auc_ori * (1-np.sqrt(self.epsilon))\n",
    "            return auc_base > auc_ori * np.sqrt(1 - self.epsilon)\n",
    "        if self.criterion == \"loss\":\n",
    "            loss_ori = self.optim_model.loglike(self.optim_results.params)\n",
    "            loss_base = optim_base_results.model.loglike(optim_base_results.params)\n",
    "            ratio = loss_base / loss_ori\n",
    "            print(f\"loss_ori: {loss_ori}, loss_base: {loss_base}, ratio: {ratio}\")\n",
    "            return ratio < np.sqrt(1 + self.epsilon)\n",
    "\n",
    "    def FAIM_model(self, dat_expl):\n",
    "        \"\"\"FAIM: Generate the nearly optimal models and compute the fairness metrics of the nearly optimal models\n",
    "\n",
    "        Args:\n",
    "            dat_expl (data frame): the data frame of the validation data\n",
    "        \"\"\"\n",
    "        self.dat_expl = dat_expl\n",
    "\n",
    "        self.coefs = {}\n",
    "        self.plots = {}\n",
    "        self.optim_base_obj_list = {}\n",
    "        self.fairmetrics_df = pd.DataFrame()\n",
    "        self.qc_df = pd.DataFrame()\n",
    "\n",
    "        if self.without_sen == \"auto\":\n",
    "            sen_senarios = generate_subsets(self.sen_name)\n",
    "            pbar = tqdm(\n",
    "                sen_senarios, desc=\"Generating nearly optimal models\", postfix=\"*Start*\"\n",
    "            )\n",
    "            for x in pbar:\n",
    "                pbar.set_postfix(postfix=f\"exclusion: {x}\")\n",
    "\n",
    "                selected_vars = [i for i in self.vars if i not in x]\n",
    "                selected_vars_cat = [i for i in self.vars_cat if i not in x]\n",
    "                optim_base_obj = self.optimal_model(selected_vars, selected_vars_cat)\n",
    "                optim_base_results = optim_base_obj.model_optim\n",
    "\n",
    "                if self.compare(\n",
    "                    dat_expl, optim_base_results, selected_vars, selected_vars_cat\n",
    "                ):\n",
    "                    self.optim_base_obj_list[\"_\".join(x)] = optim_base_obj\n",
    "\n",
    "                    if self.criterion == \"auc\":\n",
    "                        epsilon = 1 - np.sqrt(1 - self.epsilon)\n",
    "                    elif self.criterion == \"loss\":\n",
    "                        epsilon = np.sqrt(1 + self.epsilon) - 1\n",
    "\n",
    "                    coefs, plots = self.nearly_optimal_model(\n",
    "                        optim_base_obj, n_final=self.n_final, epsilon=epsilon\n",
    "                    )\n",
    "                    self.coefs[\"_\".join(x)] = coefs\n",
    "                    self.plots[\"_\".join(x)] = plots\n",
    "                    fairmetrics_df, qc_df = self.fairness_compute(\n",
    "                        dat_expl,\n",
    "                        optim_base_obj,\n",
    "                        coefs,\n",
    "                        selected_vars=selected_vars,\n",
    "                        selected_vars_cat=selected_vars_cat,\n",
    "                    )\n",
    "                    fairmetrics_df[\"auc\"] = coefs[\"perf_metric\"]\n",
    "                    qc_df[\"auc\"] = coefs[\"perf_metric\"]\n",
    "\n",
    "                    fairmetrics_df[\"sen_var_exclusion\"] = \"_\".join(x)\n",
    "                    qc_df[\"sen_var_exclusion\"] = \"_\".join(x)\n",
    "                    self.fairmetrics_df = pd.concat(\n",
    "                        [self.fairmetrics_df, fairmetrics_df]\n",
    "                    )\n",
    "                    self.qc_df = pd.concat([self.qc_df, qc_df])\n",
    "\n",
    "                else:\n",
    "                    print(f\"Exclusion of {x} degrades the discrimination performance!\")\n",
    "\n",
    "            self.fairmetrics_df = self.fairmetrics_df.reset_index(drop=True)\n",
    "            self.qc_df = self.qc_df.reset_index(drop=True)\n",
    "\n",
    "            # return fairmetrics_df, qc_df\n",
    "\n",
    "    def describe(self, selected_metrics=None):\n",
    "        \"\"\"Describe the distribution of fairness metrics for all nearly optimal models\n",
    "        Args:\n",
    "            selected_metrics (list, optional): the selected fairness metrics, e.g. [\"Statistical Parity\", \"Equalized Odds\", \"Average Accuracy\"]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            fig: the plot of the distribution of fairness metrics for all nearly optimal models\n",
    "        \"\"\"\n",
    "        sen_var_exclusion = self.fairmetrics_df[\"sen_var_exclusion\"]\n",
    "        auc_var = self.fairmetrics_df[\"auc\"]\n",
    "        fairmetrics_df = self.fairmetrics_df.drop(columns=[\"sen_var_exclusion\"])\n",
    "        qc_df = self.qc_df.drop(columns=[\"sen_var_exclusion\", \"auc\"])\n",
    "\n",
    "        if selected_metrics is None:\n",
    "            num_metrics = fairmetrics_df.shape[1]\n",
    "        else:\n",
    "            for m in selected_metrics:\n",
    "                if m not in fairmetrics_df.columns:\n",
    "                    raise ValueError(f\"The metric {m} is not in the fairness metrics!\")\n",
    "\n",
    "            qc_df = qc_df[selected_metrics]\n",
    "            fairmetrics_df = fairmetrics_df[selected_metrics]\n",
    "            num_metrics = len(selected_metrics)\n",
    "\n",
    "        ids_after_qc = np.arange(qc_df.shape[0])[\n",
    "            (np.sum(qc_df, 1) == qc_df.shape[1]).tolist()\n",
    "        ]\n",
    "        print(f\"{len(ids_after_qc)} are qualified after quality control\")\n",
    "\n",
    "        min_ones = fairmetrics_df.iloc[ids_after_qc, :].apply(axis=0, func=np.argmin)\n",
    "\n",
    "        for m in min_ones.index:\n",
    "            id = fairmetrics_df.iloc[ids_after_qc, :].index[min_ones[m]]\n",
    "            print(\n",
    "                f\"the model with minimal {m}: No.{id} -- {fairmetrics_df.loc[id, m]:.3f}, with {sen_var_exclusion[id]} excluded from regression\"\n",
    "            )\n",
    "\n",
    "        plot_df = self.fairmetrics_df.loc[ids_after_qc, :]\n",
    "        fig = plot_distribution(plot_df)\n",
    "\n",
    "        if len(ids_after_qc) < fairmetrics_df.shape[0]:\n",
    "            return (\n",
    "                fairmetrics_df.iloc[ids_after_qc, :],\n",
    "                [sen_var_exclusion[i] for i in ids_after_qc],\n",
    "                fig,\n",
    "            )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def transmit(\n",
    "        self,\n",
    "        targeted_metrics=[\"Average Accuracy\", \"Statistical Parity\", \"Equalized Odds\"],\n",
    "        thresh_show=0.3,\n",
    "        best_id=None,\n",
    "        best_sen_exclusion=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Select the best model regarding fairness\n",
    "\n",
    "        Args:\n",
    "            targeted_metrics (list, optional): the targeted fairness metrics. Defaults to [\"Average Accuracy\", \"Statistical Parity\", \"Equalized Odds\"].\n",
    "            thresh_show (float, optional): the threshold to filter the models. Defaults to 0.3.\n",
    "\n",
    "        Returns:\n",
    "            best_coef: the coefficients of the best model\n",
    "            best_sen_exclusion: the sensitive variables excluded from the best model\n",
    "            best_optim_base_obj: the object of the best model\n",
    "            p: the radar plot of the distribution of fairness metrics for all nearly optimal models\n",
    "        \"\"\"\n",
    "        FAIM_area_list = []\n",
    "        ids = np.sum(\n",
    "            self.fairmetrics_df[targeted_metrics] < thresh_show, axis=1\n",
    "        ) == len(targeted_metrics)\n",
    "        if len(ids) == 0:\n",
    "            raise ValueError(\"The thresh is too low!\")\n",
    "        fairmetrics_df = self.fairmetrics_df.loc[ids, :]\n",
    "\n",
    "        sen_var_exclusion = fairmetrics_df[\"sen_var_exclusion\"]\n",
    "        perf = fairmetrics_df[\"auc\"]\n",
    "        df = fairmetrics_df.drop(columns=[\"sen_var_exclusion\"])\n",
    "        df = df[targeted_metrics]\n",
    "\n",
    "        print(f\"There are {df.shape[0]} models for final fairness selection.\")\n",
    "\n",
    "        if \"title\" in kwargs.keys():\n",
    "            title = kwargs[\"title\"]\n",
    "        else:\n",
    "            title = None\n",
    "        p_radar = plot_radar(df, thresh_show=thresh_show, title=title)\n",
    "        # p_radar.show()\n",
    "        p, fair_idx_df = plot_scatter(df, perf, sen_var_exclusion, title=title)\n",
    "        self.p = p\n",
    "        p.show()\n",
    "\n",
    "        for i, id in enumerate(df.index):\n",
    "            values = df.loc[id, :]\n",
    "            FAIM_area_list.append(fairarea(values))\n",
    "        ranking = np.argsort(np.argsort(FAIM_area_list))\n",
    "\n",
    "        if best_id is not None:\n",
    "            assert best_sen_exclusion is not None\n",
    "            self.best_id = best_id\n",
    "            self.best_sen_exclusion = best_sen_exclusion\n",
    "        else:\n",
    "            self.best_id = df.index[np.where(ranking == 0)][0]\n",
    "            self.best_sen_exclusion = sen_var_exclusion.iloc[np.argmin(FAIM_area_list)]\n",
    "\n",
    "        id_senario = [\n",
    "            index\n",
    "            for index, item in enumerate(list(self.coefs.keys()))\n",
    "            if item == self.best_sen_exclusion\n",
    "        ][0]\n",
    "\n",
    "        self.best_coef = (\n",
    "            self.coefs[self.best_sen_exclusion]\n",
    "            .drop(columns=[\"perf_metric\"])\n",
    "            .loc[self.best_id - self.n_final * id_senario, :]\n",
    "        )\n",
    "        self.best_optim_base_obj = self.optim_base_obj_list[self.best_sen_exclusion]\n",
    "\n",
    "        # confidence interval\n",
    "        dat_uncertainty = self.dat_train.sample(\n",
    "            n=np.min([50000, self.dat_train.shape[0]]), random_state=42\n",
    "        )\n",
    "        excluded_vars = self.best_sen_exclusion.split(\"_\")\n",
    "        selected_vars = [i for i in self.vars if i not in excluded_vars]\n",
    "        selected_vars_cat = [i for i in self.vars_cat if i not in excluded_vars]\n",
    "        x_with_constant = self.data_process(\n",
    "            dat_uncertainty,\n",
    "            selected_vars=selected_vars,\n",
    "            selected_vars_cat=selected_vars_cat,\n",
    "        )[0].values\n",
    "\n",
    "        prob_train, _, _ = self.test(dat_uncertainty)\n",
    "        best_se = None\n",
    "        fisher_information = (\n",
    "            x_with_constant.T @ np.diag(prob_train * (1 - prob_train)) @ x_with_constant\n",
    "        )\n",
    "        print(\"multiplication successed!\")\n",
    "        cov = np.linalg.pinv(fisher_information)\n",
    "        best_se = [np.sqrt(cov[i, i]) for i in range(cov.shape[0])]\n",
    "\n",
    "        # self.best_thresh = self.thresh_list[self.best_id]\n",
    "        best_results = {\n",
    "            \"best_coef\": self.best_coef,\n",
    "            \"best_sen_exclusion\": self.best_sen_exclusion,\n",
    "            \"best_se\": best_se,\n",
    "            \"best_optim_base_obj\": self.best_optim_base_obj,\n",
    "        }\n",
    "\n",
    "        return best_results, fair_idx_df\n",
    "\n",
    "    def post_mitigate(self):\n",
    "        pass\n",
    "\n",
    "    def test(self, dat_test, model=None, params=None, thresh=None):\n",
    "        \"\"\"Test the best model regarding fairness\n",
    "\n",
    "        Args:\n",
    "            dat_test (data frame): the data frame of the test data\n",
    "            model (object, optional): the object of the model to be tested. Defaults to None.\n",
    "            params (optional): the parameters of the model to be tested. Defaults to None.\n",
    "            thresh (optional): the threshold of the predictions. Defaults to None.\n",
    "\n",
    "        Methods:\n",
    "        +--------------+------------+----------------------------------------+\n",
    "        | Model        | Params     | Description                            |\n",
    "        +--------------+------------+----------------------------------------+\n",
    "        | None         | None       | Test the best model produced by FAIM.  |\n",
    "        +--------------+------------+----------------------------------------+\n",
    "        | model results| None       | Test the provided model with parameters|\n",
    "        |              |            | embedded.                              |\n",
    "        +--------------+------------+----------------------------------------+\n",
    "        | model results| as required| Test the provided model with the       |\n",
    "        |              |            | parameters additionally provided.      |\n",
    "        +--------------+------------+----------------------------------------+\n",
    "\n",
    "        Returns:\n",
    "            prob_test: the predicted probabilities of the test data\n",
    "            fairmetrics: the fairness metrics of the test data\n",
    "            fairsummary: the fairness summary of the test data for each subgroup\n",
    "\n",
    "        \"\"\"\n",
    "        self.dat_test = dat_test\n",
    "        excluded_vars = self.best_sen_exclusion.split(\"_\")\n",
    "        selected_vars = [i for i in self.vars if i not in excluded_vars]\n",
    "        selected_vars_cat = [i for i in self.vars_cat if i not in excluded_vars]\n",
    "        x_with_constant, sen_var, y_test = self.data_process(\n",
    "            dat_test, selected_vars=selected_vars, selected_vars_cat=selected_vars_cat\n",
    "        )\n",
    "\n",
    "        if model is None:\n",
    "            prob_test = self.best_optim_base_obj.model_optim.model.predict(\n",
    "                params=self.best_coef, exog=x_with_constant\n",
    "            )\n",
    "        else:\n",
    "            if isinstance(model, type(self.optim_results)) and params is None:\n",
    "                prob_test = model.predict(exog=x_with_constant)\n",
    "            elif isinstance(model, type(self.optim_model)) and params is not None:\n",
    "                prob_test = model.predict(params=params, exog=x_with_constant)\n",
    "            else:\n",
    "                raise ValueError(\"Please provide the right model!\")\n",
    "\n",
    "        thresh = find_optimal_cutoff(y_test, prob_test)[0]\n",
    "        pred_test = prob_test > thresh\n",
    "        fe = FAIMEvaluator(\n",
    "            y_true=np.array(y_test),\n",
    "            y_pred_bin=pred_test,\n",
    "            y_pred=prob_test,\n",
    "            sen_var=sen_var,\n",
    "            weighted=self.weighted,\n",
    "            weights=self.weights,\n",
    "        )\n",
    "        fairmetrics = fe.fairmetrics\n",
    "        clametrics = fe.clametrics\n",
    "\n",
    "        return prob_test, fairmetrics, clametrics\n",
    "\n",
    "\n",
    "    def explain(self, method=\"best\"):\n",
    "        \"\"\"Compute SHAP values for FAIM (best) or baseline (ori) model.\"\"\"\n",
    "\n",
    "        import shap\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import os\n",
    "\n",
    "        # Choose model + coefficients\n",
    "        if method == \"best\":\n",
    "            coef = self.best_coef\n",
    "            excluded = set(self.best_sen_exclusion.split(\"_\"))\n",
    "            used_vars = [v for v in self.vars if v not in excluded]\n",
    "            used_vars_cat = [v for v in self.vars_cat if v not in excluded]\n",
    "            model = self.best_optim_base_obj.model_optim.model  # 18 features\n",
    "        else:  # method == \"ori\"\n",
    "            coef = self.optim_results.params\n",
    "            used_vars = self.vars\n",
    "            used_vars_cat = self.vars_cat\n",
    "            model = self.optim_model  # <-- FIX: Use the ORIGINAL model (23 features)\n",
    "\n",
    "        model_cols = model.exog_names  # Get the correct columns for THIS model\n",
    "\n",
    "        # ---- Build background data ----\n",
    "        bg_full = self.data_process(\n",
    "            self.dat_train,\n",
    "            selected_vars=used_vars,\n",
    "            selected_vars_cat=used_vars_cat,\n",
    "        )[0]\n",
    "\n",
    "        # IMPORTANT: reduce to model columns BEFORE kmeans\n",
    "        bg_full = bg_full[model_cols]\n",
    "\n",
    "        # Now safe to summarize\n",
    "        bg_data = shap.kmeans(bg_full, k=50)\n",
    "\n",
    "        # ---- Build explain data ----\n",
    "        ex_full = self.data_process(\n",
    "            self.dat_expl,\n",
    "            selected_vars=used_vars,\n",
    "            selected_vars_cat=used_vars_cat,\n",
    "        )[0]\n",
    "\n",
    "        # Also reduce ex_data to model columns\n",
    "        ex_data = ex_full[model_cols].sample(n=200, random_state=42)\n",
    "\n",
    "        # ---- Model function that ALWAYS uses correct columns ----\n",
    "        def f(X):\n",
    "            if not isinstance(X, pd.DataFrame):\n",
    "                X = pd.DataFrame(X, columns=model_cols)\n",
    "            return model.predict(params=coef, exog=X)\n",
    "\n",
    "        # ---- Run SHAP ----\n",
    "        explainer = shap.KernelExplainer(f, bg_data)\n",
    "        shap_vals = explainer.shap_values(ex_data)\n",
    "\n",
    "        # KernelExplainer output may be list\n",
    "        shap_vals = shap_vals[1] if isinstance(shap_vals, list) else shap_vals\n",
    "\n",
    "        # ---- Save ----\n",
    "        output_dir = os.path.join(self.output_dir, \"explain\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        pd.DataFrame(shap_vals, columns=model_cols).to_csv(\n",
    "            os.path.join(output_dir, f\"{method}.csv\")\n",
    "        )\n",
    "\n",
    "        return shap_vals\n",
    "\n",
    "        def f(bg):\n",
    "            model = self.best_optim_base_obj.model_optim.model\n",
    "            if method == \"best\":\n",
    "                return model.predict(params=self.best_coef, exog=bg)\n",
    "            else:\n",
    "                return model.predict(params=self.optim_results.params, exog=bg)\n",
    "\n",
    "        output_dir = os.path.join(self.output_dir, \"explain\")\n",
    "        if method == \"best\":\n",
    "            excluded_vars = self.best_sen_exclusion.split(\"_\")\n",
    "            selected_vars = [i for i in self.vars if i not in excluded_vars]\n",
    "            selected_vars_cat = [i for i in self.vars_cat if i not in excluded_vars]\n",
    "\n",
    "        else:\n",
    "            selected_vars = self.vars\n",
    "            selected_vars_cat = self.vars_cat\n",
    "\n",
    "        bg_data = self.data_process(\n",
    "            self.dat_train,\n",
    "            selected_vars=selected_vars,\n",
    "            selected_vars_cat=selected_vars_cat,\n",
    "        )[0].sample(n=1000, random_state=42)\n",
    "        ex_data = self.data_process(\n",
    "            self.dat_expl,\n",
    "            selected_vars=selected_vars,\n",
    "            selected_vars_cat=selected_vars_cat,\n",
    "        )[0].sample(n=200, random_state=42)\n",
    "\n",
    "        e = shap.KernelExplainer(f, bg_data)\n",
    "        shap_values_train = e.shap_values(ex_data)\n",
    "        shap_values_train_1 = shap_values_train[1].squeeze()\n",
    "        pd.DataFrame(shap_values_train_1).to_csv(\n",
    "            os.path.join(output_dir, f\"{method}.csv\")\n",
    "        )\n",
    "\n",
    "        return shap_values_train_1\n",
    "\n",
    "\n",
    "    def compare_explain(self, overide=True, top_n=None):\n",
    "        \"\"\"Compare the SHAP values of the best model and original model\"\"\"\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        import textwrap\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        def add_bar_labels(ax, values, fmt=\"{:.4f}\", padding=3):\n",
    "            \"\"\"\n",
    "            Add numeric labels to horizontal bar plots.\n",
    "            \"\"\"\n",
    "            for i, v in enumerate(values):\n",
    "                ax.text(\n",
    "                    v,\n",
    "                    i,\n",
    "                    fmt.format(v),\n",
    "                    va=\"center\",\n",
    "                    ha=\"left\",\n",
    "                    fontsize=10\n",
    "                )\n",
    "        def clean_spines(ax):\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "            ax.spines[\"left\"].set_visible(True)\n",
    "            ax.spines[\"bottom\"].set_visible(True)\n",
    "\n",
    "        output_dir = os.path.join(self.output_dir, \"explain\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # ---------- LOAD SHAP VALUES ----------\n",
    "        if (not os.path.exists(os.path.join(output_dir, \"best.csv\"))) or overide:\n",
    "            shap_best = self.explain(method=\"best\")\n",
    "        else:\n",
    "            shap_best = pd.read_csv(\n",
    "                os.path.join(output_dir, \"best.csv\"), index_col=0\n",
    "            ).values  # Don't reshape - keep as 2D\n",
    "\n",
    "        if (not os.path.exists(os.path.join(output_dir, \"ori.csv\"))) or overide:\n",
    "            shap_ori = self.explain(method=\"ori\")\n",
    "        else:\n",
    "            shap_ori = pd.read_csv(\n",
    "                os.path.join(output_dir, \"ori.csv\"), index_col=0\n",
    "            ).values  # Don't reshape - keep as 2D\n",
    "\n",
    "        # ---------- AGGREGATE SHAP VALUES (mean absolute) ----------\n",
    "        # Take mean absolute SHAP value across all samples for each feature\n",
    "        shap_best_agg = np.mean(np.abs(shap_best), axis=0)\n",
    "        shap_ori_agg = np.mean(np.abs(shap_ori), axis=0)\n",
    "\n",
    "        # ---------- SORT + SELECT TOP FEATURES ----------\n",
    "        features_best = self.best_coef.index\n",
    "        features_ori = self.optim_results.params.index\n",
    "\n",
    "        # If top_n is None, use all features\n",
    "        n_best = len(features_best) if top_n is None else top_n\n",
    "        n_ori = len(features_ori) if top_n is None else top_n\n",
    "\n",
    "        shap_best_df = (\n",
    "            pd.DataFrame({\"feature\": features_best, \"shap\": shap_best_agg})\n",
    "            .sort_values(\"shap\", ascending=False)\n",
    "            .head(n_best)\n",
    "        )\n",
    "\n",
    "        shap_ori_df = (\n",
    "            pd.DataFrame({\"feature\": features_ori, \"shap\": shap_ori_agg})\n",
    "            .sort_values(\"shap\", ascending=False)\n",
    "            .head(n_ori)\n",
    "        )\n",
    "\n",
    "        # ---------- CLEAN LABELS ----------\n",
    "        def clean_label(lbl, max_len=40):\n",
    "            lbl = lbl.replace(\"_\", \" \")\n",
    "            if len(lbl) > max_len:\n",
    "                return lbl[:max_len] + \"...\"\n",
    "            return lbl\n",
    "\n",
    "        shap_best_df[\"feature\"] = shap_best_df[\"feature\"].apply(clean_label)\n",
    "        shap_ori_df[\"feature\"] = shap_ori_df[\"feature\"].apply(clean_label)\n",
    "\n",
    "        # ---------- MAKE PLOTS ----------\n",
    "        fig, axes = plt.subplots(\n",
    "            1,\n",
    "            2,\n",
    "            figsize=(18, 10),\n",
    "            sharex=False\n",
    "        )\n",
    "\n",
    "        # BEST MODEL\n",
    "        axes[0].barh(\n",
    "            shap_best_df[\"feature\"],\n",
    "            shap_best_df[\"shap\"]\n",
    "        )\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\n",
    "            \"Fairness-aware model (FAIM) - Top SHAP features\",\n",
    "            fontsize=16,\n",
    "            weight=\"bold\"\n",
    "        )\n",
    "        axes[0].set_xlabel(\"Mean |SHAP value|\", fontsize=12)\n",
    "        add_bar_labels(axes[0], shap_best_df[\"shap\"].values)\n",
    "        axes[0].tick_params(axis=\"y\", labelsize=11)\n",
    "        clean_spines(axes[0])\n",
    "\n",
    "        # ORIGINAL MODEL\n",
    "        axes[1].barh(\n",
    "            shap_ori_df[\"feature\"],\n",
    "            shap_ori_df[\"shap\"],\n",
    "            color='darkorange'\n",
    "        )\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\n",
    "            \"Fairness-unaware model (Baseline) - Top SHAP features\",\n",
    "            fontsize=16,\n",
    "            weight=\"bold\"\n",
    "        )\n",
    "        axes[1].set_xlabel(\"Mean |SHAP value|\", fontsize=12)\n",
    "        add_bar_labels(axes[1], shap_ori_df[\"shap\"].values)\n",
    "        axes[1].tick_params(axis=\"y\", labelsize=11)\n",
    "        clean_spines(axes[1])\n",
    "\n",
    "        # CLEAN LAYOUT\n",
    "        plt.tight_layout(pad=4)\n",
    "        plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "        # SAVE TOO\n",
    "        plt.savefig(os.path.join(output_dir, \"shap_compare.png\"), dpi=300)\n",
    "\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SX-HVJjpGib"
   },
   "source": [
    "##fairness_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuI0pd2PwSBD"
   },
   "outputs": [],
   "source": [
    "def rgb01_hex(col):\n",
    "    col_hex = [round(i * 255) for i in col]\n",
    "    col_hex = \"#%02x%02x%02x\" % tuple(col_hex)\n",
    "    return col_hex\n",
    "\n",
    "\n",
    "def compute_area(fairness_metrics):\n",
    "    n_metric = len(fairness_metrics)\n",
    "    tmp = fairness_metrics.values.flatten().tolist()\n",
    "    tmp_1 = tmp[1:] + tmp[:1]\n",
    "\n",
    "    if n_metric > 2:\n",
    "        theta_c = 2 * np.pi / n_metric\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
    "    elif n_metric == 2:\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
    "    else:\n",
    "        area = np.abs(tmp[0])\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def plot_perf_metric(\n",
    "    perf_metric, eligible, x_range, select=None, plot_selected=False, x_breaks=None\n",
    "):\n",
    "    \"\"\" Plot performance metrics of sampled models\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        perf_metric : numpy.array or pandas.Series\n",
    "            Numeric vector of performance metrics for all sampled models\n",
    "        eligible : numpy.array or pandas.Series\n",
    "            Boolean vector of the same length of 'perf_metric', indicating \\\n",
    "                whether each sample is eligible.\n",
    "        x_range : list\n",
    "            Numeric vector indicating the range of eligible values for \\\n",
    "                performance metrics.\n",
    "            Will be indicated by dotted vertical lines in plots.\n",
    "        select : list or numpy.array, optional (default: None)\n",
    "            Numeric vector of indexes of 'perf_metric' to be selected\n",
    "        plot_selected : bool, optional (default: False)\n",
    "            Whether performance metrics of selected models should be plotted in \\\n",
    "                a secondary figure.\n",
    "        x_breaks : list, optional (default: None)\n",
    "            If selected models are to be plotted, the breaks to use in the \\\n",
    "                histogram\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        plot : plotnine.ggplot\n",
    "            Histogram(s) of model performance made using ggplot\n",
    "    \"\"\"\n",
    "    m = len(perf_metric)\n",
    "    perf_df = pd.DataFrame(perf_metric, columns=[\"perf_metric\"], index=None)\n",
    "    plot = (\n",
    "        pn.ggplot(perf_df, pn.aes(x=\"perf_metric\"))\n",
    "        + pn.geoms.geom_histogram(\n",
    "            breaks=np.linspace(np.min(perf_metric), np.max(perf_metric), 40)\n",
    "        )\n",
    "        + pn.geoms.geom_vline(xintercept=x_range, linetype=\"dashed\", size=0.7)\n",
    "        + pn.labels.labs(\n",
    "            x=\"Ratio of loss to minimum loss\",\n",
    "            title=\"\"\"Loss of {m:d} sampled models\n",
    "                \\n{n_elg:d} ({per_elg:.1f}%) sampled models are eligible\"\"\".format(\n",
    "                m=m, n_elg=np.sum(eligible), per_elg=np.sum(eligible) * 100 / m\n",
    "            ),\n",
    "        )\n",
    "        + pn.themes.theme_bw()\n",
    "        + pn.themes.theme(\n",
    "            title=pn.themes.element_text(ha=\"left\"),\n",
    "            axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
    "            axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
    "        )\n",
    "    )\n",
    "    if plot_selected:\n",
    "        if select is None:\n",
    "            print(\"'select' vector is not specified!\\nUsing all models instead\")\n",
    "            select = [i for i in range(len(perf_df))]\n",
    "        try:\n",
    "            perf_select = perf_df.iloc[select]\n",
    "        except:\n",
    "            print(\n",
    "                \"Invalid indexes detected in 'select' vector!\\nUsing all models instead\"\n",
    "            )\n",
    "            select = [i for i in range(len(perf_df))]\n",
    "            perf_select = perf_df.iloc[select]\n",
    "        plot2 = (\n",
    "            pn.ggplot(perf_select, pn.aes(x=\"perf_metric\"))\n",
    "            + pn.geoms.geom_histogram(breaks=x_breaks)\n",
    "            + pn.labels.labs(\n",
    "                x=\"Ratio of loss to minimum loss\",\n",
    "                title=\"{n_select:d} selected models\".format(n_select=len(select)),\n",
    "            )\n",
    "            + pn.themes.theme_bw()\n",
    "            + pn.themes.theme(\n",
    "                title=pn.themes.element_text(ha=\"left\"),\n",
    "                axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
    "                axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
    "            )\n",
    "        )\n",
    "        return (plot, plot2)\n",
    "    else:\n",
    "        return plot\n",
    "\n",
    "\n",
    "def plot_distribution(df, s=4):\n",
    "    num_metrics = df.shape[1] - 2\n",
    "    labels = df.sen_var_exclusion.unique()\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == \"\":\n",
    "            labels[i] = \"No exclusion\"\n",
    "        elif len(labels[i].split(\"_\")) == 2:\n",
    "            labels[i] = f\"Exclusion of {' and '.join(labels[i].split('_'))}\"\n",
    "        elif len(labels[i].split(\"_\")) > 2:\n",
    "            sens = labels[i].split(\"_\")\n",
    "            labels[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
    "        else:\n",
    "            labels[i] = f\"Exclusion of {labels[i]}\"\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(s * num_metrics, s))\n",
    "    for i, x in enumerate(df.columns[:-2]):\n",
    "        ax = axes[i]\n",
    "        # sns.jointplot(data=df, x=x, y=\"auc\", hue=\"sen_var_exclusion\",  ax=ax, legend=False)\n",
    "        sns.histplot(\n",
    "            data=df, x=x, hue=\"sen_var_exclusion\", bins=50, ax=ax, legend=False\n",
    "        )  # layout=(1, num_metrics), figsize=(4, 4), color=\"#595959\",\n",
    "        ax.set_title(x)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Count\" if i == 0 else \"\")\n",
    "\n",
    "    plt.legend(\n",
    "        loc=\"center left\",\n",
    "        title=\"\",\n",
    "        labels=labels[::-1],\n",
    "        ncol=1,\n",
    "        bbox_to_anchor=(1.04, 0.5),\n",
    "        borderaxespad=0,\n",
    "    )\n",
    "    # plt.tight_layout() bbox_transform=fig.transFigure,\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_scatter(df, perf, sen_var_exclusion, title, c1=20, c2=0.15, **kwargs):\n",
    "    ### basic settings ###\n",
    "    np.random.seed(0)\n",
    "    if \"figsize\" not in kwargs.keys():\n",
    "        fig_h = 400\n",
    "        figsize = [fig_h * df.shape[1] * 2.45 / 3, fig_h]\n",
    "    else:\n",
    "        figsize = kwargs[\"figsize\"]\n",
    "    caption_size = figsize[1] / c1  # control font size / figure size\n",
    "    fig_caption_ratio = 0.8\n",
    "    fig_font_size = caption_size * fig_caption_ratio\n",
    "\n",
    "    font_family = \"Arial\"\n",
    "    highlight_color = \"#D4AF37\"\n",
    "    fig_font_unit = c2  # control the relative position of elements\n",
    "    caption_font_unit = fig_font_unit * fig_caption_ratio\n",
    "    d = fig_font_unit / 8\n",
    "    legend_pos_y = 1 + fig_font_unit\n",
    "    subtitle_pos = [legend_pos_y + d, legend_pos_y + d + caption_font_unit]\n",
    "    xlab_pos_y = -fig_font_unit * 2\n",
    "\n",
    "    area_list = []\n",
    "    for i, id in enumerate(df.index):\n",
    "        values = df.loc[id, :]\n",
    "        area_list.append(1 / compute_area(values))\n",
    "    ranking = np.argsort(np.argsort(area_list)[::-1])\n",
    "\n",
    "    # map model id to index position (SAFE indexing)\n",
    "    id_to_pos = {df.index[i]: i for i in range(len(df))}\n",
    "\n",
    "    # jittering for display\n",
    "    jitter_control = np.zeros(len(ranking))\n",
    "    for idx in range(len(ranking)):\n",
    "        if ranking[idx] == 0:\n",
    "            jitter_control[idx] = 0\n",
    "        elif ranking[idx] <= 10 and ranking[idx] != 0:\n",
    "            jitter_control[idx] = 0.01 * np.random.uniform(0, 1)\n",
    "        elif ranking[idx] <= 10**2 and ranking[idx] > 10:\n",
    "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
    "        elif ranking[idx] <= 10**3 and ranking[idx] > 10**2:\n",
    "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
    "        else:\n",
    "            jitter_control[idx] = 0.02 * np.random.uniform(-1, 1)\n",
    "\n",
    "    ### plot ###\n",
    "    best_id = df.index[np.where(ranking == 0)][0]\n",
    "    worst_id = df.index[np.argmin(area_list)]\n",
    "    meduim_id = df.index[np.argsort(area_list)[int(len(area_list) / 2)]]\n",
    "\n",
    "    num_metrics = df.shape[1]\n",
    "    num_models = df.shape[0]\n",
    "\n",
    "    fig = make_subplots(cols=num_metrics, rows=1, horizontal_spacing=0.13)\n",
    "    cmap = sns.light_palette(\"steelblue\", as_cmap=False, n_colors=df.shape[0])\n",
    "    cmap = cmap[::-1]\n",
    "    colors = [rgb01_hex(cmap[x]) if x != 0 else highlight_color for x in ranking]\n",
    "    sizes = [10 if x != 0 else 20 for x in ranking]\n",
    "\n",
    "    shapes = sen_var_exclusion.copy().tolist()\n",
    "    cases = sen_var_exclusion.unique()\n",
    "    shapes_candidates = [\"square\", \"circle\", \"triangle-up\", \"star\"][: len(cases)]\n",
    "    for i, case in enumerate(cases):\n",
    "        for j, v in enumerate(sen_var_exclusion):\n",
    "            if v == case:\n",
    "                shapes[j] = shapes_candidates[i]\n",
    "\n",
    "        if cases[i] == \"\":\n",
    "            cases[i] = \"No exclusion\"\n",
    "        elif len(cases[i].split(\"_\")) == 2:\n",
    "            cases[i] = f\"Exclusion of {' and '.join(cases[i].split('_'))}\"\n",
    "        elif len(cases[i].split(\"_\")) > 2:\n",
    "            sens = cases[i].split(\"_\")\n",
    "            cases[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
    "        else:\n",
    "            cases[i] = f\"Exclusion of {cases[i]}\"\n",
    "\n",
    "    fair_index_df = pd.DataFrame(\n",
    "        {\n",
    "            \"model id\": df.index,\n",
    "            \"fair_index\": area_list,\n",
    "            \"ranking\": ranking,\n",
    "            \"eod\": df[\"Equalized Odds\"],\n",
    "            \"colors\": colors,\n",
    "            \"shapes\": shapes,\n",
    "            \"sizes\": sizes,\n",
    "            \"cases\": sen_var_exclusion,\n",
    "            \"jitter\": jitter_control,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add scatter plots to the subplots\n",
    "    for k, s in enumerate(shapes_candidates):\n",
    "        for i in range(num_metrics):\n",
    "            # index of sen_var_exclusion(shape) == s\n",
    "            s_idx = [idx for idx, x in enumerate(shapes) if x == s]\n",
    "            x = df.iloc[s_idx, i].values\n",
    "            js = fair_index_df.loc[fair_index_df.shapes == s, \"jitter\"].values\n",
    "            jittered_x = x + js\n",
    "\n",
    "            col = fair_index_df.loc[fair_index_df.shapes == s, \"colors\"]\n",
    "            size = fair_index_df.loc[fair_index_df.shapes == s, \"sizes\"]\n",
    "            fair_index = fair_index_df.loc[fair_index_df.shapes == s, \"fair_index\"]\n",
    "            ids = fair_index_df.loc[fair_index_df.shapes == s, \"model id\"]\n",
    "            rank_text = fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
    "            r = (\n",
    "                fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
    "                .apply(lambda x: math.log10(x + 1))\n",
    "                .values\n",
    "            )\n",
    "            sen_case = fair_index_df.loc[fair_index_df.shapes == s, \"cases\"]\n",
    "\n",
    "            hovertext = [\n",
    "                f\"Fairness index: {f:.3f}. Ranking: {x}. Model id: {i}\"\n",
    "                for f, x, i in zip(fair_index, rank_text, ids)\n",
    "            ]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=r,\n",
    "                    y=jittered_x,\n",
    "                    customdata=hovertext,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        color=col,\n",
    "                        symbol=s,\n",
    "                        size=size,\n",
    "                        line=dict(color=col, width=1),\n",
    "                        opacity=0.8,\n",
    "                    ),\n",
    "                    hovertemplate=\"%{customdata}.\",\n",
    "                    hoverlabel=None,\n",
    "                    hoverinfo=\"name+z\",\n",
    "                    name=cases[k],\n",
    "                ),\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "            )\n",
    "\n",
    "            if i == int((df.shape[1] + 0.5) / 2):\n",
    "                fig.update_xaxes(\n",
    "                    title_text=None,\n",
    "                    tickvals=[0, 1, 2, 3],\n",
    "                    ticktext=[1, 10, 100, 1000],\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                    tickangle=0,\n",
    "                )\n",
    "            else:\n",
    "                fig.update_xaxes(\n",
    "                    title_text=None,\n",
    "                    tickvals=[0, 1, 2, 3],\n",
    "                    ticktext=[1, 10, 100, 1000],\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                    tickangle=0,\n",
    "                )\n",
    "            fig.update_yaxes(\n",
    "                title_text=df.columns[i],\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "                showticksuffix=\"none\",\n",
    "                titlefont={\"size\": caption_size},\n",
    "            )\n",
    "\n",
    "            fig.add_vline(\n",
    "                x=0,\n",
    "                line_width=2,\n",
    "                line_dash=\"dot\",\n",
    "                line_color=highlight_color,\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "            )\n",
    "\n",
    "            min_metric = df.loc[ranking == 0, df.columns[i]].values[0]\n",
    "            max_metric = df.loc[ranking == num_models - 1, df.columns[i]].values[0]\n",
    "            meduim_metric = df.loc[\n",
    "                ranking == int(num_models / 2), df.columns[i]\n",
    "            ].values[0]\n",
    "\n",
    "            # add annotations\n",
    "            anno_size = caption_size * 0.7\n",
    "            if k == 0:\n",
    "                fig.add_hline(\n",
    "                    y=min_metric,\n",
    "                    line_width=2,\n",
    "                    line_dash=\"dot\",\n",
    "                    line_color=highlight_color,\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                )\n",
    "\n",
    "                # position_y = np.mean(df.iloc[:, i])\n",
    "                min_annotation = {\n",
    "                    \"x\": 0,\n",
    "                    \"y\": min_metric,\n",
    "                    \"text\": f\"Model ID {best_id}<br> Rank No.1\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                }\n",
    "                fig.add_annotation(min_annotation, col=i + 1, row=1)\n",
    "            if meduim_id in ids:\n",
    "                medium_annotation = {\n",
    "                    \"x\": math.log10(int(num_models / 2) + 1),\n",
    "                    \"y\": meduim_metric + jitter_control[id_to_pos[meduim_id]],\n",
    "                    \"text\": f\"Model ID {meduim_id}<br> Rank No.{int(num_models/2)}\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                }\n",
    "                fig.add_annotation(medium_annotation, col=i + 1, row=1)\n",
    "            if worst_id in ids:\n",
    "                max_annotation = {\n",
    "                    \"x\": math.log10(num_models + 1),\n",
    "                    \"y\": max_metric + jitter_control[id_to_pos[worst_id]],\n",
    "                    \"text\": f\"Model ID {worst_id}<br> Rank No.{num_models}\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                    \"align\": \"left\",\n",
    "                }\n",
    "                fig.add_annotation(max_annotation, col=i + 1, row=1)\n",
    "\n",
    "    colorbar_trace = go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode=\"markers\",\n",
    "        hoverinfo=\"none\",\n",
    "        marker=dict(\n",
    "            colorscale=[\n",
    "                rgb01_hex(np.array((243, 244, 245)) / 255),\n",
    "                \"steelblue\",\n",
    "            ],  # \"magma\",\n",
    "            showscale=True,\n",
    "            cmin=0,\n",
    "            cmax=2,\n",
    "            colorbar=dict(\n",
    "                title=None,\n",
    "                thickness=10,\n",
    "                tickvals=[0, 2],\n",
    "                ticktext=[\"Low\", \"High\"],\n",
    "                outlinewidth=0,\n",
    "                orientation=\"v\",\n",
    "                x=1,\n",
    "                y=0.5,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    fig.add_trace(colorbar_trace)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        font=dict(family=\"Arial\", size=fig_font_size),\n",
    "        hovermode=\"closest\",\n",
    "        width=figsize[0],\n",
    "        height=figsize[1],\n",
    "        showlegend=True,\n",
    "        template=\"simple_white\",\n",
    "        legend=dict(x=0, y=legend_pos_y, orientation=\"h\"),\n",
    "    )\n",
    "\n",
    "    rectangle = {\n",
    "        \"type\": \"rect\",\n",
    "        \"x0\": -0.1,\n",
    "        \"y0\": subtitle_pos[0],\n",
    "        \"x1\": 1.1,\n",
    "        \"y1\": subtitle_pos[1],\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"fillcolor\": \"steelblue\",\n",
    "        \"opacity\": 0.1,\n",
    "    }  # 'line': {'color': 'red', 'width': 2},\n",
    "    fig.add_shape(rectangle)\n",
    "    subtitle_annotation = {\n",
    "        \"x\": -0.1,\n",
    "        \"y\": subtitle_pos[1],\n",
    "        \"text\": f\"<i> The FAIM model (i.e., fairness-aware model) is with model ID {best_id}, out of {num_models} nearly-optimal models.</i>\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": caption_size * 1.1},\n",
    "        \"align\": \"left\",\n",
    "    }\n",
    "    xaxis_annotation = {\n",
    "        \"x\": 0.5,\n",
    "        \"y\": xlab_pos_y,\n",
    "        \"text\": \"Model Rank\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": caption_size},\n",
    "    }\n",
    "    colorbar_title = {\n",
    "        \"x\": 1.05,\n",
    "        \"y\": 0.5,\n",
    "        \"text\": \"Fairness Ranking Index (FRI)\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": anno_size * 0.9},\n",
    "        \"textangle\": 90,\n",
    "    }\n",
    "    fig.add_annotation(subtitle_annotation)\n",
    "    fig.add_annotation(xaxis_annotation)\n",
    "    fig.add_annotation(colorbar_title)\n",
    "\n",
    "    for i, trace in enumerate(fig.data):\n",
    "        if i % num_metrics == 1:\n",
    "            trace.update(showlegend=True)\n",
    "        else:\n",
    "            trace.update(showlegend=False)\n",
    "    # fig.show()\n",
    "\n",
    "    return fig, fair_index_df\n",
    "\n",
    "\n",
    "def plot_radar(df, thresh_show, title, **kwargs):\n",
    "    fig = go.Figure()\n",
    "    # fig = sp.make_subplots(rows=1, cols=2)\n",
    "    cmap = sns.diverging_palette(200, 20, sep=10, s=50, as_cmap=False, n=df.shape[0])\n",
    "    theta = df.columns.tolist()\n",
    "    theta += theta[:1]\n",
    "    area_list = []\n",
    "\n",
    "    for i, id in enumerate(df.index):\n",
    "        values = df.loc[id, :]\n",
    "        area_list.append(compute_area(values))\n",
    "        values = values.values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        info = [\n",
    "            f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
    "        ]\n",
    "        fig.add_trace(\n",
    "            go.Scatterpolar(\n",
    "                r=values,\n",
    "                theta=theta,\n",
    "                fill=\"toself\" if id == \"FAIReg\" else \"none\",\n",
    "                text=\"\\n\".join(info),\n",
    "                name=f\"{id}\",\n",
    "                line=dict(color=rgb01_hex(cmap[i]), dash=\"dot\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ranking = np.argsort(np.argsort(area_list))\n",
    "    best_id = df.index[np.where(ranking == 0)][0]\n",
    "    print(\n",
    "        f\"The best model is No.{best_id} with metrics on validation set:\\n {df.loc[best_id, :]}\"\n",
    "    )\n",
    "    values = df.loc[best_id, :].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    info = [\n",
    "        f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
    "    ]\n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=theta,\n",
    "            fill=\"toself\",\n",
    "            text=\"\\n\".join(info),\n",
    "            name=f\"model {best_id}\",\n",
    "            line=dict(color=\"royalblue\", dash=\"solid\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        # title = title,\n",
    "        font=dict(family=\"Arial\", size=16),\n",
    "        polar=dict(\n",
    "            # bgcolor = \"#1e2130\",\n",
    "            radialaxis=dict(\n",
    "                showgrid=True,\n",
    "                gridwidth=1,\n",
    "                gridcolor=\"lightgray\",\n",
    "                visible=True,\n",
    "                range=[0, thresh_show],\n",
    "            )\n",
    "        ),\n",
    "        legend=dict(x=0.25, y=-0.1, orientation=\"h\"),\n",
    "        showlegend=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_bar(\n",
    "    shap_values, feature_names, original_feature_names, coef=None, title=None, **kwargs\n",
    "):\n",
    "    \"\"\"Plot the bar chart of feature importance\"\"\"\n",
    "    if \"color\" not in kwargs.keys():\n",
    "        color = \"steelblue\"\n",
    "    else:\n",
    "        color = kwargs[\"color\"]\n",
    "\n",
    "    def get_prefix(v):\n",
    "        if \"_\" in v and (v not in original_feature_names):\n",
    "            tmp = [\"_\".join(v.split(\"_\")[:i]) for i in range(len(v.split(\"_\")))]\n",
    "            return [s for s in tmp if s in original_feature_names][0]\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    if shap_values is not None:\n",
    "\n",
    "        grouped_df = pd.DataFrame({\"values\": shap_values}, index=feature_names).groupby(\n",
    "            by=get_prefix, axis=0\n",
    "        )\n",
    "        df = {k: np.mean(np.abs(g.values)) for k, g in grouped_df}\n",
    "        df = pd.DataFrame.from_dict(df, orient=\"index\").reset_index()\n",
    "        df.columns = [\"Var\", \"Value\"]\n",
    "\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in df.Value],\n",
    "                        \"order\": np.abs(df.Value),\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    elif coef is not None:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"Var\": coef.index,\n",
    "                \"Value\": coef.values,\n",
    "                \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in coef.values],\n",
    "                \"order\": np.abs(coef.values),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Either shap_value or coef should be provided\")\n",
    "\n",
    "    df = df.loc[df[\"Var\"] != \"const\", :]\n",
    "    df = df.sort_values(by=\"order\", ascending=True)\n",
    "    df[\"Var\"] = pd.Categorical(df[\"Var\"], categories=df[\"Var\"].tolist(), ordered=True)\n",
    "\n",
    "    common_theme = theme(\n",
    "        text=element_text(size=24),\n",
    "        panel_grid_major_y=element_line(colour=\"lightgrey\"),\n",
    "        panel_grid_minor=element_blank(),\n",
    "        panel_background=element_blank(),\n",
    "        axis_line_x=element_line(colour=\"black\"),\n",
    "        axis_ticks_major_y=element_blank(),\n",
    "    )\n",
    "\n",
    "    x_lab = \"Feature importance\"\n",
    "\n",
    "    p = (\n",
    "        ggplot(data=df, mapping=aes(x=\"Var\", y=\"Value\", fill=\"color\"))\n",
    "        + geom_hline(yintercept=0, color=\"grey\")\n",
    "        + geom_bar(stat=\"identity\")\n",
    "        + common_theme\n",
    "        + coord_flip()\n",
    "        + labs(x=\"\", y=x_lab, title=title)\n",
    "        + theme(legend_position=\"none\")\n",
    "        + scale_fill_manual(values=[color])\n",
    "    )\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcAfJx0xq3T_"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bFqU55IWfSx",
    "outputId": "9cecac4f-cfdc-4b7e-fd7c-04bdf9a64ba5"
   },
   "outputs": [],
   "source": [
    "# Load df_master\n",
    "df_master = pd.read_csv('df_master.csv')\n",
    "print(\"df_master loaded from df_master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKAcDxxCqS-z",
    "outputId": "6e082fdb-f394-475b-e73a-586a973080de"
   },
   "outputs": [],
   "source": [
    "df_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLj2JM83cJXO"
   },
   "source": [
    "# Dataset Split (train: 0.7, val: 0.1, test: 0.2, use seed to fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IjLThK9K0Kb"
   },
   "outputs": [],
   "source": [
    "# Create 70/10/20 split\n",
    "dat_train, temp_df = train_test_split(df_master, test_size=0.3, random_state=42)\n",
    "dat_expl, dat_test = train_test_split(temp_df, test_size=2/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmKI0HVdcNut",
    "outputId": "315d0d61-389f-4c48-a33f-106ee21b11fd"
   },
   "outputs": [],
   "source": [
    "print('Training dataset size = ', len(dat_train))\n",
    "print('Validation dataset size = ', len(dat_expl))\n",
    "print('Testing dataset size = ', len(dat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUGGFX7EsvnB"
   },
   "source": [
    "# Missing Value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nLm1feZst9b",
    "outputId": "e1c19dd5-5dfc-40f9-d31f-9d70332718b9"
   },
   "outputs": [],
   "source": [
    "df_missing_stats = dat_train.isnull().sum().to_frame().T\n",
    "df_missing_stats.loc[1] = df_missing_stats.loc[0] / len(dat_train)\n",
    "df_missing_stats.index = ['no. of missing values', 'percentage of missing values']\n",
    "df_missing_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1t2wzIHkqnCH"
   },
   "outputs": [],
   "source": [
    "# from mimic-extract\n",
    "vitals_valid_range = {\n",
    "    'temperature': {'outlier_low': 14.2, 'valid_low': 26, 'valid_high': 45, 'outlier_high':47},\n",
    "    'heartrate': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 350, 'outlier_high':390},\n",
    "    'resprate': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 300, 'outlier_high':330},\n",
    "    'o2sat': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 100, 'outlier_high':150},\n",
    "    'sbp': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 375, 'outlier_high':375},\n",
    "    'dbp': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 375, 'outlier_high':375},\n",
    "    'pain': {'outlier_low': 0, 'valid_low': 0, 'valid_high': 10, 'outlier_high':10},\n",
    "    'acuity': {'outlier_low': 1, 'valid_low': 1, 'valid_high': 5, 'outlier_high':5},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Fh2dh62s206",
    "outputId": "3fcb254b-45d2-428e-e780-71b58c134ca4"
   },
   "outputs": [],
   "source": [
    "vitals_cols = [col for col in df_master.columns if len(col.split('_')) > 1 and\n",
    "                                                   col.split('_')[1] in vitals_valid_range and\n",
    "                                                   col.split('_')[1] != 'acuity']\n",
    "vitals_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYKxVEi2uZk3"
   },
   "outputs": [],
   "source": [
    "vitals_cols = ['triage_temperature', 'triage_heartrate', 'triage_resprate',\n",
    "               'triage_o2sat', 'triage_sbp', 'triage_dbp', 'triage_pain']\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "dat_train[vitals_cols] = imputer.fit_transform(dat_train[vitals_cols])\n",
    "dat_expl[vitals_cols] = imputer.transform(dat_expl[vitals_cols])\n",
    "dat_test[vitals_cols] = imputer.transform(dat_test[vitals_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQY5jSZK64xP"
   },
   "source": [
    "# 5. Add Score values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1XsIxROVenY"
   },
   "source": [
    "### Score value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jL6saenK677k"
   },
   "outputs": [],
   "source": [
    "def add_score_CCI(df):\n",
    "    conditions = [\n",
    "        (df['age'] < 50),\n",
    "        (df['age'] >= 50) & (df['age'] <= 59),\n",
    "        (df['age'] >= 60) & (df['age'] <= 69),\n",
    "        (df['age'] >= 70) & (df['age'] <= 79),\n",
    "        (df['age'] >= 80)\n",
    "    ]\n",
    "    values = [0, 1, 2, 3, 4]\n",
    "    df['score_CCI'] = np.select(conditions, values)\n",
    "    df['score_CCI'] = df['score_CCI'] + df['cci_MI'] + df['cci_CHF'] + df['cci_PVD'] + df['cci_Stroke'] + df['cci_Dementia'] + df['cci_Pulmonary'] + df['cci_PUD'] + df['cci_Rheumatic'] +df['cci_Liver1']*1 + df['cci_Liver2']*3 + df['cci_DM1'] + df['cci_DM2']*2 +df['cci_Paralysis']*2 + df['cci_Renal']*2 + df['cci_Cancer1']*2 + df['cci_Cancer2']*6 + df['cci_HIV']*6\n",
    "    print(\"Variable 'add_score_CCI' successfully added\")\n",
    "\n",
    "def add_triage_MAP(df):\n",
    "    df['triage_MAP'] = df['triage_sbp']*1/3 + df['triage_dbp']*2/3\n",
    "    print(\"Variable 'add_triage_MAP' successfully added\")\n",
    "\n",
    "def add_score_REMS(df):\n",
    "    conditions1 = [\n",
    "        (df['age'] < 45),\n",
    "        (df['age'] >= 45) & (df['age'] <= 54),\n",
    "        (df['age'] >= 55) & (df['age'] <= 64),\n",
    "        (df['age'] >= 65) & (df['age'] <= 74),\n",
    "        (df['age'] > 74)\n",
    "    ]\n",
    "    values1 = [0, 2, 3, 5, 6]\n",
    "    conditions2 = [\n",
    "        (df['triage_MAP'] > 159),\n",
    "        (df['triage_MAP'] >= 130) & (df['triage_MAP'] <= 159),\n",
    "        (df['triage_MAP'] >= 110) & (df['triage_MAP'] <= 129),\n",
    "        (df['triage_MAP'] >= 70) & (df['triage_MAP'] <= 109),\n",
    "        (df['triage_MAP'] >= 50) & (df['triage_MAP'] <= 69),\n",
    "        (df['triage_MAP'] < 49)\n",
    "    ]\n",
    "    values2 = [4, 3, 2, 0, 2, 4]\n",
    "    conditions3 = [\n",
    "        (df['triage_heartrate'] >179),\n",
    "        (df['triage_heartrate'] >= 140) & (df['triage_heartrate'] <= 179),\n",
    "        (df['triage_heartrate'] >= 110) & (df['triage_heartrate'] <= 139),\n",
    "        (df['triage_heartrate'] >= 70) & (df['triage_heartrate'] <= 109),\n",
    "        (df['triage_heartrate'] >= 55) & (df['triage_heartrate'] <= 69),\n",
    "        (df['triage_heartrate'] >= 40) & (df['triage_heartrate'] <= 54),\n",
    "        (df['triage_heartrate'] < 40)\n",
    "    ]\n",
    "    values3 = [4, 3, 2, 0, 2, 3, 4]\n",
    "    conditions4 = [\n",
    "        (df['triage_resprate'] > 49),\n",
    "        (df['triage_resprate'] >= 35) & (df['triage_resprate'] <= 49),\n",
    "        (df['triage_resprate'] >= 25) & (df['triage_resprate'] <= 34),\n",
    "        (df['triage_resprate'] >= 12) & (df['triage_resprate'] <= 24),\n",
    "        (df['triage_resprate'] >= 10) & (df['triage_resprate'] <= 11),\n",
    "        (df['triage_resprate'] >= 6) & (df['triage_resprate'] <= 9),\n",
    "        (df['triage_resprate'] < 6)\n",
    "    ]\n",
    "    values4 = [4, 3, 1, 0, 1, 2, 4]\n",
    "    conditions5 = [\n",
    "        (df['triage_o2sat'] < 75),\n",
    "        (df['triage_o2sat'] >= 75) & (df['triage_o2sat'] <= 85),\n",
    "        (df['triage_o2sat'] >= 86) & (df['triage_o2sat'] <= 89),\n",
    "        (df['triage_o2sat'] > 89)\n",
    "    ]\n",
    "    values5 = [4, 3, 1, 0]\n",
    "    df['score_REMS'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4) + np.select(conditions5, values5)\n",
    "    print(\"Variable 'Score_REMS' successfully added\")\n",
    "\n",
    "def add_score_CART(df):\n",
    "    conditions1 = [\n",
    "        (df['age'] < 55),\n",
    "        (df['age'] >= 55) & (df['age'] <= 69),\n",
    "        (df['age'] >= 70)\n",
    "    ]\n",
    "    values1 = [0, 4, 9]\n",
    "    conditions2 = [\n",
    "        (df['triage_resprate'] < 21),\n",
    "        (df['triage_resprate'] >= 21) & (df['triage_resprate'] <= 23),\n",
    "        (df['triage_resprate'] >= 24) & (df['triage_resprate'] <= 25),\n",
    "        (df['triage_resprate'] >= 26) & (df['triage_resprate'] <= 29),\n",
    "        (df['triage_resprate'] >= 30)\n",
    "    ]\n",
    "    values2 = [0, 8, 12, 15, 22]\n",
    "    conditions3 = [\n",
    "        (df['triage_heartrate'] < 110),\n",
    "        (df['triage_heartrate'] >= 110) & (df['triage_heartrate'] <= 139),\n",
    "        (df['triage_heartrate'] >= 140)\n",
    "    ]\n",
    "    values3 = [0, 4, 13]\n",
    "    conditions4 = [\n",
    "        (df['triage_dbp'] > 49),\n",
    "        (df['triage_dbp'] >= 40) & (df['triage_dbp'] <= 49),\n",
    "        (df['triage_dbp'] >= 35) & (df['triage_dbp'] <= 39),\n",
    "        (df['triage_dbp'] < 35)\n",
    "    ]\n",
    "    values4 = [0, 4, 6, 13]\n",
    "    df['score_CART'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4)\n",
    "    print(\"Variable 'Score_CART' successfully added\")\n",
    "\n",
    "def add_score_NEWS(df):\n",
    "    conditions1 = [\n",
    "        (df['triage_resprate'] <= 8),\n",
    "        (df['triage_resprate'] >= 9) & (df['triage_resprate'] <= 11),\n",
    "        (df['triage_resprate'] >= 12) & (df['triage_resprate'] <= 20),\n",
    "        (df['triage_resprate'] >= 21) & (df['triage_resprate'] <= 24),\n",
    "        (df['triage_resprate'] >= 25)\n",
    "    ]\n",
    "    values1 = [3, 1, 0, 2, 3]\n",
    "    conditions2 = [\n",
    "        (df['triage_o2sat'] <= 91),\n",
    "        (df['triage_o2sat'] >= 92) & (df['triage_o2sat'] <= 93),\n",
    "        (df['triage_o2sat'] >= 94) & (df['triage_o2sat'] <= 95),\n",
    "        (df['triage_o2sat'] >= 96)\n",
    "    ]\n",
    "    values2 = [3, 2, 1, 0]\n",
    "    conditions3 = [\n",
    "        (df['triage_temperature'] <= 35),\n",
    "        (df['triage_temperature'] > 35) & (df['triage_temperature'] <= 36),\n",
    "        (df['triage_temperature'] > 36) & (df['triage_temperature'] <= 38),\n",
    "        (df['triage_temperature'] > 38) & (df['triage_temperature'] <= 39),\n",
    "        (df['triage_temperature'] > 39)\n",
    "    ]\n",
    "    values3 = [3, 1, 0, 1, 2]\n",
    "    conditions4 = [\n",
    "        (df['triage_sbp'] <= 90),\n",
    "        (df['triage_sbp'] >= 91) & (df['triage_sbp'] <= 100),\n",
    "        (df['triage_sbp'] >= 101) & (df['triage_sbp'] <= 110),\n",
    "        (df['triage_sbp'] >= 111) & (df['triage_sbp'] <= 219),\n",
    "        (df['triage_sbp'] > 219)\n",
    "    ]\n",
    "    values4 = [3, 2, 1, 0, 3]\n",
    "    conditions5 = [\n",
    "        (df['triage_heartrate'] <= 40),\n",
    "        (df['triage_heartrate'] >= 41) & (df['triage_heartrate'] <= 50),\n",
    "        (df['triage_heartrate'] >= 51) & (df['triage_heartrate'] <= 90),\n",
    "        (df['triage_heartrate'] >= 91) & (df['triage_heartrate'] <= 110),\n",
    "        (df['triage_heartrate'] >= 111) & (df['triage_heartrate'] <= 130),\n",
    "        (df['triage_heartrate'] > 130)\n",
    "    ]\n",
    "    values5 = [3, 1, 0, 1, 2, 3]\n",
    "    df['score_NEWS'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4) + np.select(conditions5, values5)\n",
    "    print(\"Variable 'Score_NEWS' successfully added\")\n",
    "\n",
    "def add_score_NEWS2(df):\n",
    "    conditions1 = [\n",
    "        (df['triage_resprate'] <= 8),\n",
    "        (df['triage_resprate'] >= 9) & (df['triage_resprate'] <= 11),\n",
    "        (df['triage_resprate'] >= 12) & (df['triage_resprate'] <= 20),\n",
    "        (df['triage_resprate'] >= 21) & (df['triage_resprate'] <= 24),\n",
    "        (df['triage_resprate'] >= 25)\n",
    "    ]\n",
    "    values1 = [3, 1, 0, 2, 3]\n",
    "    conditions2 = [\n",
    "        (df['triage_temperature'] <= 35),\n",
    "        (df['triage_temperature'] > 35) & (df['triage_temperature'] <= 36),\n",
    "        (df['triage_temperature'] > 36) & (df['triage_temperature'] <= 38),\n",
    "        (df['triage_temperature'] > 38) & (df['triage_temperature'] <= 39),\n",
    "        (df['triage_temperature'] > 39)\n",
    "    ]\n",
    "    values2 = [3, 1, 0, 1, 2]\n",
    "    conditions3 = [\n",
    "        (df['triage_sbp'] <= 90),\n",
    "        (df['triage_sbp'] >= 91) & (df['triage_sbp'] <= 100),\n",
    "        (df['triage_sbp'] >= 101) & (df['triage_sbp'] <= 110),\n",
    "        (df['triage_sbp'] >= 111) & (df['triage_sbp'] <= 219),\n",
    "        (df['triage_sbp'] > 219)\n",
    "    ]\n",
    "    values3 = [3, 2, 1, 0, 3]\n",
    "    conditions4 = [\n",
    "        (df['triage_heartrate'] <= 40),\n",
    "        (df['triage_heartrate'] >= 41) & (df['triage_heartrate'] <= 50),\n",
    "        (df['triage_heartrate'] >= 51) & (df['triage_heartrate'] <= 90),\n",
    "        (df['triage_heartrate'] >= 91) & (df['triage_heartrate'] <= 110),\n",
    "        (df['triage_heartrate'] >= 111) & (df['triage_heartrate'] <= 130),\n",
    "        (df['triage_heartrate'] > 130)\n",
    "    ]\n",
    "    values4 = [3, 1, 0, 1, 2, 3]\n",
    "    df['score_NEWS2'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4)\n",
    "    print(\"Variable 'Score_NEWS2' successfully added\")\n",
    "\n",
    "def add_score_MEWS(df):\n",
    "    conditions1 = [\n",
    "        (df['triage_sbp'] <= 70),\n",
    "        (df['triage_sbp'] >= 71) & (df['triage_sbp'] <= 80),\n",
    "        (df['triage_sbp'] >= 81) & (df['triage_sbp'] <= 100),\n",
    "        (df['triage_sbp'] >= 101) & (df['triage_sbp'] <= 199),\n",
    "        (df['triage_sbp'] > 199)\n",
    "    ]\n",
    "    values1 = [3, 2, 1, 0, 2]\n",
    "    conditions2 = [\n",
    "        (df['triage_heartrate'] <= 40),\n",
    "        (df['triage_heartrate'] >= 41) & (df['triage_heartrate'] <= 50),\n",
    "        (df['triage_heartrate'] >= 51) & (df['triage_heartrate'] <= 100),\n",
    "        (df['triage_heartrate'] >= 101) & (df['triage_heartrate'] <= 110),\n",
    "        (df['triage_heartrate'] >= 111) & (df['triage_heartrate'] <= 129),\n",
    "        (df['triage_heartrate'] >= 130)\n",
    "    ]\n",
    "    values2 = [2, 1, 0, 1, 2, 3]\n",
    "    conditions3 = [\n",
    "        (df['triage_resprate'] < 9),\n",
    "        (df['triage_resprate'] >= 9) & (df['triage_resprate'] <= 14),\n",
    "        (df['triage_resprate'] >= 15) & (df['triage_resprate'] <= 20),\n",
    "        (df['triage_resprate'] >= 21) & (df['triage_resprate'] <= 29),\n",
    "        (df['triage_resprate'] >= 30)\n",
    "    ]\n",
    "    values3 = [2, 0, 1, 2, 3]\n",
    "    conditions4 = [\n",
    "        (df['triage_temperature'] < 35),\n",
    "        (df['triage_temperature'] >= 35) & (df['triage_temperature'] < 38.5),\n",
    "        (df['triage_temperature'] >= 38.5)\n",
    "    ]\n",
    "    values4 = [2, 0, 2]\n",
    "    df['score_MEWS'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4)\n",
    "    print(\"Variable 'Score_MEWS' successfully added\")\n",
    "\n",
    "def add_score_SERP2d(df):\n",
    "    conditions1 = [\n",
    "        (df['age'] < 30),\n",
    "        (df['age'] >= 30) & (df['age'] <= 49),\n",
    "        (df['age'] >= 50) & (df['age'] <= 79),\n",
    "        (df['age'] >= 80)\n",
    "    ]\n",
    "    values1 = [0, 9, 13, 17]\n",
    "    conditions2 = [\n",
    "        (df['triage_heartrate'] < 60),\n",
    "        (df['triage_heartrate'] >= 60) & (df['triage_heartrate'] <= 69),\n",
    "        (df['triage_heartrate'] >= 70) & (df['triage_heartrate'] <= 94),\n",
    "        (df['triage_heartrate'] >= 95) & (df['triage_heartrate'] <= 109),\n",
    "        (df['triage_heartrate'] >= 110)\n",
    "    ]\n",
    "    values2 = [3, 0, 3, 6, 10]\n",
    "    conditions3 = [\n",
    "        (df['triage_resprate'] < 16),\n",
    "        (df['triage_resprate'] >= 16) & (df['triage_resprate'] <= 19),\n",
    "        (df['triage_resprate'] >= 20)\n",
    "    ]\n",
    "    values3 = [11, 0, 7]\n",
    "    conditions4 = [\n",
    "        (df['triage_sbp'] < 100),\n",
    "        (df['triage_sbp'] >= 100) & (df['triage_sbp'] <= 114),\n",
    "        (df['triage_sbp'] >= 115) & (df['triage_sbp'] <= 149),\n",
    "        (df['triage_sbp'] >= 150)\n",
    "    ]\n",
    "    values4 = [10, 4, 1, 0]\n",
    "    conditions5 = [\n",
    "        (df['triage_dbp'] < 50),\n",
    "        (df['triage_dbp'] >= 50) & (df['triage_dbp'] <= 94),\n",
    "        (df['triage_dbp'] >= 95)\n",
    "    ]\n",
    "    values5 = [5, 0, 1]\n",
    "    conditions6 = [\n",
    "        (df['triage_o2sat'] < 90),\n",
    "        (df['triage_o2sat'] >= 90) & (df['triage_o2sat'] <= 94),\n",
    "        (df['triage_o2sat'] >= 95)\n",
    "    ]\n",
    "    values6 = [7, 5, 0]\n",
    "    df['score_SERP2d'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4) + np.select(conditions5, values5) + np.select(conditions6, values6)\n",
    "    print(\"Variable 'Score_SERP2d' successfully added\")\n",
    "\n",
    "def add_score_SERP7d(df):\n",
    "    conditions1 = [\n",
    "        (df['age'] < 30),\n",
    "        (df['age'] >= 30) & (df['age'] <= 49),\n",
    "        (df['age'] >= 50) & (df['age'] <= 79),\n",
    "        (df['age'] >= 80)\n",
    "    ]\n",
    "    values1 = [0, 10, 17, 21]\n",
    "    conditions2 = [\n",
    "        (df['triage_heartrate'] < 60),\n",
    "        (df['triage_heartrate'] >= 60) & (df['triage_heartrate'] <= 69),\n",
    "        (df['triage_heartrate'] >= 70) & (df['triage_heartrate'] <= 94),\n",
    "        (df['triage_heartrate'] >= 95) & (df['triage_heartrate'] <= 109),\n",
    "        (df['triage_heartrate'] >= 110)\n",
    "    ]\n",
    "    values2 = [2, 0, 4, 8, 12]\n",
    "    conditions3 = [\n",
    "        (df['triage_resprate'] < 16),\n",
    "        (df['triage_resprate'] >= 16) & (df['triage_resprate'] <= 19),\n",
    "        (df['triage_resprate'] >= 20)\n",
    "    ]\n",
    "    values3 = [10, 0, 6]\n",
    "    conditions4 = [\n",
    "        (df['triage_sbp'] < 100),\n",
    "        (df['triage_sbp'] >= 100) & (df['triage_sbp'] <= 114),\n",
    "        (df['triage_sbp'] >= 115) & (df['triage_sbp'] <= 149),\n",
    "        (df['triage_sbp'] >= 150)\n",
    "    ]\n",
    "    values4 = [12, 6, 1, 0]\n",
    "    conditions5 = [\n",
    "        (df['triage_dbp'] < 50),\n",
    "        (df['triage_dbp'] >= 50) & (df['triage_dbp'] <= 94),\n",
    "        (df['triage_dbp'] >= 95)\n",
    "    ]\n",
    "    values5 = [4, 0, 2]\n",
    "    df['score_SERP7d'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4) + np.select(conditions5, values5)\n",
    "    print(\"Variable 'Score_SERP7d' successfully added\")\n",
    "\n",
    "def add_score_SERP30d(df):\n",
    "    conditions1 = [\n",
    "        (df['age'] < 30),\n",
    "        (df['age'] >= 30) & (df['age'] <= 49),\n",
    "        (df['age'] >= 50) & (df['age'] <= 79),\n",
    "        (df['age'] >= 80)\n",
    "    ]\n",
    "    values1 = [0, 8, 14, 19]\n",
    "    conditions2 = [\n",
    "        (df['triage_heartrate'] < 60),\n",
    "        (df['triage_heartrate'] >= 60) & (df['triage_heartrate'] <= 69),\n",
    "        (df['triage_heartrate'] >= 70) & (df['triage_heartrate'] <= 94),\n",
    "        (df['triage_heartrate'] >= 95) & (df['triage_heartrate'] <= 109),\n",
    "        (df['triage_heartrate'] >= 110)\n",
    "    ]\n",
    "    values2 = [1, 0, 2, 6, 9]\n",
    "    conditions3 = [\n",
    "        (df['triage_resprate'] < 16),\n",
    "        (df['triage_resprate'] >= 16) & (df['triage_resprate'] <= 19),\n",
    "        (df['triage_resprate'] >= 20)\n",
    "    ]\n",
    "    values3 = [8, 0, 6]\n",
    "    conditions4 = [\n",
    "        (df['triage_sbp'] < 100),\n",
    "        (df['triage_sbp'] >= 100) & (df['triage_sbp'] <= 114),\n",
    "        (df['triage_sbp'] >= 115) & (df['triage_sbp'] <= 149),\n",
    "        (df['triage_sbp'] >= 150)\n",
    "    ]\n",
    "    values4 = [8, 5, 2, 0]\n",
    "    conditions5 = [\n",
    "        (df['triage_dbp'] < 50),\n",
    "        (df['triage_dbp'] >= 50) & (df['triage_dbp'] <= 94),\n",
    "        (df['triage_dbp'] >= 95)\n",
    "    ]\n",
    "    values5 = [3, 0, 2]\n",
    "    df['score_SERP30d'] = np.select(conditions1, values1) + np.select(conditions2, values2) + np.select(conditions3, values3) +                              np.select(conditions4, values4) + np.select(conditions5, values5) + df['cci_Cancer1']*6 + df['cci_Cancer2']*12\n",
    "    print(\"Variable 'Score_SERP30d' successfully added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVlqJecQVkYW"
   },
   "source": [
    "### Adding score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ShJ7-tX650a",
    "outputId": "2732c0ae-4874-44f3-b0c5-48f524ac3614"
   },
   "outputs": [],
   "source": [
    "def add_all_scores(df):\n",
    "    add_triage_MAP(df)\n",
    "    add_score_CCI(df)\n",
    "    add_score_CART(df)\n",
    "    add_score_REMS(df)\n",
    "    add_score_NEWS(df)\n",
    "    add_score_NEWS2(df)\n",
    "    add_score_MEWS(df)\n",
    "    add_score_SERP2d(df)\n",
    "    add_score_SERP7d(df)\n",
    "    add_score_SERP30d(df)\n",
    "    return df\n",
    "\n",
    "dat_train = add_all_scores(dat_train)\n",
    "dat_expl  = add_all_scores(dat_expl)\n",
    "dat_test  = add_all_scores(dat_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNbskXFeUoCX"
   },
   "source": [
    "# FAIM-specific preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5ENYKRHUvUo"
   },
   "outputs": [],
   "source": [
    "# esi: emergency severity index\n",
    "esi_dic = {1.0:\"High risk\", 2.0:\"High risk\", 3.0:\"@Low risk\", 4.0:\"@Low risk\", 5.0:\"@Low risk\"}\n",
    "dat_train[\"triage_acuity\"] = dat_train[\"triage_acuity\"].map(esi_dic)\n",
    "dat_expl[\"triage_acuity\"] = dat_expl[\"triage_acuity\"].map(esi_dic)\n",
    "dat_test[\"triage_acuity\"] = dat_test[\"triage_acuity\"].map(esi_dic)\n",
    "\n",
    "race_dic = {\n",
    "    # White group\n",
    "    'WHITE': '@White',\n",
    "    'WHITE - OTHER EUROPEAN': '@White',\n",
    "    'WHITE - BRAZILIAN': '@White',\n",
    "    'WHITE - RUSSIAN': '@White',\n",
    "    'WHITE - EASTERN EUROPEAN': '@White',\n",
    "\n",
    "    # Black group\n",
    "    'BLACK/AFRICAN AMERICAN': 'Black',\n",
    "    'BLACK/AFRICAN': 'Black',\n",
    "    'BLACK/CAPE VERDEAN': 'Black',\n",
    "    'BLACK/CARIBBEAN ISLAND': 'Black',\n",
    "\n",
    "    # Asian group\n",
    "    'ASIAN': 'Asian',\n",
    "    'ASIAN - CHINESE': 'Asian',\n",
    "    'ASIAN - ASIAN INDIAN': 'Asian',\n",
    "    'ASIAN - SOUTH EAST ASIAN': 'Asian',\n",
    "    'ASIAN - KOREAN': 'Asian',\n",
    "\n",
    "    # Hispanic/Latino group\n",
    "    'HISPANIC/LATINO - COLUMBIAN': 'Hispanic',\n",
    "    'HISPANIC/LATINO - DOMINICAN': 'Hispanic',\n",
    "    'HISPANIC/LATINO - PUERTO RICAN': 'Hispanic',\n",
    "    'HISPANIC OR LATINO': 'Hispanic',\n",
    "    'HISPANIC/LATINO - GUATEMALAN': 'Hispanic',\n",
    "    'HISPANIC/LATINO - HONDURAN': 'Hispanic',\n",
    "    'HISPANIC/LATINO - SALVADORAN': 'Hispanic',\n",
    "    'HISPANIC/LATINO - CENTRAL AMERICAN': 'Hispanic',\n",
    "    'HISPANIC/LATINO - CUBAN': 'Hispanic',\n",
    "    'HISPANIC/LATINO - MEXICAN': 'Hispanic',\n",
    "\n",
    "    # Others / Uncategorized\n",
    "    'OTHER': 'Others',\n",
    "    'PATIENT DECLINED TO ANSWER': 'Others',\n",
    "    'UNKNOWN': 'Others',\n",
    "    'UNABLE TO OBTAIN': 'Others',\n",
    "    'MULTIPLE RACE/ETHNICITY': 'Others',\n",
    "    'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER': 'Others',\n",
    "    'AMERICAN INDIAN/ALASKA NATIVE': 'Others',\n",
    "    'PORTUGUESE': 'Others',\n",
    "    'SOUTH AMERICAN': 'Others',\n",
    "}\n",
    "\n",
    "dat_train[\"race\"] = dat_train[\"race\"].map(race_dic)\n",
    "dat_expl[\"race\"] = dat_expl[\"race\"].map(race_dic)\n",
    "dat_test[\"race\"] = dat_test[\"race\"].map(race_dic)\n",
    "\n",
    "sex_dic = {'F': \"Female\", 'M': \"@Male\"}\n",
    "dat_train[\"sex\"] = dat_train[\"sex\"].map(sex_dic)\n",
    "dat_expl[\"sex\"] = dat_expl[\"sex\"].map(sex_dic)\n",
    "dat_test[\"sex\"] = dat_test[\"sex\"].map(sex_dic)\n",
    "\n",
    "var_dict = {\"Age\": \"Age\", \"sex\": \"Sex\", \"race\":\"Race\", \"triage_acuity\": \"ESI\", \"triage_o2sat\": \"SPO2\", \"triage_temperature\":\"Temperature\", \"n_hosp_365d\":\"Hospitalizations last year\", \"triage_pain\": \"Pain scale\", \"triage_heartrate\":\"Heartrate\", \"triage_resprate\": \"Respirate rate\", \"triage_dbp\": \"Diastolic blood pressure\", \"triage_sbp\":\"Systolic blood pressure\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQGN5pslUzoo"
   },
   "outputs": [],
   "source": [
    "y_name = 'label'\n",
    "# these 12 features are chosen because they represent patient condition on arrival\n",
    "colnames = ['Age', 'ESI', 'Systolic blood pressure', 'Heartrate', 'Diastolic blood pressure', 'Temperature', 'Pain scale', 'SPO2', 'Respirate rate', 'Hospitalizations last year', 'Sex', 'Race']\n",
    "x_names_cat = [\"ESI\", \"Sex\", \"Race\"]\n",
    "sen = [\"Sex\", \"Race\"]\n",
    "sen_ref = {\"Sex\":\"@Male\", \"Race\":\"@White\"}\n",
    "#\n",
    "\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SQk5H1zbRJw"
   },
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    'sex': 'Sex',\n",
    "    'age': 'Age',\n",
    "    'race': 'Race',\n",
    "    'triage_acuity': 'ESI',\n",
    "    'triage_o2sat': 'SPO2',\n",
    "    'triage_temperature': 'Temperature',\n",
    "    'n_hosp_365d': 'Hospitalizations last year',\n",
    "    'triage_pain': 'Pain scale',\n",
    "    'triage_heartrate': 'Heartrate',\n",
    "    'triage_resprate': 'Respirate rate',\n",
    "    'triage_dbp': 'Diastolic blood pressure',\n",
    "    'triage_sbp': 'Systolic blood pressure'\n",
    "}\n",
    "\n",
    "for df in [dat_train, dat_expl, dat_test]:\n",
    "    df.rename(columns=rename_map, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CXf8nEskZug"
   },
   "source": [
    "# Step 1: Nearly-optimal model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4gqdyZMklFN"
   },
   "outputs": [],
   "source": [
    "# drop rows with missing outcome_hospitalization\n",
    "dat_train = dat_train[dat_train['outcome_hospitalization'].notna()]\n",
    "dat_expl  = dat_expl[dat_expl['outcome_hospitalization'].notna()]\n",
    "dat_test  = dat_test[dat_test['outcome_hospitalization'].notna()]\n",
    "\n",
    "# creatng label column\n",
    "dat_train['label'] = dat_train['outcome_hospitalization'].astype(int)\n",
    "dat_expl['label']  = dat_expl['outcome_hospitalization'].astype(int)\n",
    "dat_test['label']  = dat_test['outcome_hospitalization'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIGvhrxwWF_0",
    "outputId": "cfd9adf6-2a2e-4834-ee34-a8454afee8e6"
   },
   "outputs": [],
   "source": [
    "subset_dat_train = dat_train.sample(frac=0.1, random_state=42)\n",
    "subset_dat_train.reset_index(drop=True, inplace=True)\n",
    "subset_dat_expl = dat_expl.sample(frac=0.1, random_state=42)\n",
    "subset_dat_expl.reset_index(drop=True, inplace=True)\n",
    "subset_dat_test = dat_test.sample(frac=0.1, random_state=42)\n",
    "subset_dat_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Original shape: {dat_train.shape}, New shape: {subset_dat_train.shape}\")\n",
    "print(f\"Original shape: {dat_expl.shape}, New shape: {subset_dat_expl.shape}\")\n",
    "print(f\"Original shape: {dat_test.shape}, New shape: {subset_dat_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "id": "dAdgnQb2Ye2c",
    "outputId": "e2a771c0-56e6-4d82-9b68-4cddd3795c14"
   },
   "outputs": [],
   "source": [
    "subset_dat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvxxvWW3kofj",
    "outputId": "36cc0534-1649-4e81-a358-b8c09ed05c4a"
   },
   "outputs": [],
   "source": [
    "faim_obj = FAIMGenerator(\n",
    "  dat_train,\n",
    "  selected_vars=colnames,\n",
    "  selected_vars_cat=x_names_cat,\n",
    "  y_name=\"label\",\n",
    "  sen_name=sen,\n",
    "  sen_var_ref = sen_ref,\n",
    "  criterion=\"auc\", m=800, n_final=200, output_dir=output_dir, without_sen=\"auto\", pre=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DU7Fykatk5L-"
   },
   "source": [
    "# Step 2: Fairness transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416,
     "referenced_widgets": [
      "f5e3589c6d434be8b0eee0a2f8441340",
      "e68dcfeb566e44328062aa8fe15fc547",
      "8b11b961ff5f427d95c1e17e57d38248",
      "49bcb53c78134ee3a522a5443b1ee11c",
      "da157c066d2848bdb2c0956da24b74b3",
      "3af8f74b38ad410d8154bd0a867cdd76",
      "cdcd171ebc00441a9da48cd912a0c64b",
      "098e22fac3b1428b92e29dfbbcf6a166",
      "af465caa0a9d4215ba56bbbea635a9a2",
      "052aeffdd07249f095723a6f7321c7e1",
      "a400c735ba394652919f915bb99724de"
     ]
    },
    "id": "zIeCz7cvk_LH",
    "outputId": "25322ea9-6aae-4fb6-a242-cad49edef74a"
   },
   "outputs": [],
   "source": [
    "# takes around 60 minutes to run\n",
    "faim_obj.FAIM_model(dat_expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "id": "bC_MFMIxlKbu",
    "outputId": "6ed3f7f7-237d-4c56-9641-734292037bf2"
   },
   "outputs": [],
   "source": [
    "best_results, fair_idx_df = faim_obj.transmit(\n",
    "  targeted_metrics = [\"Equalized Odds\", \"Equal Opportunity\", \"BER Equality\"]\n",
    "  )\n",
    "print(best_results['best_sen_exclusion'])\n",
    "print(best_results['best_coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcDTnkcYlx67"
   },
   "outputs": [],
   "source": [
    "pred_test, fairmetrics_faim, fairsummary_faim = faim_obj.test(dat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LA5FtySGlx68",
    "outputId": "2ecf6f46-873b-49e6-d46e-9e40c419165c"
   },
   "outputs": [],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "X-2Q-aAYlx68",
    "outputId": "71090138-e81b-48c8-e1f7-bc1be3de67de"
   },
   "outputs": [],
   "source": [
    "fairmetrics_faim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "UXKJ3oRElx69",
    "outputId": "5ad9d19d-7296-46be-ea17-82b4a49d972f"
   },
   "outputs": [],
   "source": [
    "fairsummary_faim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcG8xktKl43Z"
   },
   "source": [
    "# Step 3: Shap-based model explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e82b6dcd2fe44ed28902913fff21a1f5",
      "83a0a10e2e3b42b589aa3b48b6b6ecf0",
      "4c9d0356818645e29a34babc4d5d7a53",
      "522a52a7ba7845c082d5cdd49da58b42",
      "548c3966f9054cea9f64f59ef4a363f2",
      "636df08d9edb449c9a8139679233ecb4",
      "6c9761f61afe4db0a2e6f43017f7745d",
      "34d15114dfef4f26ac382ef416964fda",
      "9d60e408002540bfa4896bb7332fb529",
      "65159c26b644409e9c0ee14ca6ec7de1",
      "01a1a222e0f5427689e3b49b3ee58fa0",
      "a6816bd6bc254129a5530d4c6dc3ff2e",
      "ed1571b426c7492391e8a4e52850f377",
      "fb760423ca3b4e058148979144bfee3c",
      "7d21e814cd9843e6b013c6c8bec1603f",
      "719ab95497f244158a2d83b8f9a57adc",
      "c1b4da4b898b490887db37dd3517957a",
      "eb1c3094892b48e5b55d01342000b566",
      "c309434b23bf4fd68ea2ebf7c7fcbb39",
      "75d68be798544f07a919cbf548811a96",
      "e692080b762c47649f104c103ebb6f73",
      "a5fadbcb5cbe4a15b90a826e3511aefd"
     ]
    },
    "id": "V6hlYesul43a",
    "outputId": "29353430-56f0-4d31-a5b1-8e4151b34dd5"
   },
   "outputs": [],
   "source": [
    "shap_compare = faim_obj.compare_explain(overide=True)\n",
    "shap_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_E4lrHwl44b"
   },
   "source": [
    "# Comparison with other bias-mitigation methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcEfME8el44h"
   },
   "outputs": [],
   "source": [
    "fairbase = FairBase(\n",
    "  dat_train,\n",
    "  selected_vars=colnames,\n",
    "  selected_vars_cat=x_names_cat,\n",
    "  y_name=\"label\",\n",
    "  sen_name=sen,\n",
    "  sen_var_ref=sen_ref,\n",
    "  weighted=True,\n",
    "  weights={\"tnr\": 0.5, \"tpr\": 0.5}\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFFVKK7wl44i"
   },
   "source": [
    "### Original LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "Z4WeVTWsl44j",
    "outputId": "da40292a-a92c-4930-9199-1c32af44fd93"
   },
   "outputs": [],
   "source": [
    "lr_results = fairbase.model(method=\"OriginalLR\")\n",
    "dat_test = dat_test.reset_index(drop=True)\n",
    "pred_ori, fairmetrics_ori, clametrics_ori = fairbase.test(dat_test, model=lr_results)\n",
    "fairmetrics_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "180ek4Yp6C2c"
   },
   "source": [
    "###Underblindness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3UwuFrV6Cb6"
   },
   "outputs": [],
   "source": [
    "m_unaw = fairbase.model(method=\"Unawareness\")\n",
    "pred_unaw, fair_unaw, cla_unaw = fairbase.test(dat_test, model=m_unaw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "68555316c26543a8b826d4b8beea1de0",
      "ca8d53137c95448b83f74745d902ec99",
      "5ed2623d98c94223a111bbca89984679",
      "7b64730ec55143cb9bca54c1111c15ce",
      "9c3adca87fdd47f3a78dd592f5e85cc2",
      "b94ac81c113349b7bbf84ad5732e9856",
      "e62191aa5bb34b85978478d7879fbf59",
      "5f337057c9f348ca818f6cb74428fa6c",
      "346033de56a04f9286427cf3aff35708",
      "6e85c023df5747898c18cf08cf3b2503",
      "b534b608c1464f2386b9bbfd0a1795ca"
     ]
    },
    "id": "tC0yax41PW1D",
    "outputId": "21179e37-55e5-4a89-f777-cee9d4d7c54c"
   },
   "outputs": [],
   "source": [
    "# Prepare Data for SHAP\n",
    "\n",
    "# Prepare the data (exclude sensitive variables for unawareness)\n",
    "X_test_for_shap = dat_test[colnames].copy()\n",
    "\n",
    "# Remove sensitive variables\n",
    "sen_vars_to_remove = sen  # ['sex', 'race']\n",
    "X_test_features = X_test_for_shap.drop(columns=sen_vars_to_remove, errors='ignore')\n",
    "\n",
    "# Get the training data (also without sensitive variables)\n",
    "X_train_for_shap = dat_train[colnames].copy()\n",
    "X_train_features = X_train_for_shap.drop(columns=sen_vars_to_remove, errors='ignore')\n",
    "\n",
    "# Handle categorical variables - convert to numeric for SHAP\n",
    "X_train_encoded = pd.get_dummies(X_train_features, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test_features, drop_first=True)\n",
    "\n",
    "# Ensure test set has same columns as training set\n",
    "missing_cols = set(X_train_encoded.columns) - set(X_test_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    X_test_encoded[col] = 0\n",
    "X_test_encoded = X_test_encoded[X_train_encoded.columns]\n",
    "\n",
    "print(f\"Training data shape: {X_train_encoded.shape}\")\n",
    "print(f\"Test data shape: {X_test_encoded.shape}\")\n",
    "print(f\"Model type: {type(m_unaw)}\")\n",
    "print(f\"Model expects {len(m_unaw.params)} parameters\")\n",
    "\n",
    "# ==========================================\n",
    "# Create SHAP Explainer (FIXED)\n",
    "# ==========================================\n",
    "\n",
    "# Create a prediction function for SHAP that works with statsmodels\n",
    "def predict_fn(X):\n",
    "    \"\"\"\n",
    "    Fully robust prediction wrapper for statsmodels GLM + SHAP KernelExplainer\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert input to numpy array\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_arr = X.values\n",
    "    else:\n",
    "        X_arr = np.asarray(X)\n",
    "\n",
    "    # FORCE 2D\n",
    "    if X_arr.ndim == 1:\n",
    "        X_arr = X_arr.reshape(1, -1)\n",
    "\n",
    "    # Rebuild DataFrame with correct columns\n",
    "    X_df = pd.DataFrame(X_arr, columns=X_train_encoded.columns)\n",
    "\n",
    "    # Add constant\n",
    "    X_df = sm.add_constant(X_df, has_constant=\"add\")\n",
    "\n",
    "    # ENSURE correct column order (CRITICAL)\n",
    "    X_df = X_df[m_unaw.model.exog_names]\n",
    "\n",
    "    # FORCE numeric dtype (CRITICAL)\n",
    "    X_df = X_df.astype(float)\n",
    "\n",
    "    # Predict probabilities\n",
    "    return m_unaw.predict(X_df).values\n",
    "\n",
    "# Use KernelExplainer for statsmodels\n",
    "print(\"\\nCalculating SHAP values (this may take a few minutes)...\")\n",
    "background = shap.sample(X_train_encoded, 50)  # Sample without constant\n",
    "explainer = shap.KernelExplainer(predict_fn, background)\n",
    "\n",
    "# Calculate SHAP values for a subset of test data\n",
    "n_samples = min(200, len(X_test_encoded))\n",
    "print(f\"Using {n_samples} test samples...\")\n",
    "shap_values = explainer.shap_values(X_test_encoded[:n_samples])\n",
    "print(\" SHAP values calculated successfully!\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# Aggregate SHAP Values (mean absolute)\n",
    "# ==========================================\n",
    "\n",
    "# Take mean absolute SHAP value across all samples for each feature\n",
    "shap_agg = np.mean(np.abs(shap_values), axis=0)\n",
    "\n",
    "# Create DataFrame with features and their importance\n",
    "feature_names = X_test_encoded.columns\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap': shap_agg\n",
    "}).sort_values('shap', ascending=False)\n",
    "\n",
    "# ==========================================\n",
    "# Clean Feature Labels\n",
    "# ==========================================\n",
    "\n",
    "def clean_label(lbl, max_len=40):\n",
    "    \"\"\"Clean feature names for better visualization\"\"\"\n",
    "    lbl = lbl.replace(\"_\", \" \")\n",
    "    if len(lbl) > max_len:\n",
    "        return lbl[:max_len] + \"...\"\n",
    "    return lbl\n",
    "\n",
    "shap_df['feature'] = shap_df['feature'].apply(clean_label)\n",
    "\n",
    "# ==========================================\n",
    "# Create Bar Plot\n",
    "# ==========================================\n",
    "\n",
    "# Optional: Select top N features (set to None to show all)\n",
    "top_n = 15  # Change this or set to None to show all features\n",
    "\n",
    "def clean_spines(ax):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(True)\n",
    "    ax.spines[\"bottom\"].set_visible(True)\n",
    "\n",
    "def add_bar_labels(ax, values, fmt=\"{:.4f}\", padding=3):\n",
    "    \"\"\"Add numeric labels to horizontal bar plots.\"\"\"\n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(\n",
    "            v,\n",
    "            i,\n",
    "            fmt.format(v),\n",
    "            va=\"center\",\n",
    "            ha=\"left\",\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "if top_n is not None:\n",
    "    shap_df_plot = shap_df.head(top_n)\n",
    "else:\n",
    "    shap_df_plot = shap_df\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.barh(\n",
    "    shap_df_plot['feature'],\n",
    "    shap_df_plot['shap'],\n",
    "    color=\"#FFC20A\"\n",
    ")\n",
    "\n",
    "ax.invert_yaxis()  # Highest importance at top\n",
    "ax.set_title(\n",
    "    \"Unawareness Model - Feature Importance (SHAP)\",\n",
    "    fontsize=16,\n",
    "    weight=\"bold\",\n",
    "    pad=20\n",
    ")\n",
    "ax.set_xlabel(\"Mean |SHAP value|\", fontsize=12)\n",
    "ax.tick_params(axis='y', labelsize=11)\n",
    "add_bar_labels(ax, shap_df_plot['shap'].values)\n",
    "clean_spines(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"{output_dir}/shap_unawareness.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n Plot saved to: {output_dir}/shap_unawareness.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# Display Feature Importance Table\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nTop Feature Importances (Unawareness Model):\")\n",
    "print(\"=\" * 60)\n",
    "print(shap_df.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8gXdRqQQKK4"
   },
   "source": [
    "dbp highly important to unawareness model, but not to faim. if we look at correlation matrix, dbp has high correlation with race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqpzW7lDl44p"
   },
   "source": [
    "### Reweigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "DoBQSt-pl44q",
    "outputId": "32b62ae6-06b2-4614-8e47-872a549a76f7"
   },
   "outputs": [],
   "source": [
    "_, rw_results, _ = fairbase.model(method_type=\"pre\", method=\"Reweigh\", label_names=[\"label\"])\n",
    "pred_rw, fairmetrics_rw, clametrics_rw = fairbase.test(dat_test, model=rw_results)\n",
    "fairmetrics_rw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8CEptT46Lm2"
   },
   "source": [
    "###In-processing: Reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBFMoO5k6P5k"
   },
   "outputs": [],
   "source": [
    "m_red = fairbase.model(method_type=\"in\", method=\"Reductions\")\n",
    "pred_red, fair_red, cla_red = fairbase.test(dat_test, model=m_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBc6ixgr6QZ9"
   },
   "source": [
    "###Post-processing: Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-eUTQoB6XPZ"
   },
   "outputs": [],
   "source": [
    "m_eq = fairbase.model(method_type=\"post\", method=\"EqOdds\", dat_expl=dat_expl)\n",
    "pred_eq, fair_eq, cla_eq = fairbase.test(dat_test, model=m_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC9G37jt6ghs"
   },
   "source": [
    "###Post-processing: Calibrated Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruQP-Xgp6hg7"
   },
   "outputs": [],
   "source": [
    "m_cal = fairbase.model(\n",
    "    method_type=\"post\",\n",
    "    method=\"CalEqOdds\",\n",
    "    dat_expl=dat_expl,\n",
    "    cost_constraint=\"weighted\",\n",
    ")\n",
    "pred_cal, fair_cal, cla_cal = fairbase.test(dat_test, model=m_cal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PbP9UEK6l6N"
   },
   "source": [
    "###Post-processing: Reject Option Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcFEqLvf6mbp"
   },
   "outputs": [],
   "source": [
    "m_roc = fairbase.model(\n",
    "    method_type=\"post\",\n",
    "    method=\"ROC\",\n",
    "    dat_expl=dat_expl,\n",
    "    metric_name=\"Equal opportunity difference\",\n",
    "    ub=0.05,\n",
    "    lb=-0.05,\n",
    ")\n",
    "pred_roc, fair_roc, cla_roc = fairbase.test(dat_test, model=m_roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4N8bMwSl44r"
   },
   "source": [
    "### Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13mzZOLG6uCA"
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Original\": (pred_ori, fairmetrics_ori, clametrics_ori),\n",
    "    \"Unawareness\": (pred_unaw, fair_unaw, cla_unaw),\n",
    "    \"Reweighing\": (pred_rw, fairmetrics_rw, clametrics_rw),\n",
    "    \"Reductions\": (pred_red, fair_red, cla_red),\n",
    "    \"EqOdds\": (pred_eq, fair_eq, cla_eq),\n",
    "    \"CalEqOdds\": (pred_cal, fair_cal, cla_cal),\n",
    "    \"ROC\": (pred_roc, fair_roc, cla_roc),\n",
    "    \"FAIM\": (pred_test, fairmetrics_faim, fairsummary_faim),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "5n5FPDn02aZW",
    "outputId": "87631b4f-7f6d-4597-dd95-2519eed86829"
   },
   "outputs": [],
   "source": [
    "# Read the adversarial results CSV\n",
    "#adv_results = pd.read_csv(\"test_metrics_adv.csv\")\n",
    "\n",
    "# Extract the fairness metrics you want (let's use the best performing row)\n",
    "# You can choose based on which metric is most important to you\n",
    "# For example, let's use the row with lowest equalized_odds_diff\n",
    "#best_adv_idx = adv_results['equalized_odds_diff'].abs().idxmin()\n",
    "#adv_metrics = adv_results.loc[best_adv_idx]\n",
    "\n",
    "# Create a row for the adversarial method\n",
    "# Match the column names to your fairmetrics format\n",
    "# You'll need to map the adversarial metric names to your fairness metric names\n",
    "'''adv_row = pd.Series({\n",
    "    'Equal Opportunity': adv_metrics['equal_opportunity_diff'],\n",
    "    'Equalized Odds': adv_metrics['equalized_odds_diff'],\n",
    "    'BER Equality': adv_metrics['ber_equality_diff'],\n",
    "    'Statistical Parity': adv_metrics['statistical_parity_diff'],\n",
    "    'Accuracy Equality': adv_metrics['accuracy_equality_diff'],\n",
    "})'''\n",
    "\n",
    "# Add to your comparison dataframe\n",
    "faircompare_df = pd.concat([\n",
    "    fairmetrics_ori,\n",
    "    fair_unaw,\n",
    "    fairmetrics_rw,\n",
    "    fair_red,\n",
    "    fair_eq,\n",
    "    fair_cal,\n",
    "    fair_roc,\n",
    "    fairmetrics_faim\n",
    "    #adv_row.to_frame().T  # Convert series to dataframe row\n",
    "])\n",
    "\n",
    "faircompare_df.index = [\n",
    "    \"Original\",\n",
    "    \"Unawareness\",\n",
    "    \"Reweighing\",\n",
    "    \"Reductions\",\n",
    "    \"EqOdds\",\n",
    "    \"CalEqOdds\",\n",
    "    \"ROC\",\n",
    "    \"FAIM\"\n",
    "    #\"Adnet\"\n",
    "]\n",
    "\n",
    "faircompare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lggLxk34tE78"
   },
   "source": [
    "Equal Opportunity: max TPR gap between any two sensitive groups (within a sensitive attribute)\\\n",
    "Equalized Odds: worst-case fairness gap\\\n",
    "Statistical Parity: max SR gap\\\n",
    "Accuracy Equality: max accuracy diff \\\n",
    "BER Equality: weighed combination of worst TPR/TNR gaps\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "Pv1cwSbpxn42",
    "outputId": "e9282f43-078e-45a9-8495-9475fdac2b4f"
   },
   "outputs": [],
   "source": [
    "'''adv_row = pd.Series({\n",
    "    'auc': adv_metrics['auc'],\n",
    "    'auc_low': adv_metrics['auc_low'],\n",
    "    'auc_high': adv_metrics['auc_high'],\n",
    "    'sensitivity': adv_metrics['recall'],\n",
    "    'specificity': adv_metrics['specificity'],\n",
    "})'''\n",
    "\n",
    "# Extract fairmetrics from each result\n",
    "faircompare_df = pd.concat([\n",
    "    clametrics_ori,\n",
    "    cla_unaw,\n",
    "    clametrics_rw,\n",
    "    cla_red,\n",
    "    cla_eq,\n",
    "    cla_cal,\n",
    "    cla_roc,\n",
    "    fairsummary_faim,\n",
    "])\n",
    "\n",
    "# Set descriptive index names\n",
    "faircompare_df.index = [\n",
    "    \"Original\",\n",
    "    \"Unawareness\",\n",
    "    \"Reweighing\",\n",
    "    \"Reductions\",\n",
    "    \"EqOdds\",\n",
    "    \"CalEqOdds\",\n",
    "    \"ROC\",\n",
    "    \"FAIM\",\n",
    "]\n",
    "\n",
    "faircompare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT0nu5dVHt3F"
   },
   "source": [
    "# Bias Analysis (Baseline) on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y1TuBgsP73b"
   },
   "outputs": [],
   "source": [
    "y_true = dat_test[\"label\"]\n",
    "\n",
    "y_pred_prob_baseline = faim_obj.optim_model.predict(\n",
    "    params=faim_obj.optim_results.params,\n",
    "    exog=faim_obj.data_process(dat_test, faim_obj.vars, faim_obj.vars_cat)[0]\n",
    ")\n",
    "\n",
    "# USE OPTIMAL THRESHOLD (not 0.5!)\n",
    "threshold_baseline = find_optimal_cutoff(y_true, y_pred_prob_baseline, method=\"auc\")[0]\n",
    "y_pred_bin_baseline = (y_pred_prob_baseline > threshold_baseline).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcUXKe8hHvO5"
   },
   "outputs": [],
   "source": [
    "# Reset indices\n",
    "y_true = y_true.reset_index(drop=True)\n",
    "y_pred = pd.Series(y_pred_prob_baseline).reset_index(drop=True)\n",
    "y_pred_bin = pd.Series(y_pred_bin_baseline).reset_index(drop=True)\n",
    "\n",
    "# sex\n",
    "fair_sex_baseline = FAIMEvaluator(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    y_pred_bin=y_pred_bin,\n",
    "    sen_var=dat_test[\"Sex\"],\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n",
    "\n",
    "# Race\n",
    "fair_race_baseline = FAIMEvaluator(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    y_pred_bin=y_pred_bin,\n",
    "    sen_var=dat_test[\"Race\"],\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n",
    "\n",
    "# intersectional sensitive attribute\n",
    "dat_test[\"Sex_Race\"] = dat_test[\"Sex\"].astype(str) + \"_\" + dat_test[\"Race\"].astype(str)\n",
    "\n",
    "fair_sex_race_baseline = FAIMEvaluator(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    y_pred_bin=y_pred_bin,\n",
    "    sen_var=dat_test[\"Sex_Race\"],\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "yL8OyLePH7-m",
    "outputId": "58e74be9-dac9-47d0-95e4-21ed6878d205"
   },
   "outputs": [],
   "source": [
    "fair_sex_baseline.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "bOqqC0PcIAEh",
    "outputId": "0865a509-873a-41d3-ecea-efe6053a884a"
   },
   "outputs": [],
   "source": [
    "fair_race_baseline.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "qTqLogwTH7Qo",
    "outputId": "741e2f62-f9f2-4a64-a951-c995caaabbf0"
   },
   "outputs": [],
   "source": [
    "fair_sex_race_baseline.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "EvtC470UCRUq",
    "outputId": "545c419e-0af9-4cbb-8625-2c758bdceb67"
   },
   "outputs": [],
   "source": [
    "fair_sex_race_baseline.disparity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "xgu0CHCeCb8t",
    "outputId": "551bbbb0-3f83-4762-d2f4-70c13cda6792"
   },
   "outputs": [],
   "source": [
    "fair_race_baseline.disparity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "lkUpkUF5E3OV",
    "outputId": "78a7dba5-0449-4248-a0fc-97ea9478c87b"
   },
   "outputs": [],
   "source": [
    "fair_sex_baseline.disparity_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF6zeKdQ1GVp"
   },
   "source": [
    "#Bias Analysis (FAIM) on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Us1XjDUoTwRJ"
   },
   "outputs": [],
   "source": [
    "y_true = dat_test[\"label\"]\n",
    "\n",
    "excluded_vars = faim_obj.best_sen_exclusion.split(\"_\")\n",
    "selected_vars = [i for i in faim_obj.vars if i not in excluded_vars]\n",
    "selected_vars_cat = [i for i in faim_obj.vars_cat if i not in excluded_vars]\n",
    "\n",
    "y_pred_prob_faim = faim_obj.best_optim_base_obj.model_optim.model.predict(\n",
    "    params=faim_obj.best_coef,\n",
    "    exog=faim_obj.data_process(dat_test, selected_vars, selected_vars_cat)[0]\n",
    ")\n",
    "\n",
    "# USE OPTIMAL THRESHOLD\n",
    "threshold_faim = find_optimal_cutoff(y_true, y_pred_prob_faim, method=\"auc\")[0]\n",
    "y_pred_bin_faim = (y_pred_prob_faim > threshold_faim).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "hGFBBXs81UHz",
    "outputId": "cde635aa-1f06-4b9c-f6ad-720b2f2ac201"
   },
   "outputs": [],
   "source": [
    "# Bias Analysis (FAIM) on Test set\n",
    "\n",
    "# On test set only\n",
    "dat_test_reset = dat_test.reset_index(drop=True)\n",
    "\n",
    "# Reset indices\n",
    "y_true = dat_test_reset[\"label\"]                # target\n",
    "y_pred = pd.Series(y_pred_prob_faim).reset_index(drop=True)\n",
    "y_pred_bin = pd.Series(y_pred_bin_faim).reset_index(drop=True)\n",
    "\n",
    "# sex\n",
    "fair_sex_faim = FAIMEvaluator(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    y_pred_bin=y_pred_bin,\n",
    "    sen_var=dat_test[\"Sex\"],\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n",
    "\n",
    "# Race\n",
    "fair_race_faim = FAIMEvaluator(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    y_pred_bin=y_pred_bin,\n",
    "    sen_var=dat_test[\"Race\"],\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n",
    "\n",
    "# Intersectional sensitive attribute\n",
    "dat_test[\"Sex_Race\"] = dat_test[\"Sex\"].astype(str) + \"_\" + dat_test[\"Race\"].astype(str)\n",
    "\n",
    "fair_sex_race_faim = FAIMEvaluator(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    y_pred_bin=y_pred_bin,\n",
    "    sen_var=dat_test[\"Sex_Race\"],\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n",
    "\n",
    "# View results\n",
    "fair_sex_faim.fairsummary.by_group\n",
    "fair_race_faim.fairsummary.by_group\n",
    "fair_sex_race_faim.fairsummary.by_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "K0kor5N_y8h7",
    "outputId": "86cfd50e-3802-4f16-9de7-8a79d27477da"
   },
   "outputs": [],
   "source": [
    "fair_race_faim.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "BFmpFCgWzBig",
    "outputId": "d41a8d05-f011-4c99-a669-0295d63d7e3e"
   },
   "outputs": [],
   "source": [
    "fair_sex_faim.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "_4PNOWZ6DDgB",
    "outputId": "01670994-34b4-4f43-a51f-7abc43634622"
   },
   "outputs": [],
   "source": [
    "fair_sex_race_faim.disparity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "CggscOo6DPk-",
    "outputId": "a9970bf3-aa66-43f5-fb52-2d0686bd3bf1"
   },
   "outputs": [],
   "source": [
    "fair_race_faim.disparity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "CGnDEfUuEi1E",
    "outputId": "d84bcbba-b04a-4612-ba52-986772373f32"
   },
   "outputs": [],
   "source": [
    "fair_sex_faim.disparity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnkgQ_SOc0a-",
    "outputId": "8fabe9b9-5f67-44fc-e475-7e44a16bb902"
   },
   "outputs": [],
   "source": [
    "# verify the sizes match\n",
    "print(f\"dat_test size: {len(dat_test)}\")\n",
    "print(f\"y_pred_bin size: {len(y_pred_bin)}\")\n",
    "print(f\"y_true size: {len(y_true)}\")\n",
    "\n",
    "# Regenerate test set predictions to be sure\n",
    "excluded_vars = faim_obj.best_sen_exclusion.split(\"_\")\n",
    "selected_vars = [i for i in faim_obj.vars if i not in excluded_vars]\n",
    "selected_vars_cat = [i for i in faim_obj.vars_cat if i not in excluded_vars]\n",
    "\n",
    "y_pred_test = faim_obj.best_optim_base_obj.model_optim.model.predict(\n",
    "    params=faim_obj.best_coef,\n",
    "    exog=faim_obj.data_process(dat_test, selected_vars, selected_vars_cat)[0]\n",
    ")\n",
    "y_pred_bin_test = (y_pred_test > 0.5).astype(int)\n",
    "y_true_test = dat_test[\"label\"].values\n",
    "\n",
    "# Verify sizes now match\n",
    "print(f\"\\nAfter regeneration:\")\n",
    "print(f\"dat_test size: {len(dat_test)}\")\n",
    "print(f\"y_pred_bin_test size: {len(y_pred_bin_test)}\")\n",
    "print(f\"y_true_test size: {len(y_true_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqKJoWHGd-jp",
    "outputId": "4b458032-05cf-4cc2-c3c9-504527d6243e"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def compute_tpr_ci(y_true, y_pred_bin, subgroup_mask):\n",
    "    \"\"\"Compute 95% CI for TPR in a subgroup\"\"\"\n",
    "    y_true_sub = y_true[subgroup_mask.values]\n",
    "    y_pred_sub = y_pred_bin[subgroup_mask.values]\n",
    "\n",
    "    # TPR = TP / (TP + FN)\n",
    "    positives = y_true_sub == 1\n",
    "    n_pos = positives.sum()\n",
    "\n",
    "    if n_pos == 0:\n",
    "        return None, None, None, 0\n",
    "\n",
    "    tp = ((y_true_sub == 1) & (y_pred_sub == 1)).sum()\n",
    "    tpr = tp / n_pos\n",
    "\n",
    "    # Wilson score interval for binomial proportion\n",
    "    if n_pos < 5:  # Too small for reliable CI\n",
    "        return tpr, np.nan, np.nan, n_pos\n",
    "\n",
    "    ci = stats.binom.interval(0.95, n_pos, tpr)\n",
    "    ci_lower, ci_upper = ci[0] / n_pos, ci[1] / n_pos\n",
    "\n",
    "    return tpr, ci_lower, ci_upper, n_pos\n",
    "\n",
    "# Get ALL unique subgroups from test set\n",
    "all_subgroups = dat_test['Sex_Race'].unique()\n",
    "\n",
    "print(f\"Found {len(all_subgroups)} unique subgroups in test set:\")\n",
    "print(sorted(all_subgroups))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test Set TPR with 95% Confidence Intervals:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for subgroup in sorted(all_subgroups):\n",
    "    mask = dat_test['Sex_Race'] == subgroup\n",
    "    n_subgroup = mask.sum()\n",
    "\n",
    "    tpr, ci_low, ci_high, n_pos = compute_tpr_ci(\n",
    "        y_true_test,\n",
    "        y_pred_bin_test,\n",
    "        mask\n",
    "    )\n",
    "\n",
    "    if tpr is not None:\n",
    "        results.append({\n",
    "            'Subgroup': subgroup,\n",
    "            'N_Total': n_subgroup,\n",
    "            'N_Positive': n_pos,\n",
    "            'TPR': tpr,\n",
    "            'CI_Lower': ci_low,\n",
    "            'CI_Upper': ci_high,\n",
    "            'CI_Width': ci_high - ci_low if not np.isnan(ci_high) else np.nan\n",
    "        })\n",
    "\n",
    "        if not np.isnan(ci_low):\n",
    "            print(f\"{subgroup:20s} (n={n_subgroup:4d}, pos={n_pos:3d}): TPR={tpr:.3f} [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "        else:\n",
    "            print(f\"{subgroup:20s} (n={n_subgroup:4d}, pos={n_pos:3d}): TPR={tpr:.3f} [insufficient sample]\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('TPR', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANKED by TPR (highest to lowest):\")\n",
    "print(results_df[['Subgroup', 'N_Total', 'N_Positive', 'TPR', 'CI_Lower', 'CI_Upper']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Small subgroups (Total N < 8000):\")\n",
    "small = results_df[results_df['N_Total'] < 8000]\n",
    "print(small[['Subgroup', 'N_Total', 'N_Positive', 'TPR', 'CI_Width']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERLAP ANALYSIS - Top 3 performers:\")\n",
    "\n",
    "if len(results_df) >= 3:\n",
    "    top3 = results_df.head(3)\n",
    "\n",
    "    for i, row in top3.iterrows():\n",
    "        print(f\"{row['Subgroup']:20s}: TPR={row['TPR']:.3f} [{row['CI_Lower']:.3f}, {row['CI_Upper']:.3f}]\")\n",
    "\n",
    "    print(\"\\nPairwise overlap checks:\")\n",
    "    # Check top vs second\n",
    "    top1 = results_df.iloc[0]\n",
    "    top2 = results_df.iloc[1]\n",
    "    top3_row = results_df.iloc[2]\n",
    "\n",
    "    overlap_1_2 = (top1['CI_Lower'] <= top2['CI_Upper']) and (top2['CI_Lower'] <= top1['CI_Upper'])\n",
    "    overlap_1_3 = (top1['CI_Lower'] <= top3_row['CI_Upper']) and (top3_row['CI_Lower'] <= top1['CI_Upper'])\n",
    "    overlap_2_3 = (top2['CI_Lower'] <= top3_row['CI_Upper']) and (top3_row['CI_Lower'] <= top2['CI_Upper'])\n",
    "\n",
    "    print(f\"  {top1['Subgroup']} vs {top2['Subgroup']}: {'OVERLAP ' if overlap_1_2 else 'DISTINCT '}\")\n",
    "    print(f\"  {top1['Subgroup']} vs {top3_row['Subgroup']}: {'OVERLAP ' if overlap_1_3 else 'DISTINCT '}\")\n",
    "    print(f\"  {top2['Subgroup']} vs {top3_row['Subgroup']}: {'OVERLAP ' if overlap_2_3 else 'DISTINCT '}\")\n",
    "\n",
    "    if overlap_1_2 and overlap_1_3:\n",
    "        print(\"\\n Top 3 performers are NOT significantly different (overlapping CIs)\")\n",
    "    else:\n",
    "        print(f\"\\n {top1['Subgroup']} is significantly better than some others\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORST PERFORMER:\")\n",
    "worst = results_df.iloc[-1]\n",
    "print(f\"{worst['Subgroup']:20s}: TPR={worst['TPR']:.3f} [{worst['CI_Lower']:.3f}, {worst['CI_Upper']:.3f}]\")\n",
    "print(f\"  Total patients: {worst['N_Total']}, Positive cases: {worst['N_Positive']}\")\n",
    "\n",
    "# Check if worst is significantly different from second worst\n",
    "if len(results_df) >= 2:\n",
    "    second_worst = results_df.iloc[-2]\n",
    "    overlap_worst = (worst['CI_Lower'] <= second_worst['CI_Upper']) and (second_worst['CI_Lower'] <= worst['CI_Upper'])\n",
    "\n",
    "    print(f\"\\nSecond worst: {second_worst['Subgroup']} TPR={second_worst['TPR']:.3f} [{second_worst['CI_Lower']:.3f}, {second_worst['CI_Upper']:.3f}]\")\n",
    "    print(f\"Overlap with worst: {'YES ' if overlap_worst else 'NO '}\")\n",
    "\n",
    "    if not overlap_worst:\n",
    "        print(f\" {worst['Subgroup']} is SIGNIFICANTLY worse than other groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "f4sM5tFeer4E",
    "outputId": "b30521a1-fd06-4892-829b-552abbe53804"
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "results_df_sorted = results_df.sort_values('TPR', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))  # Made wider to accommodate legend\n",
    "\n",
    "# Color code by sample size\n",
    "colors = ['red' if n < 8000 else 'steelblue' for n in results_df_sorted['N_Total']]\n",
    "\n",
    "# Plot with error bars\n",
    "y_pos = np.arange(len(results_df_sorted))\n",
    "ax.barh(y_pos, results_df_sorted['TPR'], color=colors, alpha=0.6)\n",
    "\n",
    "# Add confidence intervals\n",
    "for i, row in enumerate(results_df_sorted.itertuples()):\n",
    "    ax.plot([row.CI_Lower, row.CI_Upper], [i, i], 'k-', linewidth=2)\n",
    "    ax.plot([row.CI_Lower, row.CI_Lower], [i-0.2, i+0.2], 'k-', linewidth=2)\n",
    "    ax.plot([row.CI_Upper, row.CI_Upper], [i-0.2, i+0.2], 'k-', linewidth=2)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(results_df_sorted['Subgroup'])\n",
    "ax.set_xlabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
    "ax.set_title('Model Performance by Demographic Subgroup with 95% Confidence Intervals', fontsize=14, weight='bold')\n",
    "ax.axvline(x=0.75, color='gray', linestyle='--', alpha=0.5, label='Target threshold')\n",
    "\n",
    "# Legend - placed outside plot area on the right\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', alpha=0.6, label='Large subgroup (n8000)'),\n",
    "    Patch(facecolor='red', alpha=0.6, label='Small subgroup (n<8000, unreliable)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('subgroup_performance_with_ci.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Figure saved as 'subgroup_performance_with_ci.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q3sqb3chB6j"
   },
   "source": [
    "#Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt-NZ0Zygfv3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_gap_fixed_threshold(y_true, y_pred_prob, sen_var, threshold,\n",
    "                                   metric='tpr', n_bootstrap=1000, alpha=0.05, seed=42):\n",
    "    \"\"\"\n",
    "    Bootstrap with FIXED threshold (matches your disparity_table calculation)\n",
    "\n",
    "    This is the correct way to match your reported gaps!\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n = len(y_true)\n",
    "    groups = sen_var.unique()\n",
    "\n",
    "    # Use FIXED threshold to get predictions\n",
    "    y_pred_bin = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "    gaps = []\n",
    "\n",
    "    # Pre-convert to numpy\n",
    "    y_true_arr = y_true.values if hasattr(y_true, 'values') else np.array(y_true)\n",
    "    y_pred_arr = y_pred_bin.values if hasattr(y_pred_bin, 'values') else np.array(y_pred_bin)\n",
    "    sen_var_arr = sen_var.values if hasattr(sen_var, 'values') else np.array(sen_var)\n",
    "\n",
    "    print(f\"Running {n_bootstrap} bootstrap iterations with fixed threshold={threshold:.4f}...\")\n",
    "\n",
    "    for _ in tqdm(range(n_bootstrap), desc=\"Bootstrap\"):\n",
    "        # Resample\n",
    "        indices = np.random.choice(n, size=n, replace=True)\n",
    "\n",
    "        y_true_boot = y_true_arr[indices]\n",
    "        y_pred_boot = y_pred_arr[indices]\n",
    "        sen_var_boot = sen_var_arr[indices]\n",
    "\n",
    "        # Calculate metric for each group\n",
    "        group_metrics = []\n",
    "\n",
    "        for group in groups:\n",
    "            mask = sen_var_boot == group\n",
    "            if mask.sum() < 10:\n",
    "                continue\n",
    "\n",
    "            y_true_g = y_true_boot[mask]\n",
    "            y_pred_g = y_pred_boot[mask]\n",
    "\n",
    "            if metric == 'tpr':\n",
    "                n_pos = (y_true_g == 1).sum()\n",
    "                if n_pos == 0:\n",
    "                    continue\n",
    "                tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "                val = tp / n_pos\n",
    "            elif metric == 'fpr':\n",
    "                n_neg = (y_true_g == 0).sum()\n",
    "                if n_neg == 0:\n",
    "                    continue\n",
    "                fp = ((y_true_g == 0) & (y_pred_g == 1)).sum()\n",
    "                val = fp / n_neg\n",
    "            elif metric == 'ber':\n",
    "                n_pos = (y_true_g == 1).sum()\n",
    "                n_neg = (y_true_g == 0).sum()\n",
    "                if n_pos == 0 or n_neg == 0:\n",
    "                    continue\n",
    "                tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "                fp = ((y_true_g == 0) & (y_pred_g == 1)).sum()\n",
    "                tpr = tp / n_pos\n",
    "                fpr = fp / n_neg\n",
    "                val = 0.5 * (fpr + (1 - tpr))\n",
    "\n",
    "            group_metrics.append(val)\n",
    "\n",
    "        if len(group_metrics) >= 2:\n",
    "            gaps.append(max(group_metrics) - min(group_metrics))\n",
    "\n",
    "    gaps = np.array(gaps)\n",
    "\n",
    "    # Calculate CI\n",
    "    ci_lower = np.percentile(gaps, alpha/2 * 100)\n",
    "    ci_upper = np.percentile(gaps, (1 - alpha/2) * 100)\n",
    "    gap_mean = np.mean(gaps)\n",
    "\n",
    "    # Also calculate the OBSERVED gap (should match your table!)\n",
    "    y_pred_bin_full = (y_pred_prob > threshold).astype(int)\n",
    "    observed_metrics = []\n",
    "    for group in groups:\n",
    "        mask = sen_var == group\n",
    "        if mask.sum() < 10:\n",
    "            continue\n",
    "\n",
    "        y_true_g = y_true[mask]\n",
    "        y_pred_g = y_pred_bin_full[mask]\n",
    "\n",
    "        if metric == 'tpr':\n",
    "            n_pos = (y_true_g == 1).sum()\n",
    "            if n_pos == 0:\n",
    "                continue\n",
    "            tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "            val = tp / n_pos\n",
    "        elif metric == 'ber':\n",
    "            n_pos = (y_true_g == 1).sum()\n",
    "            n_neg = (y_true_g == 0).sum()\n",
    "            if n_pos == 0 or n_neg == 0:\n",
    "                continue\n",
    "            tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "            fp = ((y_true_g == 0) & (y_pred_g == 1)).sum()\n",
    "            tpr = tp / n_pos\n",
    "            fpr = fp / n_neg\n",
    "            val = 0.5 * (fpr + (1 - tpr))\n",
    "\n",
    "        observed_metrics.append(val)\n",
    "\n",
    "    observed_gap = max(observed_metrics) - min(observed_metrics)\n",
    "\n",
    "    return {\n",
    "        'gap': gap_mean,\n",
    "        'observed_gap': observed_gap,  # This should match your table!\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'bootstrap_dist': gaps\n",
    "    }\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# COMPREHENSIVE SIGNIFICANCE TESTS\n",
    "# ==========================================\n",
    "\n",
    "def comprehensive_significance_test(y_true,\n",
    "                                    y_pred_prob_baseline, threshold_baseline,\n",
    "                                    y_pred_prob_faim, threshold_faim,\n",
    "                                    dat_test,\n",
    "                                    metric='tpr',\n",
    "                                    n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Test significance for SEX, RACE, and INTERSECTIONS separately\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Reset indices\n",
    "    y_true = y_true.reset_index(drop=True)\n",
    "    dat_test_reset = dat_test.reset_index(drop=True)\n",
    "    y_pred_prob_baseline = pd.Series(y_pred_prob_baseline).reset_index(drop=True)\n",
    "    y_pred_prob_faim = pd.Series(y_pred_prob_faim).reset_index(drop=True)\n",
    "\n",
    "    # ===== TEST 1: SEX =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 1: SEX\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sen_sex = dat_test_reset[\"Sex\"]\n",
    "\n",
    "    baseline_sex = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_baseline, sen_sex, threshold_baseline,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=42\n",
    "    )\n",
    "\n",
    "    faim_sex = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_faim, sen_sex, threshold_faim,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=43\n",
    "    )\n",
    "\n",
    "    # Calculate p-value\n",
    "    diff_sex = baseline_sex['bootstrap_dist'] - faim_sex['bootstrap_dist']\n",
    "    p_value_sex = np.mean(diff_sex <= 0)\n",
    "\n",
    "    print(f\"\\nBaseline {metric.upper()} gap (sex): {baseline_sex['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {baseline_sex['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{baseline_sex['ci_lower']:.4f}, {baseline_sex['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nFAIM {metric.upper()} gap (sex): {faim_sex['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {faim_sex['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{faim_sex['ci_lower']:.4f}, {faim_sex['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nGap reduction: {baseline_sex['observed_gap'] - faim_sex['observed_gap']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {p_value_sex:.4f}\")\n",
    "\n",
    "    if p_value_sex < 0.05:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['sex'] = {\n",
    "        'baseline_gap': baseline_sex['observed_gap'],\n",
    "        'faim_gap': faim_sex['observed_gap'],\n",
    "        'reduction': baseline_sex['observed_gap'] - faim_sex['observed_gap'],\n",
    "        'p_value': p_value_sex,\n",
    "        'significant': p_value_sex < 0.05\n",
    "    }\n",
    "\n",
    "    # ===== TEST 2: RACE =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 2: RACE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sen_race = dat_test_reset[\"Race\"]\n",
    "\n",
    "    baseline_race = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_baseline, sen_race, threshold_baseline,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=44\n",
    "    )\n",
    "\n",
    "    faim_race = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_faim, sen_race, threshold_faim,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=45\n",
    "    )\n",
    "\n",
    "    diff_race = baseline_race['bootstrap_dist'] - faim_race['bootstrap_dist']\n",
    "    p_value_race = np.mean(diff_race <= 0)\n",
    "\n",
    "    print(f\"\\nBaseline {metric.upper()} gap (race): {baseline_race['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {baseline_race['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{baseline_race['ci_lower']:.4f}, {baseline_race['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nFAIM {metric.upper()} gap (race): {faim_race['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {faim_race['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{faim_race['ci_lower']:.4f}, {faim_race['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nGap reduction: {baseline_race['observed_gap'] - faim_race['observed_gap']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {p_value_race:.4f}\")\n",
    "\n",
    "    if p_value_race < 0.05:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['race'] = {\n",
    "        'baseline_gap': baseline_race['observed_gap'],\n",
    "        'faim_gap': faim_race['observed_gap'],\n",
    "        'reduction': baseline_race['observed_gap'] - faim_race['observed_gap'],\n",
    "        'p_value': p_value_race,\n",
    "        'significant': p_value_race < 0.05\n",
    "    }\n",
    "\n",
    "    # ===== TEST 3: INTERSECTIONS =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 3: INTERSECTIONS (SEX  RACE)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sen_intersect = dat_test_reset[\"Sex\"].astype(str) + \"_\" + dat_test_reset[\"Race\"].astype(str)\n",
    "    sen_intersect = pd.Series(sen_intersect).reset_index(drop=True)\n",
    "\n",
    "    baseline_intersect = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_baseline, sen_intersect, threshold_baseline,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=46\n",
    "    )\n",
    "\n",
    "    faim_intersect = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_faim, sen_intersect, threshold_faim,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=47\n",
    "    )\n",
    "\n",
    "    diff_intersect = baseline_intersect['bootstrap_dist'] - faim_intersect['bootstrap_dist']\n",
    "    p_value_intersect = np.mean(diff_intersect <= 0)\n",
    "\n",
    "    print(f\"\\nBaseline {metric.upper()} gap (intersections): {baseline_intersect['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {baseline_intersect['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{baseline_intersect['ci_lower']:.4f}, {baseline_intersect['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nFAIM {metric.upper()} gap (intersections): {faim_intersect['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {faim_intersect['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{faim_intersect['ci_lower']:.4f}, {faim_intersect['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nGap reduction: {baseline_intersect['observed_gap'] - faim_intersect['observed_gap']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {p_value_intersect:.4f}\")\n",
    "\n",
    "    if p_value_intersect < 0.05:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['intersections'] = {\n",
    "        'baseline_gap': baseline_intersect['observed_gap'],\n",
    "        'faim_gap': faim_intersect['observed_gap'],\n",
    "        'reduction': baseline_intersect['observed_gap'] - faim_intersect['observed_gap'],\n",
    "        'p_value': p_value_intersect,\n",
    "        'significant': p_value_intersect < 0.05\n",
    "    }\n",
    "\n",
    "    # ===== SUMMARY =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY OF SIGNIFICANCE TESTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Grouping': ['Sex', 'Race', 'Sex  Race'],\n",
    "        'Baseline Gap': [results['sex']['baseline_gap'],\n",
    "                        results['race']['baseline_gap'],\n",
    "                        results['intersections']['baseline_gap']],\n",
    "        'FAIM Gap': [results['sex']['faim_gap'],\n",
    "                    results['race']['faim_gap'],\n",
    "                    results['intersections']['faim_gap']],\n",
    "        'Reduction': [results['sex']['reduction'],\n",
    "                     results['race']['reduction'],\n",
    "                     results['intersections']['reduction']],\n",
    "        'P-value': [results['sex']['p_value'],\n",
    "                   results['race']['p_value'],\n",
    "                   results['intersections']['p_value']],\n",
    "        'Significant': [results['sex']['significant'],\n",
    "                       results['race']['significant'],\n",
    "                       results['intersections']['significant']]\n",
    "    })\n",
    "\n",
    "    print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "    n_significant = summary_df['Significant'].sum()\n",
    "    print(f\"\\n{n_significant}/3 tests show significant improvement\")\n",
    "\n",
    "    return results, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWi-Z64Wgl9-",
    "outputId": "2cd649bf-b5e2-4f51-f441-850262973b69"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Get predictions with FIXED thresholds\n",
    "y_true = dat_test[\"label\"].reset_index(drop=True)\n",
    "\n",
    "# Baseline\n",
    "X_baseline, _, _ = fairbase.data_process(dat_test)\n",
    "y_pred_prob_baseline = faim_obj.optim_model.predict(\n",
    "    params=faim_obj.optim_results.params,\n",
    "    exog=X_baseline\n",
    ")\n",
    "threshold_baseline = find_optimal_cutoff(y_true, y_pred_prob_baseline, method=\"auc\")[0]\n",
    "\n",
    "# FAIM\n",
    "excluded = faim_obj.best_sen_exclusion.split(\"_\")\n",
    "sel_vars = [v for v in faim_obj.vars if v not in excluded]\n",
    "sel_vars_cat = [v for v in faim_obj.vars_cat if v not in excluded]\n",
    "X_faim, _, _ = faim_obj.data_process(dat_test, sel_vars, sel_vars_cat)\n",
    "y_pred_prob_faim = faim_obj.best_optim_base_obj.model_optim.model.predict(\n",
    "    params=faim_obj.best_coef,\n",
    "    exog=X_faim\n",
    ")\n",
    "threshold_faim = find_optimal_cutoff(y_true, y_pred_prob_faim, method=\"auc\")[0]\n",
    "\n",
    "print(f\"Baseline threshold: {threshold_baseline:.4f}\")\n",
    "print(f\"FAIM threshold: {threshold_faim:.4f}\")\n",
    "\n",
    "# 2. Run comprehensive tests\n",
    "results, summary = comprehensive_significance_test(\n",
    "    y_true,\n",
    "    y_pred_prob_baseline, threshold_baseline,\n",
    "    y_pred_prob_faim, threshold_faim,\n",
    "    dat_test,\n",
    "    metric='tpr',  # or 'ber'\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "# 3. Check results\n",
    "print(\"\\\\nYour disparity table shows:\")\n",
    "print(\"  Baseline TPR gap (intersections): 0.2118\")\n",
    "print(\"  FAIM TPR gap (intersections): 0.1902\")\n",
    "\n",
    "print(\"\\\\nOur calculated gaps:\")\n",
    "print(f\"  Baseline: {results['intersections']['baseline_gap']:.4f}\")\n",
    "print(f\"  FAIM: {results['intersections']['faim_gap']:.4f}\")\n",
    "\n",
    "# They should MATCH now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j9A0fX4ziNHZ",
    "outputId": "f38bfd2b-3098-4e1c-a6c9-0b4d606f6b24"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PERMUTATION TESTING + VISUALIZATIONS\n",
    "# ==========================================\n",
    "\n",
    "# ==========================================\n",
    "# PERMUTATION TEST FUNCTION\n",
    "# ==========================================\n",
    "\n",
    "def permutation_test_fairness(y_true,\n",
    "                              y_pred_prob_baseline, threshold_baseline,\n",
    "                              y_pred_prob_faim, threshold_faim,\n",
    "                              sen_var,\n",
    "                              metric='tpr',\n",
    "                              n_permutations=1000,\n",
    "                              seed=42):\n",
    "    \"\"\"\n",
    "    Permutation test to compare fairness gaps between baseline and FAIM\n",
    "\n",
    "    H0: The two models have the same fairness gap\n",
    "    H1: FAIM has smaller gap than baseline\n",
    "\n",
    "    Returns p-value and permutation distribution\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    print(f\"Running {n_permutations} permutations with fixed thresholds...\")\n",
    "\n",
    "    # Convert to binary predictions with FIXED thresholds\n",
    "    y_pred_baseline = (y_pred_prob_baseline > threshold_baseline).astype(int)\n",
    "    y_pred_faim = (y_pred_prob_faim > threshold_faim).astype(int)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_true_arr = y_true.values if hasattr(y_true, 'values') else np.array(y_true)\n",
    "    y_pred_baseline_arr = y_pred_baseline.values if hasattr(y_pred_baseline, 'values') else np.array(y_pred_baseline)\n",
    "    y_pred_faim_arr = y_pred_faim.values if hasattr(y_pred_faim, 'values') else np.array(y_pred_faim)\n",
    "    sen_var_arr = sen_var.values if hasattr(sen_var, 'values') else np.array(sen_var)\n",
    "\n",
    "    # Function to calculate gap\n",
    "    def calculate_gap(y_true, y_pred, sen_var, metric):\n",
    "        groups = np.unique(sen_var)\n",
    "        group_metrics = []\n",
    "\n",
    "        for group in groups:\n",
    "            mask = sen_var == group\n",
    "            if mask.sum() < 10:\n",
    "                continue\n",
    "\n",
    "            y_true_g = y_true[mask]\n",
    "            y_pred_g = y_pred[mask]\n",
    "\n",
    "            if metric == 'tpr':\n",
    "                n_pos = (y_true_g == 1).sum()\n",
    "                if n_pos == 0:\n",
    "                    continue\n",
    "                tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "                val = tp / n_pos\n",
    "            elif metric == 'ber':\n",
    "                n_pos = (y_true_g == 1).sum()\n",
    "                n_neg = (y_true_g == 0).sum()\n",
    "                if n_pos == 0 or n_neg == 0:\n",
    "                    continue\n",
    "                tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "                fp = ((y_true_g == 0) & (y_pred_g == 1)).sum()\n",
    "                tpr = tp / n_pos\n",
    "                fpr = fp / n_neg\n",
    "                val = 0.5 * (fpr + (1 - tpr))\n",
    "\n",
    "            group_metrics.append(val)\n",
    "\n",
    "        return max(group_metrics) - min(group_metrics) if len(group_metrics) >= 2 else 0\n",
    "\n",
    "    # Calculate observed difference\n",
    "    gap_baseline = calculate_gap(y_true_arr, y_pred_baseline_arr, sen_var_arr, metric)\n",
    "    gap_faim = calculate_gap(y_true_arr, y_pred_faim_arr, sen_var_arr, metric)\n",
    "    observed_diff = gap_baseline - gap_faim\n",
    "\n",
    "    # Permutation test\n",
    "    permuted_diffs = []\n",
    "\n",
    "    for _ in tqdm(range(n_permutations), desc=\"Permutations\"):\n",
    "        # Randomly swap model labels for each observation\n",
    "        swap = np.random.rand(len(y_true_arr)) > 0.5\n",
    "\n",
    "        y_pred_perm1 = y_pred_baseline_arr.copy()\n",
    "        y_pred_perm2 = y_pred_faim_arr.copy()\n",
    "\n",
    "        # Swap predictions where swap is True\n",
    "        y_pred_perm1[swap] = y_pred_faim_arr[swap]\n",
    "        y_pred_perm2[swap] = y_pred_baseline_arr[swap]\n",
    "\n",
    "        # Calculate gaps for permuted predictions\n",
    "        gap1 = calculate_gap(y_true_arr, y_pred_perm1, sen_var_arr, metric)\n",
    "        gap2 = calculate_gap(y_true_arr, y_pred_perm2, sen_var_arr, metric)\n",
    "\n",
    "        permuted_diffs.append(gap1 - gap2)\n",
    "\n",
    "    permuted_diffs = np.array(permuted_diffs)\n",
    "\n",
    "    # Calculate p-value (one-sided: H1: baseline > FAIM)\n",
    "    p_value_one_sided = np.mean(permuted_diffs >= observed_diff)\n",
    "\n",
    "    # Two-sided p-value\n",
    "    p_value_two_sided = np.mean(np.abs(permuted_diffs) >= np.abs(observed_diff))\n",
    "\n",
    "    return {\n",
    "        'observed_diff': observed_diff,\n",
    "        'gap_baseline': gap_baseline,\n",
    "        'gap_faim': gap_faim,\n",
    "        'p_value_one_sided': p_value_one_sided,\n",
    "        'p_value_two_sided': p_value_two_sided,\n",
    "        'permuted_dist': permuted_diffs,\n",
    "        'significant': p_value_one_sided < 0.05\n",
    "    }\n",
    "\n",
    "\n",
    "def comprehensive_permutation_test(y_true,\n",
    "                                   y_pred_prob_baseline, threshold_baseline,\n",
    "                                   y_pred_prob_faim, threshold_faim,\n",
    "                                   dat_test,\n",
    "                                   metric='tpr',\n",
    "                                   n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Run permutation tests for sex, race, and intersections\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Reset indices\n",
    "    y_true = y_true.reset_index(drop=True)\n",
    "    dat_test_reset = dat_test.reset_index(drop=True)\n",
    "    y_pred_prob_baseline = pd.Series(y_pred_prob_baseline).reset_index(drop=True)\n",
    "    y_pred_prob_faim = pd.Series(y_pred_prob_faim).reset_index(drop=True)\n",
    "\n",
    "    # Test 1: Sex\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERMUTATION TEST 1: SEX\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    perm_sex = permutation_test_fairness(\n",
    "        y_true, y_pred_prob_baseline, threshold_baseline,\n",
    "        y_pred_prob_faim, threshold_faim,\n",
    "        dat_test_reset[\"Sex\"],\n",
    "        metric=metric, n_permutations=n_permutations, seed=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\nObserved baseline gap: {perm_sex['gap_baseline']:.4f}\")\n",
    "    print(f\"Observed FAIM gap: {perm_sex['gap_faim']:.4f}\")\n",
    "    print(f\"Observed difference: {perm_sex['observed_diff']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {perm_sex['p_value_one_sided']:.4f}\")\n",
    "    print(f\"P-value (two-sided): {perm_sex['p_value_two_sided']:.4f}\")\n",
    "\n",
    "    if perm_sex['significant']:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['Sex'] = perm_sex\n",
    "\n",
    "    # Test 2: Race\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERMUTATION TEST 2: RACE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    perm_race = permutation_test_fairness(\n",
    "        y_true, y_pred_prob_baseline, threshold_baseline,\n",
    "        y_pred_prob_faim, threshold_faim,\n",
    "        dat_test_reset[\"Race\"],\n",
    "        metric=metric, n_permutations=n_permutations, seed=43\n",
    "    )\n",
    "\n",
    "    print(f\"\\nObserved baseline gap: {perm_race['gap_baseline']:.4f}\")\n",
    "    print(f\"Observed FAIM gap: {perm_race['gap_faim']:.4f}\")\n",
    "    print(f\"Observed difference: {perm_race['observed_diff']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {perm_race['p_value_one_sided']:.4f}\")\n",
    "    print(f\"P-value (two-sided): {perm_race['p_value_two_sided']:.4f}\")\n",
    "\n",
    "    if perm_race['significant']:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['Race'] = perm_race\n",
    "\n",
    "    # Test 3: Intersections\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERMUTATION TEST 3: INTERSECTIONS (SEX  RACE)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sen_intersect = dat_test_reset[\"Sex\"].astype(str) + \"_\" + dat_test_reset[\"Race\"].astype(str)\n",
    "\n",
    "    perm_intersect = permutation_test_fairness(\n",
    "        y_true, y_pred_prob_baseline, threshold_baseline,\n",
    "        y_pred_prob_faim, threshold_faim,\n",
    "        sen_intersect,\n",
    "        metric=metric, n_permutations=n_permutations, seed=44\n",
    "    )\n",
    "\n",
    "    print(f\"\\nObserved baseline gap: {perm_intersect['gap_baseline']:.4f}\")\n",
    "    print(f\"Observed FAIM gap: {perm_intersect['gap_faim']:.4f}\")\n",
    "    print(f\"Observed difference: {perm_intersect['observed_diff']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {perm_intersect['p_value_one_sided']:.4f}\")\n",
    "    print(f\"P-value (two-sided): {perm_intersect['p_value_two_sided']:.4f}\")\n",
    "\n",
    "    if perm_intersect['significant']:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['intersections'] = perm_intersect\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERMUTATION TEST SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Grouping': ['Sex', 'Race', 'Sex  Race'],\n",
    "        'Baseline Gap': [perm_sex['gap_baseline'], perm_race['gap_baseline'], perm_intersect['gap_baseline']],\n",
    "        'FAIM Gap': [perm_sex['gap_faim'], perm_race['gap_faim'], perm_intersect['gap_faim']],\n",
    "        'Difference': [perm_sex['observed_diff'], perm_race['observed_diff'], perm_intersect['observed_diff']],\n",
    "        'P-value': [perm_sex['p_value_one_sided'], perm_race['p_value_one_sided'], perm_intersect['p_value_one_sided']],\n",
    "        'Significant': [perm_sex['significant'], perm_race['significant'], perm_intersect['significant']]\n",
    "    })\n",
    "\n",
    "    print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "    n_significant = summary_df['Significant'].sum()\n",
    "    print(f\"\\n{n_significant}/3 tests show significant improvement\")\n",
    "\n",
    "    return results, summary_df\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# COMPREHENSIVE VISUALIZATIONS\n",
    "# ==========================================\n",
    "\n",
    "def create_comprehensive_visualizations(bootstrap_results, permutation_results,\n",
    "                                        metric='TPR', output_dir='output'):\n",
    "    \"\"\"\n",
    "    Create publication-quality visualizations\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_palette(\"colorblind\")\n",
    "\n",
    "    groupings = ['sex', 'race', 'intersections']\n",
    "    titles = ['Sex', 'Race', 'Sex  Race']\n",
    "\n",
    "    # ==========================================\n",
    "    # FIGURE 1: Bootstrap Distributions (3x2 grid)\n",
    "    # ==========================================\n",
    "\n",
    "    fig1, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig1.suptitle(f'Bootstrap Distributions: {metric} Gap Comparison',\n",
    "                  fontsize=22, weight='bold', y=0.995)\n",
    "\n",
    "    for i, (grouping, title) in enumerate(zip(groupings, titles)):\n",
    "        # Get bootstrap distributions from results dict\n",
    "        baseline_dist = bootstrap_results[grouping]['baseline']['bootstrap_dist']\n",
    "        faim_dist = bootstrap_results[grouping]['faim']['bootstrap_dist']\n",
    "\n",
    "        baseline_gap = bootstrap_results[grouping]['baseline']['observed_gap']\n",
    "        faim_gap = bootstrap_results[grouping]['faim']['observed_gap']\n",
    "\n",
    "        # Left: Histograms\n",
    "        ax = axes[i, 0]\n",
    "        ax.hist(baseline_dist, bins=50, alpha=0.65, label='Baseline', color='darkorange', edgecolor='black', linewidth=0.5)\n",
    "        ax.hist(faim_dist, bins=50, alpha=0.65, label='FAIM', color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "        ax.axvline(baseline_gap, color='darkorange', linestyle='--', linewidth=3, label=f'Baseline: {baseline_gap:.3f}')\n",
    "        ax.axvline(faim_gap, color='steelblue', linestyle='--', linewidth=3, label=f'FAIM: {faim_gap:.3f}')\n",
    "\n",
    "        ax.set_xlabel(f'{metric} Gap', fontsize=14, weight='bold')\n",
    "        ax.set_ylabel('Frequency', fontsize=14, weight='bold')\n",
    "        ax.set_title(f'{title}: Bootstrap Distributions', fontsize=16, weight='bold', pad=10)\n",
    "        ax.legend(fontsize=11, loc='upper right', framealpha=0.9)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.tick_params(labelsize=11)\n",
    "\n",
    "        # Right: Box plots with CIs\n",
    "        ax = axes[i, 1]\n",
    "\n",
    "        data = [baseline_dist, faim_dist]\n",
    "        bp = ax.boxplot(data, labels=['Baseline', 'FAIM'], patch_artist=True, widths=0.6)\n",
    "\n",
    "        bp['boxes'][0].set_facecolor('darkorange')\n",
    "        bp['boxes'][0].set_alpha(0.7)\n",
    "        bp['boxes'][1].set_facecolor('steelblue')\n",
    "        bp['boxes'][1].set_alpha(0.7)\n",
    "\n",
    "        # Add CI markers\n",
    "        baseline_ci_lower = bootstrap_results[grouping]['baseline']['ci_lower']\n",
    "        baseline_ci_upper = bootstrap_results[grouping]['baseline']['ci_upper']\n",
    "        faim_ci_lower = bootstrap_results[grouping]['faim']['ci_lower']\n",
    "        faim_ci_upper = bootstrap_results[grouping]['faim']['ci_upper']\n",
    "\n",
    "        ax.plot([1, 1], [baseline_ci_lower, baseline_ci_upper], 'k-', linewidth=4, label='95% CI')\n",
    "        ax.plot([2, 2], [faim_ci_lower, faim_ci_upper], 'k-', linewidth=4)\n",
    "\n",
    "        # Add significance annotation\n",
    "        p_val = bootstrap_results[grouping]['p_value']\n",
    "        reduction = baseline_gap - faim_gap\n",
    "        sig_text = f\"p={p_val:.3f}{'*' if p_val < 0.05 else ''}\\n={reduction:.3f}\"\n",
    "        ax.text(1.5, ax.get_ylim()[1]*0.95, sig_text,\n",
    "               ha='center', fontsize=13, weight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen' if p_val < 0.05 else 'lightcoral', alpha=0.8))\n",
    "\n",
    "        ax.set_ylabel(f'{metric} Gap', fontsize=14, weight='bold')\n",
    "        ax.set_title(f'{title}: Comparison with 95% CI', fontsize=16, weight='bold', pad=10)\n",
    "        ax.grid(alpha=0.3, axis='y')\n",
    "        ax.tick_params(labelsize=11)\n",
    "        ax.legend(fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/bootstrap_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_dir}/bootstrap_distributions.png\")\n",
    "\n",
    "    # ==========================================\n",
    "    # FIGURE 2: Permutation Test Distributions\n",
    "    # ==========================================\n",
    "\n",
    "    fig2, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    fig2.suptitle(f'Permutation Test: {metric} Gap Differences',\n",
    "                  fontsize=22, weight='bold', y=1.02)\n",
    "\n",
    "    for i, (grouping, title) in enumerate(zip(groupings, titles)):\n",
    "        ax = axes[i]\n",
    "\n",
    "        perm_dist = permutation_results[grouping]['permuted_dist']\n",
    "        observed_diff = permutation_results[grouping]['observed_diff']\n",
    "        p_val = permutation_results[grouping]['p_value_one_sided']\n",
    "\n",
    "        # Histogram of permuted differences\n",
    "        ax.hist(perm_dist, bins=50, color='gray', alpha=0.7, edgecolor='black', linewidth=0.5, label='Null distribution')\n",
    "\n",
    "        # Observed difference\n",
    "        ax.axvline(observed_diff, color='red', linestyle='--', linewidth=3,\n",
    "                  label=f'Observed: {observed_diff:.3f}')\n",
    "\n",
    "        # Shade significant region\n",
    "        if observed_diff > 0:\n",
    "            x_sig = perm_dist[perm_dist >= observed_diff]\n",
    "            if len(x_sig) > 0:\n",
    "                ax.axvspan(observed_diff, perm_dist.max(), alpha=0.3, color='red', label=f'p={p_val:.3f}')\n",
    "\n",
    "        ax.set_xlabel('Gap Difference (Baseline - FAIM)', fontsize=14, weight='bold')\n",
    "        ax.set_ylabel('Frequency', fontsize=14, weight='bold')\n",
    "        ax.set_title(f'{title}', fontsize=17, weight='bold', pad=10)\n",
    "        ax.legend(fontsize=12, framealpha=0.9)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.tick_params(labelsize=11)\n",
    "\n",
    "        # Add significance annotation\n",
    "        sig_marker = ' Significant' if p_val < 0.05 else ' Not significant'\n",
    "        ax.text(0.05, 0.95, f\"{sig_marker}\\np = {p_val:.3f}\",\n",
    "               transform=ax.transAxes, fontsize=13, weight='bold',\n",
    "               verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen' if p_val < 0.05 else 'lightcoral', alpha=0.9))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/permutation_tests.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_dir}/permutation_tests.png\")\n",
    "\n",
    "    # ==========================================\n",
    "    # FIGURE 3: Combined Summary (2x2 grid)\n",
    "    # ==========================================\n",
    "\n",
    "    fig3, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    fig3.suptitle(f'Significance Testing Summary: {metric} Gap',\n",
    "                  fontsize=22, weight='bold', y=0.995)\n",
    "\n",
    "    # Subplot 1: Gap comparison bar chart\n",
    "    ax = axes[0, 0]\n",
    "\n",
    "    x = np.arange(len(groupings))\n",
    "    width = 0.35\n",
    "\n",
    "    baseline_gaps = [bootstrap_results[g]['baseline']['observed_gap'] for g in groupings]\n",
    "    faim_gaps = [bootstrap_results[g]['faim']['observed_gap'] for g in groupings]\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, baseline_gaps, width, label='Baseline', color='darkorange', alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, faim_gaps, width, label='FAIM', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=12, weight='bold')\n",
    "\n",
    "    ax.set_ylabel(f'{metric} Gap', fontsize=15, weight='bold')\n",
    "    ax.set_title('Gap Comparison by Grouping', fontsize=17, weight='bold', pad=10)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(titles, fontsize=13)\n",
    "    ax.legend(fontsize=13, framealpha=0.9)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    # Subplot 2: Reduction percentages\n",
    "    ax = axes[0, 1]\n",
    "\n",
    "    reductions = [(b - f) / b * 100 for b, f in zip(baseline_gaps, faim_gaps)]\n",
    "    colors = ['green' if r > 0 else 'red' for r in reductions]\n",
    "\n",
    "    bars = ax.barh(titles, reductions, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "    for i, (bar, val) in enumerate(zip(bars, reductions)):\n",
    "        ax.text(val, i, f' {val:.1f}%', va='center', ha='left' if val > 0 else 'right',\n",
    "               fontsize=13, weight='bold')\n",
    "\n",
    "    ax.set_xlabel('Gap Reduction (%)', fontsize=15, weight='bold')\n",
    "    ax.set_title('Percentage Gap Reduction', fontsize=17, weight='bold', pad=10)\n",
    "    ax.axvline(0, color='black', linewidth=2)\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    # Subplot 3: P-values comparison\n",
    "    ax = axes[1, 0]\n",
    "\n",
    "    boot_pvals = [bootstrap_results[g]['p_value'] for g in groupings]\n",
    "    perm_pvals = [permutation_results[g]['p_value_one_sided'] for g in groupings]\n",
    "\n",
    "    x = np.arange(len(groupings))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, boot_pvals, width, label='Bootstrap', color='purple', alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, perm_pvals, width, label='Permutation', color='teal', alpha=0.7, edgecolor='black')\n",
    "\n",
    "    # Add significance line\n",
    "    ax.axhline(0.05, color='red', linestyle='--', linewidth=2.5, label='=0.05')\n",
    "\n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=11, weight='bold')\n",
    "\n",
    "    ax.set_ylabel('P-value', fontsize=15, weight='bold')\n",
    "    ax.set_title('Statistical Significance (p-values)', fontsize=17, weight='bold', pad=10)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(titles, fontsize=13)\n",
    "    ax.set_ylim(0, max(max(boot_pvals), max(perm_pvals)) * 1.3)\n",
    "    ax.legend(fontsize=12, framealpha=0.9)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    # Subplot 4: Summary table\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create summary table\n",
    "    table_data = []\n",
    "    for i, (grouping, title) in enumerate(zip(groupings, titles)):\n",
    "        boot_sig = '' if bootstrap_results[grouping]['significant'] else ''\n",
    "        perm_sig = '' if permutation_results[grouping]['significant'] else ''\n",
    "\n",
    "        row = [\n",
    "            title,\n",
    "            f\"{baseline_gaps[i]:.3f}\",\n",
    "            f\"{faim_gaps[i]:.3f}\",\n",
    "            f\"{baseline_gaps[i] - faim_gaps[i]:.3f}\",\n",
    "            f\"{boot_pvals[i]:.3f} {boot_sig}\",\n",
    "            f\"{perm_pvals[i]:.3f} {perm_sig}\"\n",
    "        ]\n",
    "        table_data.append(row)\n",
    "\n",
    "    table = ax.table(cellText=table_data,\n",
    "                    colLabels=['Grouping', 'Baseline\\nGap', 'FAIM\\nGap', 'Reduction',\n",
    "                              'Bootstrap\\np-value', 'Permutation\\np-value'],\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    bbox=[0, 0, 1, 1])\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1, 3.2)\n",
    "\n",
    "    # Color code header\n",
    "    for i in range(6):\n",
    "        table[(0, i)].set_facecolor('#4472C4')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white', fontsize=13)\n",
    "\n",
    "    # Color code significance\n",
    "    for i in range(1, 4):\n",
    "        # Bootstrap\n",
    "        if '' in table_data[i-1][4]:\n",
    "            table[(i, 4)].set_facecolor('lightgreen')\n",
    "        else:\n",
    "            table[(i, 4)].set_facecolor('lightcoral')\n",
    "\n",
    "        # Permutation\n",
    "        if '' in table_data[i-1][5]:\n",
    "            table[(i, 5)].set_facecolor('lightgreen')\n",
    "        else:\n",
    "            table[(i, 5)].set_facecolor('lightcoral')\n",
    "\n",
    "    ax.set_title('Comprehensive Summary Table', fontsize=17, weight='bold', pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/significance_summary.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_dir}/significance_summary.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VISUALIZATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nGenerated 3 figures:\")\n",
    "    print(f\"  1. {output_dir}/bootstrap_distributions.png\")\n",
    "    print(f\"  2. {output_dir}/permutation_tests.png\")\n",
    "    print(f\"  3. {output_dir}/significance_summary.png\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# RUN PERMUTATION TESTS\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING PERMUTATION TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run permutation tests (using same data as bootstrap)\n",
    "permutation_results, permutation_summary = comprehensive_permutation_test(\n",
    "    y_true,\n",
    "    y_pred_prob_baseline, threshold_baseline,\n",
    "    y_pred_prob_faim, threshold_faim,\n",
    "    dat_test,\n",
    "    metric='tpr',\n",
    "    n_permutations=1000\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# CREATE VISUALIZATIONS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reorganize bootstrap results for visualization\n",
    "# IMPORTANT: Use the SAME keys as permutation_results: 'sex', 'race', 'intersections'\n",
    "\n",
    "bootstrap_vis = {\n",
    "    'sex': {\n",
    "        'baseline': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_baseline, dat_test.reset_index(drop=True)[\"Sex\"],\n",
    "            threshold_baseline, metric='tpr', n_bootstrap=1000, seed=42\n",
    "        ),\n",
    "        'faim': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_faim, dat_test.reset_index(drop=True)[\"Sex\"],\n",
    "            threshold_faim, metric='tpr', n_bootstrap=1000, seed=43\n",
    "        ),\n",
    "        'p_value': results['sex']['p_value'],\n",
    "        'significant': results['sex']['significant']\n",
    "    },\n",
    "    'race': {\n",
    "        'baseline': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_baseline, dat_test.reset_index(drop=True)[\"Race\"],\n",
    "            threshold_baseline, metric='tpr', n_bootstrap=1000, seed=44\n",
    "        ),\n",
    "        'faim': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_faim, dat_test.reset_index(drop=True)[\"Race\"],\n",
    "            threshold_faim, metric='tpr', n_bootstrap=1000, seed=45\n",
    "        ),\n",
    "        'p_value': results['race']['p_value'],\n",
    "        'significant': results['race']['significant']\n",
    "    },\n",
    "    'intersections': {\n",
    "        'baseline': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_baseline,\n",
    "            (dat_test.reset_index(drop=True)[\"Sex\"].astype(str) + \"_\" + dat_test.reset_index(drop=True)[\"Race\"].astype(str)),\n",
    "            threshold_baseline, metric='tpr', n_bootstrap=1000, seed=46\n",
    "        ),\n",
    "        'faim': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_faim,\n",
    "            (dat_test.reset_index(drop=True)[\"Sex\"].astype(str) + \"_\" + dat_test.reset_index(drop=True)[\"Race\"].astype(str)),\n",
    "            threshold_faim, metric='tpr', n_bootstrap=1000, seed=47\n",
    "        ),\n",
    "        'p_value': results['intersections']['p_value'],\n",
    "        'significant': results['intersections']['significant']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Also need to update permutation_results keys to match\n",
    "# The permutation test returned 'Sex', 'Race', but we need 'sex', 'race', 'intersections'\n",
    "permutation_results_fixed = {\n",
    "    'sex': permutation_results['Sex'],\n",
    "    'race': permutation_results['Race'],\n",
    "    'intersections': permutation_results['intersections']\n",
    "}\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "create_comprehensive_visualizations(\n",
    "    bootstrap_vis,\n",
    "    permutation_results_fixed,\n",
    "    metric='TPR',\n",
    "    output_dir='output'\n",
    ")\n",
    "\n",
    "print(\"\\n All significance testing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xrmy3_IRG56K",
    "outputId": "c91fea0c-4c68-4ae1-a357-9792807c8a11"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reorganize bootstrap results for visualization\n",
    "# IMPORTANT: Use the SAME keys as permutation_results: 'sex', 'race', 'intersections'\n",
    "\n",
    "bootstrap_vis = {\n",
    "    'sex': {\n",
    "        'baseline': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_baseline, dat_test.reset_index(drop=True)[\"Sex\"],\n",
    "            threshold_baseline, metric='tpr', n_bootstrap=1000, seed=42\n",
    "        ),\n",
    "        'faim': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_faim, dat_test.reset_index(drop=True)[\"Sex\"],\n",
    "            threshold_faim, metric='tpr', n_bootstrap=1000, seed=43\n",
    "        ),\n",
    "        'p_value': results['sex']['p_value'],\n",
    "        'significant': results['sex']['significant']\n",
    "    },\n",
    "    'race': {\n",
    "        'baseline': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_baseline, dat_test.reset_index(drop=True)[\"Race\"],\n",
    "            threshold_baseline, metric='tpr', n_bootstrap=1000, seed=44\n",
    "        ),\n",
    "        'faim': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_faim, dat_test.reset_index(drop=True)[\"Race\"],\n",
    "            threshold_faim, metric='tpr', n_bootstrap=1000, seed=45\n",
    "        ),\n",
    "        'p_value': results['race']['p_value'],\n",
    "        'significant': results['race']['significant']\n",
    "    },\n",
    "    'intersections': {\n",
    "        'baseline': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_baseline,\n",
    "            (dat_test.reset_index(drop=True)[\"Sex\"].astype(str) + \"_\" + dat_test.reset_index(drop=True)[\"Race\"].astype(str)),\n",
    "            threshold_baseline, metric='tpr', n_bootstrap=1000, seed=46\n",
    "        ),\n",
    "        'faim': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_faim,\n",
    "            (dat_test.reset_index(drop=True)[\"Sex\"].astype(str) + \"_\" + dat_test.reset_index(drop=True)[\"Race\"].astype(str)),\n",
    "            threshold_faim, metric='tpr', n_bootstrap=1000, seed=47\n",
    "        ),\n",
    "        'p_value': results['intersections']['p_value'],\n",
    "        'significant': results['intersections']['significant']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Also need to update permutation_results keys to match\n",
    "# The permutation test returned 'Sex', 'Race', but we need 'sex', 'race', 'intersections'\n",
    "permutation_results_fixed = {\n",
    "    'sex': permutation_results['Sex'],\n",
    "    'race': permutation_results['Race'],\n",
    "    'intersections': permutation_results['intersections']\n",
    "}\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "create_comprehensive_visualizations(\n",
    "    bootstrap_vis,\n",
    "    permutation_results_fixed,\n",
    "    metric='TPR',\n",
    "    output_dir='output'\n",
    ")\n",
    "\n",
    "print(\"\\n All significance testing complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "R9DX5crvbgcL",
    "bABb8zSsCRp2",
    "TeVk96X1ofrn",
    "ewa00AjZCGeJ",
    "dcAfJx0xq3T_",
    "bLj2JM83cJXO",
    "RUGGFX7EsvnB",
    "HQY5jSZK64xP",
    "K1XsIxROVenY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
