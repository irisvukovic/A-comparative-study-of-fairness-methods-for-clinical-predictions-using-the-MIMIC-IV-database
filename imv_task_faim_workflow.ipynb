{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeVk96X1ofrn"
   },
   "source": [
    "#pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xan63BhKuAQe",
    "outputId": "c5543f56-cb43-41f5-fa69-b3ad355d76d4"
   },
   "outputs": [],
   "source": [
    "! pip install git+\"https://github.com/nliulab/ShapleyVIC#egg=ShapleyVIC&subdirectory=python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-jlMicVtrO0",
    "outputId": "cbeec63b-7fd3-4aa8-d492-7040167adddd"
   },
   "outputs": [],
   "source": [
    "! pip install 'aif360[inFairness]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYySnHLUu2Ee",
    "outputId": "08f597bf-a2af-4b30-a728-5344c486543d"
   },
   "outputs": [],
   "source": [
    "! pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pbV03SD0ikx",
    "outputId": "cc4edc72-3249-4637-99f8-fc107c613a77"
   },
   "outputs": [],
   "source": [
    "! pip install patchworklib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewa00AjZCGeJ"
   },
   "source": [
    "#FAIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "gOhvSx6Ppjvv",
    "outputId": "e2447610-a22a-4abd-9b37-8ff77237d7b3"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import collections\n",
    "from pandas.io import gbq\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "import plotnine as pn\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy import integrate\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import copy\n",
    "import statsmodels.api as sm\n",
    "import ShapleyVIC\n",
    "from ShapleyVIC import model, _util\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.postprocessing.eq_odds_postprocessing import EqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import (\n",
    "    CalibratedEqOddsPostprocessing,\n",
    ")\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import (\n",
    "    RejectOptionClassification,\n",
    ")\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fairlearn.metrics import (\n",
    "    equalized_odds_difference,\n",
    "    demographic_parity_difference,\n",
    "    true_negative_rate,\n",
    "    selection_rate,\n",
    "    MetricFrame,\n",
    ")\n",
    "import warnings\n",
    "import patchworklib as pw\n",
    "import inspect\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import shap\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai3rD4aG7N7b"
   },
   "source": [
    "##ShapleyVIC modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8F44CIDIs2Cs",
    "outputId": "24d1d3a9-da18-4b25-e80d-6dee792aae61"
   },
   "outputs": [],
   "source": [
    "print(ShapleyVIC.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pejS3kYxugWE",
    "outputId": "5ea51e40-4899-47c0-97db-962e379aa815"
   },
   "outputs": [],
   "source": [
    "# see folders in ShapleyVIC\n",
    "!ls /usr/local/lib/python3.12/dist-packages/ShapleyVIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dT1D9C1hojk3"
   },
   "outputs": [],
   "source": [
    "!grep -r \"sample_w\" /usr/local/lib/python3.12/dist-packages/ShapleyVIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6knfcPbOukBA",
    "outputId": "85495f21-3dce-459f-fa65-310b12216438"
   },
   "outputs": [],
   "source": [
    "file_path = \"/usr/local/lib/python3.12/dist-packages/ShapleyVIC/model.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# --- First replacement: add sample_w to __init__ ---\n",
    "text = text.replace(\n",
    "    'class models:\\n    def __init__(self, x, y, output_dir, outcome_type=\"binary\", ordinal_link=\"logit\", criterion=\"loss\", epsilon = 0.05, x_names_cat=None, save_data=True):',\n",
    "    'class models:\\n    def __init__(self, x, y, output_dir, outcome_type=\"binary\", ordinal_link=\"logit\",\\n                 criterion=\"loss\", epsilon=0.05, x_names_cat=None, save_data=True,\\n                 sample_w=None):'\n",
    ")\n",
    "\n",
    "# --- Second replacement: add sample_w logic for binary/continuous ---\n",
    "text = text.replace(\n",
    "    '''if outcome_type == \"binary\":\n",
    "            x_with_constant = sm.add_constant(x_dm)\n",
    "            m0 = sm.GLM(y, x_with_constant, family=sm.families.Binomial())\n",
    "            m = m0.fit()\n",
    "        elif outcome_type == \"continuous\":\n",
    "            x_with_constant = sm.add_constant(x_dm)\n",
    "            m0 = sm.OLS(y, x_with_constant)\n",
    "            m = m0.fit()''',\n",
    "    '''if outcome_type == \"binary\":\n",
    "            x_with_constant = sm.add_constant(x_dm)\n",
    "            if sample_w is not None:\n",
    "                m0 = sm.GLM(y, x_with_constant, family=sm.families.Binomial(), freq_weights=sample_w)\n",
    "            else:\n",
    "                m0 = sm.GLM(y, x_with_constant, family=sm.families.Binomial())\n",
    "            m = m0.fit()\n",
    "        elif outcome_type == \"continuous\":\n",
    "            x_with_constant = sm.add_constant(x_dm)\n",
    "            if sample_w is not None:\n",
    "                m0 = sm.WLS(y, x_with_constant, weights=sample_w)\n",
    "            else:\n",
    "                m0 = sm.OLS(y, x_with_constant)\n",
    "            m = m0.fit()'''\n",
    ")\n",
    "\n",
    "# Save back to file\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(\"File updated successfully with sample_w support!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkiB4fJWureu",
    "outputId": "67ac8ad9-f25d-4b31-bc73-266d984ad37d"
   },
   "outputs": [],
   "source": [
    "!grep -r \"sample_w\" /usr/local/lib/python3.12/dist-packages/ShapleyVIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5prZZ2x9pWAP"
   },
   "source": [
    "##functions and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RRNOvD_pYYR"
   },
   "outputs": [],
   "source": [
    "def rgb01_hex(col):\n",
    "    col_hex = [round(i * 255) for i in col]\n",
    "    col_hex = \"#%02x%02x%02x\" % tuple(col_hex)\n",
    "    return col_hex\n",
    "\n",
    "\n",
    "def compute_area(fairness_metrics):\n",
    "    n_metric = len(fairness_metrics)\n",
    "    tmp = fairness_metrics.values.flatten().tolist()\n",
    "    tmp_1 = tmp[1:] + tmp[:1]\n",
    "\n",
    "    if n_metric > 2:\n",
    "        theta_c = 2 * np.pi / n_metric\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
    "    elif n_metric == 2:\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
    "    else:\n",
    "        area = np.abs(tmp[0])\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def plot_perf_metric(\n",
    "    perf_metric, eligible, x_range, select=None, plot_selected=False, x_breaks=None\n",
    "):\n",
    "    \"\"\" Plot performance metrics of sampled models\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        perf_metric : numpy.array or pandas.Series\n",
    "            Numeric vector of performance metrics for all sampled models\n",
    "        eligible : numpy.array or pandas.Series\n",
    "            Boolean vector of the same length of 'perf_metric', indicating \\\n",
    "                whether each sample is eligible.\n",
    "        x_range : list\n",
    "            Numeric vector indicating the range of eligible values for \\\n",
    "                performance metrics.\n",
    "            Will be indicated by dotted vertical lines in plots.\n",
    "        select : list or numpy.array, optional (default: None)\n",
    "            Numeric vector of indexes of 'perf_metric' to be selected\n",
    "        plot_selected : bool, optional (default: False)\n",
    "            Whether performance metrics of selected models should be plotted in \\\n",
    "                a secondary figure.\n",
    "        x_breaks : list, optional (default: None)\n",
    "            If selected models are to be plotted, the breaks to use in the \\\n",
    "                histogram\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        plot : plotnine.ggplot\n",
    "            Histogram(s) of model performance made using ggplot\n",
    "    \"\"\"\n",
    "    m = len(perf_metric)\n",
    "    perf_df = pd.DataFrame(perf_metric, columns=[\"perf_metric\"], index=None)\n",
    "    plot = (\n",
    "        pn.ggplot(perf_df, pn.aes(x=\"perf_metric\"))\n",
    "        + pn.geoms.geom_histogram(\n",
    "            breaks=np.linspace(np.min(perf_metric), np.max(perf_metric), 40)\n",
    "        )\n",
    "        + pn.geoms.geom_vline(xintercept=x_range, linetype=\"dashed\", size=0.7)\n",
    "        + pn.labels.labs(\n",
    "            x=\"Ratio of loss to minimum loss\",\n",
    "            title=\"\"\"Loss of {m:d} sampled models\n",
    "                \\n{n_elg:d} ({per_elg:.1f}%) sampled models are eligible\"\"\".format(\n",
    "                m=m, n_elg=np.sum(eligible), per_elg=np.sum(eligible) * 100 / m\n",
    "            ),\n",
    "        )\n",
    "        + pn.themes.theme_bw()\n",
    "        + pn.themes.theme(\n",
    "            title=pn.themes.element_text(ha=\"left\"),\n",
    "            axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
    "            axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
    "        )\n",
    "    )\n",
    "    if plot_selected:\n",
    "        if select is None:\n",
    "            print(\"'select' vector is not specified!\\nUsing all models instead\")\n",
    "            select = [i for i in range(len(perf_df))]\n",
    "        try:\n",
    "            perf_select = perf_df.iloc[select]\n",
    "        except:\n",
    "            print(\n",
    "                \"Invalid indexes detected in 'select' vector!\\nUsing all models instead\"\n",
    "            )\n",
    "            select = [i for i in range(len(perf_df))]\n",
    "            perf_select = perf_df.iloc[select]\n",
    "        plot2 = (\n",
    "            pn.ggplot(perf_select, pn.aes(x=\"perf_metric\"))\n",
    "            + pn.geoms.geom_histogram(breaks=x_breaks)\n",
    "            + pn.labels.labs(\n",
    "                x=\"Ratio of loss to minimum loss\",\n",
    "                title=\"{n_select:d} selected models\".format(n_select=len(select)),\n",
    "            )\n",
    "            + pn.themes.theme_bw()\n",
    "            + pn.themes.theme(\n",
    "                title=pn.themes.element_text(ha=\"left\"),\n",
    "                axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
    "                axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
    "            )\n",
    "        )\n",
    "        return (plot, plot2)\n",
    "    else:\n",
    "        return plot\n",
    "\n",
    "\n",
    "def plot_distribution(df, s=4):\n",
    "    num_metrics = df.shape[1] - 2\n",
    "    labels = df.sen_var_exclusion.unique()\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == \"\":\n",
    "            labels[i] = \"No exclusion\"\n",
    "        elif len(labels[i].split(\"_\")) == 2:\n",
    "            labels[i] = f\"Exclusion of {' and '.join(labels[i].split('_'))}\"\n",
    "        elif len(labels[i].split(\"_\")) > 2:\n",
    "            sens = labels[i].split(\"_\")\n",
    "            labels[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
    "        else:\n",
    "            labels[i] = f\"Exclusion of {labels[i]}\"\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(s * num_metrics, s))\n",
    "    for i, x in enumerate(df.columns[:-2]):\n",
    "        ax = axes[i]\n",
    "        # sns.jointplot(data=df, x=x, y=\"auc\", hue=\"sen_var_exclusion\",  ax=ax, legend=False)\n",
    "        sns.histplot(\n",
    "            data=df, x=x, hue=\"sen_var_exclusion\", bins=50, ax=ax, legend=False\n",
    "        )  # layout=(1, num_metrics), figsize=(4, 4), color=\"#595959\",\n",
    "        ax.set_title(x)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Count\" if i == 0 else \"\")\n",
    "\n",
    "    plt.legend(\n",
    "        loc=\"center left\",\n",
    "        title=\"\",\n",
    "        labels=labels[::-1],\n",
    "        ncol=1,\n",
    "        bbox_to_anchor=(1.04, 0.5),\n",
    "        borderaxespad=0,\n",
    "    )\n",
    "    # plt.tight_layout() bbox_transform=fig.transFigure,\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_scatter(df, perf, sen_var_exclusion, title, c1=20, c2=0.15, **kwargs):\n",
    "    ### basic settings ###\n",
    "    np.random.seed(0)\n",
    "    if \"figsize\" not in kwargs.keys():\n",
    "        fig_h = 400\n",
    "        figsize = [fig_h * df.shape[1] * 2.45 / 3, fig_h]\n",
    "    else:\n",
    "        figsize = kwargs[\"figsize\"]\n",
    "    caption_size = figsize[1] / c1  # control font size / figure size\n",
    "    fig_caption_ratio = 0.8\n",
    "    fig_font_size = caption_size * fig_caption_ratio\n",
    "\n",
    "    font_family = \"Arial\"\n",
    "    highlight_color = \"#D4AF37\"\n",
    "    fig_font_unit = c2  # control the relative position of elements\n",
    "    caption_font_unit = fig_font_unit * fig_caption_ratio\n",
    "    d = fig_font_unit / 8\n",
    "    legend_pos_y = 1 + fig_font_unit\n",
    "    subtitle_pos = [legend_pos_y + d, legend_pos_y + d + caption_font_unit]\n",
    "    xlab_pos_y = -fig_font_unit * 2\n",
    "\n",
    "    area_list = []\n",
    "    for i, id in enumerate(df.index):\n",
    "        values = df.loc[id, :]\n",
    "        area_list.append(1 / compute_area(values))\n",
    "    ranking = np.argsort(np.argsort(area_list)[::-1])\n",
    "\n",
    "    # jittering for display\n",
    "    jitter_control = np.zeros(len(ranking))\n",
    "    for idx in range(len(ranking)):\n",
    "        if ranking[idx] == 0:\n",
    "            jitter_control[idx] = 0\n",
    "        elif ranking[idx] <= 10 and ranking[idx] != 0:\n",
    "            jitter_control[idx] = 0.01 * np.random.uniform(0, 1)\n",
    "        elif ranking[idx] <= 10**2 and ranking[idx] > 10:\n",
    "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
    "        elif ranking[idx] <= 10**3 and ranking[idx] > 10**2:\n",
    "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
    "        else:\n",
    "            jitter_control[idx] = 0.02 * np.random.uniform(-1, 1)\n",
    "\n",
    "    ### plot ###\n",
    "    best_id = df.index[np.where(ranking == 0)][0]\n",
    "    worst_id = df.index[np.argmin(area_list)]\n",
    "    meduim_id = df.index[np.argsort(area_list)[int(len(area_list) / 2)]]\n",
    "\n",
    "    num_metrics = df.shape[1]\n",
    "    num_models = df.shape[0]\n",
    "\n",
    "    fig = make_subplots(cols=num_metrics, rows=1, horizontal_spacing=0.13)\n",
    "    cmap = sns.light_palette(\"steelblue\", as_cmap=False, n_colors=df.shape[0])\n",
    "    cmap = cmap[::-1]\n",
    "    colors = [rgb01_hex(cmap[x]) if x != 0 else highlight_color for x in ranking]\n",
    "    sizes = [10 if x != 0 else 20 for x in ranking]\n",
    "\n",
    "    shapes = sen_var_exclusion.copy().tolist()\n",
    "    cases = sen_var_exclusion.unique()\n",
    "    shapes_candidates = [\"square\", \"circle\", \"triangle-up\", \"star\"][: len(cases)]\n",
    "    for i, case in enumerate(cases):\n",
    "        for j, v in enumerate(sen_var_exclusion):\n",
    "            if v == case:\n",
    "                shapes[j] = shapes_candidates[i]\n",
    "\n",
    "        if cases[i] == \"\":\n",
    "            cases[i] = \"No exclusion\"\n",
    "        elif len(cases[i].split(\"_\")) == 2:\n",
    "            cases[i] = f\"Exclusion of {' and '.join(cases[i].split('_'))}\"\n",
    "        elif len(cases[i].split(\"_\")) > 2:\n",
    "            sens = cases[i].split(\"_\")\n",
    "            cases[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
    "        else:\n",
    "            cases[i] = f\"Exclusion of {cases[i]}\"\n",
    "\n",
    "    fair_index_df = pd.DataFrame(\n",
    "        {\n",
    "            \"model id\": df.index,\n",
    "            \"fair_index\": area_list,\n",
    "            \"ranking\": ranking,\n",
    "            \"eod\": df[\"Equalized Odds\"],\n",
    "            \"colors\": colors,\n",
    "            \"shapes\": shapes,\n",
    "            \"sizes\": sizes,\n",
    "            \"cases\": sen_var_exclusion,\n",
    "            \"jitter\": jitter_control,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add scatter plots to the subplots\n",
    "    for k, s in enumerate(shapes_candidates):\n",
    "        for i in range(num_metrics):\n",
    "            # index of sen_var_exclusion(shape) == s\n",
    "            s_idx = [idx for idx, x in enumerate(shapes) if x == s]\n",
    "            x = df.iloc[s_idx, i].values\n",
    "            js = fair_index_df.loc[fair_index_df.shapes == s, \"jitter\"].values\n",
    "            jittered_x = x + js\n",
    "\n",
    "            col = fair_index_df.loc[fair_index_df.shapes == s, \"colors\"]\n",
    "            size = fair_index_df.loc[fair_index_df.shapes == s, \"sizes\"]\n",
    "            fair_index = fair_index_df.loc[fair_index_df.shapes == s, \"fair_index\"]\n",
    "            ids = fair_index_df.loc[fair_index_df.shapes == s, \"model id\"]\n",
    "            rank_text = fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
    "            r = (\n",
    "                fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
    "                .apply(lambda x: math.log10(x + 1))\n",
    "                .values\n",
    "            )\n",
    "            sen_case = fair_index_df.loc[fair_index_df.shapes == s, \"cases\"]\n",
    "\n",
    "            hovertext = [\n",
    "                f\"Fairness index: {f:.3f}. Ranking: {x}. Model id: {i}\"\n",
    "                for f, x, i in zip(fair_index, rank_text, ids)\n",
    "            ]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=r,\n",
    "                    y=jittered_x,\n",
    "                    customdata=hovertext,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        color=col,\n",
    "                        symbol=s,\n",
    "                        size=size,\n",
    "                        line=dict(color=col, width=1),\n",
    "                        opacity=0.8,\n",
    "                    ),\n",
    "                    hovertemplate=\"%{customdata}.\",\n",
    "                    hoverlabel=None,\n",
    "                    hoverinfo=\"name+z\",\n",
    "                    name=cases[k],\n",
    "                ),\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "            )\n",
    "\n",
    "            if i == int((df.shape[1] + 0.5) / 2):\n",
    "                fig.update_xaxes(\n",
    "                    title_text=None,\n",
    "                    tickvals=[0, 1, 2, 3],\n",
    "                    ticktext=[1, 10, 100, 1000],\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                    tickangle=0,\n",
    "                )\n",
    "            else:\n",
    "                fig.update_xaxes(\n",
    "                    title_text=None,\n",
    "                    tickvals=[0, 1, 2, 3],\n",
    "                    ticktext=[1, 10, 100, 1000],\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                    tickangle=0,\n",
    "                )\n",
    "            fig.update_yaxes(\n",
    "                title_text=df.columns[i],\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "                showticksuffix=\"none\",\n",
    "                titlefont={\"size\": caption_size},\n",
    "            )\n",
    "\n",
    "            fig.add_vline(\n",
    "                x=0,\n",
    "                line_width=2,\n",
    "                line_dash=\"dot\",\n",
    "                line_color=highlight_color,\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "            )\n",
    "\n",
    "            min_metric = df.loc[ranking == 0, df.columns[i]].values[0]\n",
    "            max_metric = df.loc[ranking == num_models - 1, df.columns[i]].values[0]\n",
    "            meduim_metric = df.loc[\n",
    "                ranking == int(num_models / 2), df.columns[i]\n",
    "            ].values[0]\n",
    "\n",
    "            # add annotations\n",
    "            anno_size = caption_size * 0.7\n",
    "            if k == 0:\n",
    "                fig.add_hline(\n",
    "                    y=min_metric,\n",
    "                    line_width=2,\n",
    "                    line_dash=\"dot\",\n",
    "                    line_color=highlight_color,\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                )\n",
    "\n",
    "                # position_y = np.mean(df.iloc[:, i])\n",
    "                min_annotation = {\n",
    "                    \"x\": 0,\n",
    "                    \"y\": min_metric,\n",
    "                    \"text\": f\"Model ID {best_id}<br> Rank No.1\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                }\n",
    "                fig.add_annotation(min_annotation, col=i + 1, row=1)\n",
    "            if meduim_id in ids:\n",
    "                medium_annotation = {\n",
    "                    \"x\": math.log10(int(num_models / 2) + 1),\n",
    "                    \"y\": meduim_metric + jitter_control[meduim_id],\n",
    "                    \"text\": f\"Model ID {meduim_id}<br> Rank No.{int(num_models/2)}\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                }\n",
    "                fig.add_annotation(medium_annotation, col=i + 1, row=1)\n",
    "            if worst_id in ids:\n",
    "                max_annotation = {\n",
    "                    \"x\": math.log10(num_models + 1),\n",
    "                    \"y\": max_metric + jitter_control[worst_id],\n",
    "                    \"text\": f\"Model ID {worst_id}<br> Rank No.{num_models}\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                    \"align\": \"left\",\n",
    "                }\n",
    "                fig.add_annotation(max_annotation, col=i + 1, row=1)\n",
    "\n",
    "    colorbar_trace = go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode=\"markers\",\n",
    "        hoverinfo=\"none\",\n",
    "        marker=dict(\n",
    "            colorscale=[\n",
    "                rgb01_hex(np.array((243, 244, 245)) / 255),\n",
    "                \"steelblue\",\n",
    "            ],  # \"magma\",\n",
    "            showscale=True,\n",
    "            cmin=0,\n",
    "            cmax=2,\n",
    "            colorbar=dict(\n",
    "                title=None,\n",
    "                thickness=10,\n",
    "                tickvals=[0, 2],\n",
    "                ticktext=[\"Low\", \"High\"],\n",
    "                outlinewidth=0,\n",
    "                orientation=\"v\",\n",
    "                x=1,\n",
    "                y=0.5,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    fig.add_trace(colorbar_trace)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        font=dict(family=\"Arial\", size=fig_font_size),\n",
    "        hovermode=\"closest\",\n",
    "        width=figsize[0],\n",
    "        height=figsize[1],\n",
    "        showlegend=True,\n",
    "        template=\"simple_white\",\n",
    "        legend=dict(x=0, y=legend_pos_y, orientation=\"h\"),\n",
    "    )\n",
    "\n",
    "    rectangle = {\n",
    "        \"type\": \"rect\",\n",
    "        \"x0\": -0.1,\n",
    "        \"y0\": subtitle_pos[0],\n",
    "        \"x1\": 1.1,\n",
    "        \"y1\": subtitle_pos[1],\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"fillcolor\": \"steelblue\",\n",
    "        \"opacity\": 0.1,\n",
    "    }  # 'line': {'color': 'red', 'width': 2},\n",
    "    fig.add_shape(rectangle)\n",
    "    subtitle_annotation = {\n",
    "        \"x\": -0.1,\n",
    "        \"y\": subtitle_pos[1],\n",
    "        \"text\": f\"<i> The FAIM model (i.e., fairness-aware model) is with model ID {best_id}, out of {num_models} nearly-optimal models.</i>\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": caption_size * 1.1},\n",
    "        \"align\": \"left\",\n",
    "    }\n",
    "    xaxis_annotation = {\n",
    "        \"x\": 0.5,\n",
    "        \"y\": xlab_pos_y,\n",
    "        \"text\": \"Model Rank\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": caption_size},\n",
    "    }\n",
    "    colorbar_title = {\n",
    "        \"x\": 1.05,\n",
    "        \"y\": 0.5,\n",
    "        \"text\": \"Fairness Ranking Index (FRI)\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": anno_size * 0.9},\n",
    "        \"textangle\": 90,\n",
    "    }\n",
    "    fig.add_annotation(subtitle_annotation)\n",
    "    fig.add_annotation(xaxis_annotation)\n",
    "    fig.add_annotation(colorbar_title)\n",
    "\n",
    "    for i, trace in enumerate(fig.data):\n",
    "        if i % num_metrics == 1:\n",
    "            trace.update(showlegend=True)\n",
    "        else:\n",
    "            trace.update(showlegend=False)\n",
    "    # fig.show()\n",
    "\n",
    "    return fig, fair_index_df\n",
    "\n",
    "\n",
    "def plot_radar(df, thresh_show, title, **kwargs):\n",
    "    fig = go.Figure()\n",
    "    # fig = sp.make_subplots(rows=1, cols=2)\n",
    "    cmap = sns.diverging_palette(200, 20, sep=10, s=50, as_cmap=False, n=df.shape[0])\n",
    "    theta = df.columns.tolist()\n",
    "    theta += theta[:1]\n",
    "    area_list = []\n",
    "\n",
    "    for i, id in enumerate(df.index):\n",
    "        values = df.loc[id, :]\n",
    "        area_list.append(compute_area(values))\n",
    "        values = values.values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        info = [\n",
    "            f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
    "        ]\n",
    "        fig.add_trace(\n",
    "            go.Scatterpolar(\n",
    "                r=values,\n",
    "                theta=theta,\n",
    "                fill=\"toself\" if id == \"FAIReg\" else \"none\",\n",
    "                text=\"\\n\".join(info),\n",
    "                name=f\"{id}\",\n",
    "                line=dict(color=rgb01_hex(cmap[i]), dash=\"dot\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ranking = np.argsort(np.argsort(area_list))\n",
    "    best_id = df.index[np.where(ranking == 0)][0]\n",
    "    print(\n",
    "        f\"The best model is No.{best_id} with metrics on validation set:\\n {df.loc[best_id, :]}\"\n",
    "    )\n",
    "    values = df.loc[best_id, :].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    info = [\n",
    "        f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
    "    ]\n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=theta,\n",
    "            fill=\"toself\",\n",
    "            text=\"\\n\".join(info),\n",
    "            name=f\"model {best_id}\",\n",
    "            line=dict(color=\"royalblue\", dash=\"solid\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        # title = title,\n",
    "        font=dict(family=\"Arial\", size=16),\n",
    "        polar=dict(\n",
    "            # bgcolor = \"#1e2130\",\n",
    "            radialaxis=dict(\n",
    "                showgrid=True,\n",
    "                gridwidth=1,\n",
    "                gridcolor=\"lightgray\",\n",
    "                visible=True,\n",
    "                range=[0, thresh_show],\n",
    "            )\n",
    "        ),\n",
    "        legend=dict(x=0.25, y=-0.1, orientation=\"h\"),\n",
    "        showlegend=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_bar(\n",
    "    shap_values, feature_names, original_feature_names, coef=None, title=None, **kwargs\n",
    "):\n",
    "    \"\"\"Plot the bar chart of feature importance\"\"\"\n",
    "    if \"color\" not in kwargs.keys():\n",
    "        color = \"steelblue\"\n",
    "    else:\n",
    "        color = kwargs[\"color\"]\n",
    "\n",
    "    def get_prefix(v):\n",
    "        if \"_\" in v and (v not in original_feature_names):\n",
    "            tmp = [\"_\".join(v.split(\"_\")[:i]) for i in range(len(v.split(\"_\")))]\n",
    "            return [s for s in tmp if s in original_feature_names][0]\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    if shap_values is not None:\n",
    "\n",
    "        grouped_df = pd.DataFrame({\"values\": shap_values}, index=feature_names).groupby(\n",
    "            by=get_prefix, axis=0\n",
    "        )\n",
    "        df = {k: np.mean(np.abs(g.values)) for k, g in grouped_df}\n",
    "        df = pd.DataFrame.from_dict(df, orient=\"index\").reset_index()\n",
    "        df.columns = [\"Var\", \"Value\"]\n",
    "\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in df.Value],\n",
    "                        \"order\": np.abs(df.Value),\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    elif coef is not None:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"Var\": coef.index,\n",
    "                \"Value\": coef.values,\n",
    "                \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in coef.values],\n",
    "                \"order\": np.abs(coef.values),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Either shap_value or coef should be provided\")\n",
    "\n",
    "    df = df.loc[df[\"Var\"] != \"const\", :]\n",
    "    df = df.sort_values(by=\"order\", ascending=True)\n",
    "    df[\"Var\"] = pd.Categorical(df[\"Var\"], categories=df[\"Var\"].tolist(), ordered=True)\n",
    "\n",
    "    common_theme = theme(\n",
    "        text=element_text(size=24),\n",
    "        panel_grid_major_y=element_line(colour=\"lightgrey\"),\n",
    "        panel_grid_minor=element_blank(),\n",
    "        panel_background=element_blank(),\n",
    "        axis_line_x=element_line(colour=\"black\"),\n",
    "        axis_ticks_major_y=element_blank(),\n",
    "    )\n",
    "\n",
    "    x_lab = \"Feature importance\"\n",
    "\n",
    "    p = (\n",
    "        ggplot(data=df, mapping=aes(x=\"Var\", y=\"Value\", fill=\"color\"))\n",
    "        + geom_hline(yintercept=0, color=\"grey\")\n",
    "        + geom_bar(stat=\"identity\")\n",
    "        + common_theme\n",
    "        + coord_flip()\n",
    "        + labs(x=\"\", y=x_lab, title=title)\n",
    "        + theme(legend_position=\"none\")\n",
    "        + scale_fill_manual(values=[color])\n",
    "    )\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "\n",
    "# metrics\n",
    "def get_ci_auc(y_true, y_pred, alpha=0.05, type=\"auc\"):\n",
    "    \"\"\"Calculate the confidence interval for the AUC (Area Under the Curve) score\n",
    "    or PR (Precision-Recall) score using bootstrapping.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): True labels.\n",
    "        y_pred (array-like): Predicted scores or probabilities.\n",
    "        alpha (float, optional): Significance level for the confidence interval. Default is 0.05.\n",
    "        type (str, optional): Type of score to calculate: 'auc' (default) or 'pr' (precision-recall).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "\n",
    "    n_bootstraps = 1000\n",
    "    bootstrapped_scores = []\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(y_pred) - 1, len(y_pred))\n",
    "\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "\n",
    "        if type == \"pr\":\n",
    "            precision, recall, thresholds = precision_recall_curve(\n",
    "                y_true[indices], y_pred[indices]\n",
    "            )\n",
    "            score = auc(recall, precision)\n",
    "        else:\n",
    "            score = roc_auc_score(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    # 95% c.i.\n",
    "    confidence_lower = sorted_scores[int(alpha / 2 * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(1 - alpha / 2 * len(sorted_scores))]\n",
    "\n",
    "    return confidence_lower, np.median(sorted_scores), confidence_upper\n",
    "\n",
    "\n",
    "def find_optimal_cutoff(target, predicted, method=\"auc\"):\n",
    "    \"\"\"Find the optimal probability cutoff point for a classification model related to event rate.\n",
    "\n",
    "    Args:\n",
    "        target (array-like): True labels.\n",
    "        predicted (array-like): Predicted scores or probabilities.\n",
    "        method (str, optional): Method for finding the optimal cutoff. Default is 'auc'.\n",
    "\n",
    "    Returns:\n",
    "        list: List of optimal cutoff values.\n",
    "    \"\"\"\n",
    "    if method == \"auc\":\n",
    "        fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "        i = np.arange(len(tpr))\n",
    "        roc = pd.DataFrame(\n",
    "            {\n",
    "                \"tf\": pd.Series(tpr + (1 - fpr), index=i),\n",
    "                \"threshold\": pd.Series(threshold, index=i),\n",
    "            }\n",
    "        )\n",
    "        roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[::-1][:1]]\n",
    "    elif method == \"pr-auc\":\n",
    "        precision, recall, threshold = precision_recall_curve(target, predicted)\n",
    "        i = np.arange(len(precision))\n",
    "        prc = pd.DataFrame(\n",
    "            {\n",
    "                \"tf\": pd.Series(tpr - (1 - fpr), index=i),\n",
    "                \"threshold\": pd.Series(threshold, index=i),\n",
    "            }\n",
    "        )\n",
    "        prc_t = prc.iloc[(prc.tf - 0).abs().argsort()[:1]]\n",
    "\n",
    "    return list(roc_t[\"threshold\"])\n",
    "\n",
    "\n",
    "def get_cal_fairness(df):\n",
    "    def absolute_difference(x):\n",
    "        return np.abs(spline1(x) - spline0(x))\n",
    "\n",
    "    df.groupby(\"group\").apply(lambda x: np.max(x[\"p_obs\"]))\n",
    "    # for g in df_calib.group.unique():\n",
    "\n",
    "    gs = df.group.unique()\n",
    "    pairs = list(combinations(gs, 2))\n",
    "\n",
    "    x_min_thresh = np.min(df.groupby(\"group\").apply(lambda x: np.min(x[\"p_pred\"])))\n",
    "    x_max_thresh = np.max(df.groupby(\"group\").apply(lambda x: np.max(x[\"p_pred\"])))\n",
    "    num_points = 100\n",
    "    diff_cal = []\n",
    "\n",
    "    for p in pairs:\n",
    "        p0 = df.loc[df.group == p[0], [\"p_obs\", \"p_pred\"]].sort_values(\n",
    "            by=\"p_pred\", ascending=True\n",
    "        )\n",
    "        p1 = df.loc[df.group == p[1], [\"p_obs\", \"p_pred\"]].sort_values(\n",
    "            by=\"p_pred\", ascending=True\n",
    "        )\n",
    "\n",
    "        x0 = p0[\"p_pred\"]\n",
    "        y0 = p0[\"p_obs\"]\n",
    "        spline0 = CubicSpline(x0, y0)\n",
    "\n",
    "        x1 = p1[\"p_pred\"]\n",
    "        y1 = p1[\"p_obs\"]\n",
    "        spline1 = CubicSpline(x1, y1)\n",
    "\n",
    "        x_sample = np.linspace(x_min_thresh, x_max_thresh, num_points)\n",
    "        y_sample = absolute_difference(x_sample)\n",
    "        area = integrate.simpson(y_sample, x_sample)\n",
    "        diff_cal.append(area)\n",
    "\n",
    "    cal_metric = np.mean(diff_cal)\n",
    "    # print(f\"Calibration metric: {cal_metric:.2f}\")\n",
    "    return cal_metric\n",
    "\n",
    "\n",
    "## small functions\n",
    "def col_gap(col_train, col_test, x_with_constant):\n",
    "    if len(col_train) != len(col_test):\n",
    "        col_gap = [i not in col_test for i in col_train]\n",
    "        x_with_constant[col_train[col_gap]] = 0\n",
    "        x_with_constant = x_with_constant.loc[:, col_train]\n",
    "\n",
    "    return x_with_constant\n",
    "\n",
    "\n",
    "def generate_subsets(input_list):\n",
    "    subsets = []\n",
    "    n = len(input_list)\n",
    "    for subset_size in range(n + 1):\n",
    "        for subset in itertools.combinations(input_list, subset_size):\n",
    "            subsets.append(list(subset))\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHDw9mDnovxk"
   },
   "source": [
    "##fairness_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sE_0l1VCRnK7"
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "\n",
    "class FairBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dat_train,\n",
    "        selected_vars,\n",
    "        selected_vars_cat,\n",
    "        y_name,\n",
    "        sen_name,\n",
    "        sen_var_ref,\n",
    "        without_sen=False,\n",
    "        weighted=True,\n",
    "        weights={\"tnr\": 0.5, \"tpr\": 0.5},\n",
    "        class_weight=\"balanced\",\n",
    "    ):\n",
    "        \"\"\"Initialize the fairness base class\n",
    "\n",
    "        Args:\n",
    "            dat_train (data frame): training data\n",
    "            selected_vars (list): selected variables including sensitive variables\n",
    "            selected_vars_cat (list): selected categorical variables\n",
    "            y_name (str): the name of the label\n",
    "            sen_name (list): the name of the sensitive variable\n",
    "            sen_var_ref (dict): the reference level of the sensitive variables\n",
    "            without_sen (bool, optional): directly exclude the sensitive variables. Defaults to False.\n",
    "            weighted (bool, optional): compute the weighted version of metrics \"tnr\" and \"tpr\". Defaults to True.\n",
    "            weights (dict, optional): the weightage for \"tnr\" and \"tpr\", summing up to 1. Defaults to {\"tnr\": 0.5, \"tpr\": 0.5}.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dat_train = dat_train\n",
    "        self.vars = selected_vars\n",
    "        self.vars_cat = selected_vars_cat\n",
    "        self.y_name = y_name\n",
    "\n",
    "        if not isinstance(sen_name, list):\n",
    "            self.sen_name = [self.sen_name]\n",
    "        for s in sen_name:\n",
    "            if sen_var_ref[s] not in dat_train[s].unique():\n",
    "                raise ValueError(\n",
    "                    f\"Please provide the right reference level of sensitive variables {s}!\"\n",
    "                )\n",
    "            if s not in self.vars:\n",
    "                self.vars.append(s)\n",
    "        else:\n",
    "            self.sen_name = sen_name\n",
    "            self.sen_var_ref = sen_var_ref\n",
    "\n",
    "        self.without_sen = without_sen\n",
    "        self.weighted = weighted\n",
    "        self.weights = weights\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "    def compute_class_weights(self, y):\n",
    "        \"\"\"Compute class weights for balanced training\n",
    "\n",
    "        Args:\n",
    "            y: target labels\n",
    "\n",
    "        Returns:\n",
    "            array of weights for each sample\n",
    "        \"\"\"\n",
    "        if self.class_weight == \"balanced\":\n",
    "            classes, counts = np.unique(y, return_counts=True)\n",
    "            n_samples = len(y)\n",
    "            n_classes = len(classes)\n",
    "            class_weights = n_samples / (n_classes * counts)\n",
    "            weights = np.array([class_weights[c] for c in y])\n",
    "            return weights\n",
    "        else:\n",
    "            return np.ones(len(y))\n",
    "\n",
    "    def data_process(\n",
    "        self, dat, selected_vars=None, selected_vars_cat=None, without_sen=None\n",
    "    ):\n",
    "        \"\"\"Data preprocess\n",
    "\n",
    "        Args:\n",
    "            dat (data frame): data\n",
    "            selected_vars (list, optional): selected variables (can include sensitive variables). This needs to be provided if the case considered is beyond completely inclusion or exclusion of sensitive variables. Defaults to None.\n",
    "            selected_vars_cat (list, optional): selected categorical variables, subset of selected variables. Defaults to None.\n",
    "            without_sen (bool, optional): directly exclude the sensitive variables. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            x_1: predictors with one-coding and with constant\n",
    "            sen_var: the vector of sensitive variable. combined by \"_\", if there are several sensitive variables\n",
    "            y: the vector of the label\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def combine_sen(dat, sen):\n",
    "            new_sen = [\"_\".join(v) for v in zip(*[dat[s].astype(\"str\") for s in sen])]\n",
    "            return new_sen\n",
    "\n",
    "        if selected_vars is None:\n",
    "            selected_vars = self.vars\n",
    "            selected_vars_cat = self.vars_cat\n",
    "\n",
    "        x = dat.drop(\n",
    "            columns=[\n",
    "                c for c in dat.columns if c == self.y_name or c not in selected_vars\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            self.without_sen != \"auto\"\n",
    "            and (without_sen is None and self.without_sen)\n",
    "            or without_sen\n",
    "        ):\n",
    "            x = x.drop(columns=self.sen_name)\n",
    "\n",
    "        if len(self.sen_name) > 1:\n",
    "            sen_var = combine_sen(dat, self.sen_name)\n",
    "        else:\n",
    "            sen_var = dat[self.sen_name[0]]\n",
    "\n",
    "        y = dat[self.y_name]\n",
    "\n",
    "        x_dm, x_groups = _util.model_matrix(x=x, x_names_cat=selected_vars_cat)\n",
    "        x_1 = sm.add_constant(x_dm).astype(\"float\")\n",
    "        return x_1, sen_var, y\n",
    "\n",
    "    def data_prepare(self, dat_expl=None):\n",
    "        \"\"\"Shape the data to AIF360 format\n",
    "\n",
    "        Args:\n",
    "            dat_expl (_type_, optional): validation data needed for post-processing methods. Defaults to None.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.method == \"Unawareness\":\n",
    "            x_with_constant, _, y_train = self.data_process(\n",
    "                self.dat_train if dat_expl is None else dat_expl, without_sen=True\n",
    "            )\n",
    "            return x_with_constant\n",
    "\n",
    "        if self.method_type == \"pre\":\n",
    "            x_with_constant, _, y_train = self.data_process(self.dat_train)\n",
    "            x_with_constant_sen_bin = copy.deepcopy(x_with_constant)\n",
    "            for s in self.sen_name:\n",
    "                x_with_constant_sen_bin[s] = [\n",
    "                    0 if i == self.sen_var_ref[s] else 1 for i in self.dat_train[s]\n",
    "                ]\n",
    "            # print(x_with_constant_expl_sen_bin.columns.head(), flush=True)\n",
    "\n",
    "            pre_train_df = pd.concat([x_with_constant_sen_bin, y_train], axis=1)\n",
    "            pre_train = BinaryLabelDataset(\n",
    "                favorable_label=1,\n",
    "                df=pre_train_df,\n",
    "                label_names=[self.y_name],\n",
    "                protected_attribute_names=self.sen_name,\n",
    "            )\n",
    "            return pre_train\n",
    "\n",
    "        elif self.method_type == \"post\":\n",
    "            if dat_expl is None:\n",
    "                raise ValueError(\"Please provide validation data.\")\n",
    "            else:\n",
    "                x_with_constant_expl, sen_var, y_expl = self.data_process(dat_expl)\n",
    "                x_with_constant_expl_sen_bin = copy.deepcopy(x_with_constant_expl)\n",
    "\n",
    "                for s in self.sen_name:\n",
    "                    x_with_constant_expl_sen_bin[s] = [\n",
    "                        0 if i == self.sen_var_ref[s] else 1 for i in dat_expl[s]\n",
    "                    ]\n",
    "\n",
    "                prob_expl_ori = self.lr_results.predict(x_with_constant_expl)\n",
    "                ori_thresh = find_optimal_cutoff(y_expl, prob_expl_ori)[0]\n",
    "                pred_expl_ori = prob_expl_ori > ori_thresh\n",
    "\n",
    "                post_expl_df = pd.concat([x_with_constant_expl_sen_bin, y_expl], axis=1)\n",
    "                post_expl = BinaryLabelDataset(\n",
    "                    favorable_label=1,\n",
    "                    df=post_expl_df,\n",
    "                    label_names=[self.y_name],\n",
    "                    protected_attribute_names=self.sen_name,\n",
    "                )\n",
    "                post_expl_pred = post_expl.copy(deepcopy=True)\n",
    "                post_expl_pred.scores = prob_expl_ori.values.reshape(-1, 1)\n",
    "                post_expl_pred.labels = pred_expl_ori.values.reshape(-1, 1)\n",
    "                return post_expl, post_expl_pred\n",
    "\n",
    "    def model(self, method_type=None, method=None, dat_expl=None, **kwargs):\n",
    "        \"\"\"Fit the model\n",
    "\n",
    "        Args:\n",
    "            method_type (str, optional): the type of bias mitigation method (pre/in/post). Defaults to None.\n",
    "            method (str, optional): the name of bias mitigation method. Defaults to None.\n",
    "            dat_expl (_type_, optional): validation data needed for post-processing methods. Defaults to None.\n",
    "\n",
    "            Methods:\n",
    "            +------------------+--------------------------------+\n",
    "            | Method type      | Specific methods               |\n",
    "            +==================+================================+\n",
    "            | None             | \"OriginalLR\", \"Unawareness\"   |\n",
    "            +------------------+--------------------------------+\n",
    "            | \"pre\"            | \"Reweigh\"                      |\n",
    "            +------------------+--------------------------------+\n",
    "            | \"in\"             | \"Reductions\"                   |\n",
    "            +------------------+--------------------------------+\n",
    "            | \"post\"           | \"EqOdds\", \"CalEqOdds\", \"ROC\"  |\n",
    "            +------------------+--------------------------------+\n",
    "\n",
    "        Returns:\n",
    "            model results that can be used for prediction\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.method_type = method_type\n",
    "\n",
    "        if isinstance(self.sen_name, list):\n",
    "            privileged_groups = [{s: 0 for s in self.sen_name}]\n",
    "            unprivileged_groups = [{s: 1 for s in self.sen_name}]\n",
    "        else:\n",
    "            privileged_groups = [{self.sen_name: 0}]\n",
    "            unprivileged_groups = [{self.sen_name: 1}]\n",
    "\n",
    "        x_with_constant_nosen, _, y_train = self.data_process(\n",
    "            self.dat_train, without_sen=True\n",
    "        )\n",
    "        x_with_constant, _, y_train = self.data_process(self.dat_train)\n",
    "\n",
    "        # original LR\n",
    "        if self.class_weight == \"balanced\":\n",
    "            sample_weights = self.compute_class_weights(self.dat_train[self.y_name])\n",
    "            lr_model = sm.GLM(\n",
    "                self.dat_train[self.y_name], x_with_constant, family=sm.families.Binomial(),\n",
    "                freq_weights=sample_weights\n",
    "            )\n",
    "        else:\n",
    "            lr_model = sm.GLM(\n",
    "                self.dat_train[self.y_name], x_with_constant, family=sm.families.Binomial()\n",
    "            )\n",
    "        self.lr_results = lr_model.fit()\n",
    "\n",
    "        if method_type == None:\n",
    "            if method == \"OriginalLR\":\n",
    "                return self.lr_results\n",
    "\n",
    "            elif method == \"Unawareness\":\n",
    "                un_model = sm.GLM(\n",
    "                    self.dat_train[self.y_name],\n",
    "                    x_with_constant_nosen,\n",
    "                    family=sm.families.Binomial(),\n",
    "                )\n",
    "                un_results = un_model.fit()\n",
    "                return un_results\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Please confirm the method: 'OriginalLR' if no bias mitigation is needed; 'Unawareness' if simply excluding the sensitive variabl is enough.\"\n",
    "                )\n",
    "\n",
    "        elif method_type == \"pre\":\n",
    "            pre_train = self.data_prepare()\n",
    "\n",
    "            if method == \"Reweigh\":\n",
    "                reweigh_model = Reweighing(\n",
    "                    privileged_groups=privileged_groups,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                )\n",
    "                rw_train = reweigh_model.fit_transform(pre_train)\n",
    "                rw_model = sm.GLM(\n",
    "                    self.dat_train[self.y_name],\n",
    "                    x_with_constant,\n",
    "                    family=sm.families.Binomial(),\n",
    "                    freq_weights=rw_train.instance_weights,\n",
    "                )\n",
    "                plt.hist(rw_train.instance_weights)\n",
    "                rw_results = rw_model.fit()\n",
    "                return rw_model, rw_results, rw_train.instance_weights\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Please specify the type of pre-process bias mitigation method among ['Reweigh']!\"\n",
    "                )\n",
    "\n",
    "        elif method_type == \"in\":\n",
    "            if method == \"Reductions\":\n",
    "\n",
    "                constraint = EqualizedOdds(difference_bound=0.01)\n",
    "                np.random.seed(\n",
    "                    0\n",
    "                )  # set seed for consistent results with ExponentiatedGradient\n",
    "                lr_model_sk = LogisticRegression(max_iter=5000, penalty=None)\n",
    "                mitigator = ExponentiatedGradient(lr_model_sk, constraint)\n",
    "\n",
    "                mitigator.fit(\n",
    "                    x_with_constant,\n",
    "                    y_train,\n",
    "                    sensitive_features=self.dat_train[self.sen_name],\n",
    "                )\n",
    "                return mitigator\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Please specify the type of in-process bias mitigation method among ['Reductions']!\"\n",
    "                )\n",
    "\n",
    "        elif method_type == \"post\":\n",
    "            post_expl, post_expl_pred = self.data_prepare(dat_expl=dat_expl)\n",
    "\n",
    "            if method == \"EqOdds\":\n",
    "                eq_model = EqOddsPostprocessing(\n",
    "                    privileged_groups=privileged_groups,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                    seed=seed,\n",
    "                )\n",
    "                eq_results = eq_model.fit(post_expl, post_expl_pred)\n",
    "                return eq_results\n",
    "\n",
    "            elif method == \"CalEqOdds\":\n",
    "                if \"cost_constraint\" in kwargs:\n",
    "                    cost_constraint = kwargs[\"cost_constraint\"]\n",
    "                else:\n",
    "                    cost_constraint = \"weighted\"  # \"fnr\", \"fpr\", \"weighted\"\n",
    "                cal_eq_model = CalibratedEqOddsPostprocessing(\n",
    "                    privileged_groups=privileged_groups,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                    cost_constraint=cost_constraint,\n",
    "                    seed=seed,\n",
    "                )\n",
    "                cal_eq_results = cal_eq_model.fit(post_expl, post_expl_pred)\n",
    "                return cal_eq_results\n",
    "\n",
    "            elif method == \"ROC\":\n",
    "                ub = 0.05 if \"ub\" not in kwargs else kwargs[\"ub\"]\n",
    "                lb = -0.05 if \"lb\" not in kwargs else kwargs[\"lb\"]\n",
    "                metric_name = (\n",
    "                    \"Equal opportunity difference\"\n",
    "                    if \"metric_name\" not in kwargs\n",
    "                    else kwargs[\"metric_name\"]\n",
    "                )\n",
    "                # allowed_metrics = [\"Statistical parity difference\", \"Average odds difference\", \"Equal opportunity difference\"]\n",
    "                ROC_model = RejectOptionClassification(\n",
    "                    privileged_groups=privileged_groups,\n",
    "                    unprivileged_groups=unprivileged_groups,\n",
    "                    low_class_thresh=0.01,\n",
    "                    high_class_thresh=0.99,\n",
    "                    num_class_thresh=100,\n",
    "                    num_ROC_margin=50,\n",
    "                    metric_name=metric_name,\n",
    "                    metric_ub=ub,\n",
    "                    metric_lb=lb,\n",
    "                )\n",
    "                ROC_results = ROC_model.fit(post_expl, post_expl_pred)\n",
    "                return ROC_results\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Please specify the type of post-process bias mitigation method among ['EqOdds', 'CalEqOdds', 'ROC']!\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Please specify the type of bias mitigation method (pre/in/post)!\"\n",
    "            )\n",
    "\n",
    "    def test(self, dat_test, model=None, params=None, thresh=None, **kwargs):\n",
    "        \"\"\"Test the fairness of the model\n",
    "\n",
    "        Args:\n",
    "            dat_test (data frame): test data\n",
    "            model (_type_, optional): the fitted model. Defaults to None.\n",
    "            params (_type_, optional): coefficients for the model. Defaults to None.\n",
    "            thresh (_type_, optional): threshold for the predictions. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pred_test / prob_test (array): predicted labels / predicted probabilities\n",
    "            fairmetrics (data frame): fairness metrics\n",
    "            fairsummary (data frame): fairness summary for each subgroup\n",
    "        \"\"\"\n",
    "        if \"without_sen\" in kwargs.keys():\n",
    "            without_sen = kwargs[\"without_sen\"]\n",
    "        else:\n",
    "            without_sen = self.without_sen\n",
    "        x_with_constant_test, sen_var, y_test = self.data_process(dat_test)\n",
    "        prob_test = None\n",
    "        thresh = None\n",
    "\n",
    "        if model is not None:\n",
    "            if self.method_type == \"post\":\n",
    "                _, post_test_pred = self.data_prepare(dat_expl=dat_test)\n",
    "                pred_test = model.predict(post_test_pred).labels.reshape(-1)\n",
    "            else:\n",
    "                if self.method_type == None and self.method == \"Unawareness\":\n",
    "                    x_with_constant_test = self.data_prepare(dat_expl=dat_test)\n",
    "\n",
    "                if self.method == \"Reductions\":\n",
    "                    pred_test = model.predict(x_with_constant_test)\n",
    "                else:\n",
    "                    prob_test = model.predict(x_with_constant_test)\n",
    "                    thresh = find_optimal_cutoff(y_test, prob_test)[0]\n",
    "                    pred_test = prob_test > thresh\n",
    "                    # print(prob_test)\n",
    "        else:\n",
    "            raise ValueError(\"Please provide the right model!\")\n",
    "\n",
    "        fe = FAIMEvaluator(\n",
    "            y_true=y_test,\n",
    "            y_pred=prob_test,\n",
    "            y_pred_bin=pred_test,\n",
    "            sen_var=sen_var,\n",
    "            weighted=self.weighted,\n",
    "            weights=self.weights,\n",
    "        )\n",
    "        fairmetrics = fe.fairmetrics\n",
    "        fairsummary = fe.fairsummary\n",
    "        clametrics = fe.clametrics\n",
    "\n",
    "        return pred_test if prob_test is None else prob_test, fairmetrics, clametrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmuQmuFto1c1"
   },
   "source": [
    "##fairness_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt0L8vOiyVsC"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "my_fairness_bases = {\n",
    "    \"tpr\": recall_score,\n",
    "    \"tnr\": true_negative_rate,\n",
    "    \"sr\": selection_rate,\n",
    "    \"acc\": accuracy_score,\n",
    "    \"conf_mat\": confusion_matrix,\n",
    "}\n",
    "# the situation for each group should not be bad; tnr -> fpr\n",
    "my_bases_bound = {\"tpr\": 0.6, \"tnr\": 0.6, \"sr\": 0, \"acc\": 0.6, \"conf_mat\": pd.NA}\n",
    "\n",
    "\n",
    "def fairarea(fairness_metrics):\n",
    "    n_metric = len(fairness_metrics)\n",
    "    tmp = fairness_metrics.values.flatten().tolist()\n",
    "    tmp_1 = tmp[1:] + tmp[:1]\n",
    "\n",
    "    if n_metric > 2:\n",
    "        theta_c = 2 * np.pi / n_metric\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
    "    elif n_metric == 2:\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
    "    else:\n",
    "        area = np.abs(tmp[0])\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "class FAIMEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        y_pred_bin,\n",
    "        sen_var,\n",
    "        fair_only=False,\n",
    "        cla_metrics=[\"auc\"],\n",
    "        weighted=False,\n",
    "        weights=None,\n",
    "        bases=my_fairness_bases,\n",
    "        bound=my_bases_bound,\n",
    "    ):\n",
    "        \"\"\"Initialize the fairness evaluator.\n",
    "\n",
    "        Args:\n",
    "            y_true: true labels\n",
    "            y_pred: predicted scores or probabilities\n",
    "            y_pred_bin: predicted binary labels\n",
    "            sen_var: the vector of sensitive variables\n",
    "            fair_only (bool, optional): whether to compute fairness metrics only. Defaults to False.\n",
    "            cla_metrics (list, optional): classification metrics. Defaults to [\"auc\"].\n",
    "            weighted (bool, optional): whether to create a customized fairness metric based on weighted combining of 'tnr' and 'tpr'. Defaults to False.\n",
    "            weights (_type_, optional): the weights for weighted combining of 'tnr' and 'tpr'. Required when `weighted` is True. Defaults to None.\n",
    "            bases (_type_, optional): the bases for fairness metrics. Defaults to my_fairness_bases (see above).\n",
    "            bound (_type_, optional): the bound for base metrics. Defaults to my_bases_bound.\n",
    "        \"\"\"\n",
    "        # super().__init__(y_obs=y_true, y_pred=y_pred, y_pred_bin=y_pred_bin, sens_var=pd.Series(sen_var), y_pos=True)\n",
    "\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.y_pred_bin = pd.Series(y_pred_bin)\n",
    "        self.sen_var = pd.Series(sen_var)\n",
    "        self.cla_metrics = cla_metrics\n",
    "\n",
    "        self.my_fairness_bases = bases\n",
    "        self.my_bases_bound = bound\n",
    "\n",
    "        if weighted:\n",
    "            if weights is None or not isinstance(weights, dict):\n",
    "                raise TypeError(\n",
    "                    \"The weights need to be specified and the type should be dict!\"\n",
    "                )\n",
    "            elif len(weights) != 2 or np.sum(list(weights.values())) != 1:\n",
    "                raise ValueError(\n",
    "                    \"The weights should be a dict containing two values respectively for 'tpr' and 'tnr'. In addition, the sum of weights should be equal to 1!\"\n",
    "                )\n",
    "            else:\n",
    "                self.weighted = weighted\n",
    "                self.weights = weights\n",
    "\n",
    "        self._fairsummary_generation()\n",
    "        self._fairmetrics_generation()\n",
    "        if not fair_only:\n",
    "            if cla_metrics is None:\n",
    "                raise ValueError(\"The classification metrics should be specified!\")\n",
    "            self._clametric_generation()\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_sen(y_obs, sen_var, sens_var_ref):\n",
    "        # print(\"Checking the sensitive variable...\")\n",
    "        return {\"sens_var\": sen_var, \"sens_var_ref\": pd.unique(sen_var)[0]}\n",
    "\n",
    "    def _fairsummary_generation(self):\n",
    "        \"\"\"Computation primary metrics (e.g., TPR, TNR, etc.) among subgroups\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        self.fairsummary = MetricFrame(\n",
    "            y_true=self.y_true,\n",
    "            y_pred=self.y_pred_bin,\n",
    "            metrics=self.my_fairness_bases,\n",
    "            sensitive_features=self.sen_var,\n",
    "        )\n",
    "\n",
    "        # Create performance metrics table with additional metrics\n",
    "        by_group = self.fairsummary.by_group\n",
    "\n",
    "        tpr = by_group[\"tpr\"]\n",
    "        tnr = by_group[\"tnr\"]\n",
    "        sr = by_group[\"sr\"]\n",
    "        acc = by_group[\"acc\"]\n",
    "\n",
    "        # Calculate FPR, FNR, and BER\n",
    "        fpr = 1 - tnr\n",
    "        fnr = 1 - tpr\n",
    "        ber = (fpr + fnr) / 2\n",
    "\n",
    "        # Create comprehensive performance table\n",
    "        self.performance_table = pd.DataFrame({\n",
    "            \"TPR\": tpr,\n",
    "            \"FPR\": fpr,\n",
    "            \"TNR\": tnr,\n",
    "            \"FNR\": fnr,\n",
    "            \"SR\": sr,\n",
    "            \"Accuracy\": acc,\n",
    "            \"BER\": ber\n",
    "        })\n",
    "\n",
    "    def _fairmetrics_generation(self):\n",
    "        \"\"\"Generate fairness metrics and disparity tables.\"\"\"\n",
    "\n",
    "        # ----- machine learning performance-based fairness metrics -----\n",
    "        bases = self.my_fairness_bases.keys()\n",
    "        fairmetrics = {}\n",
    "        qc = {}\n",
    "        diff_ = self.fairsummary.difference()\n",
    "        for b in list(bases)[:-1]:\n",
    "            qc[b] = self.fairsummary.overall[b] > self.my_bases_bound[b]\n",
    "\n",
    "        fairmetrics[\"Equal Opportunity\"] = diff_[\"tpr\"]\n",
    "        fairmetrics[\"Equalized Odds\"] = np.max([diff_[\"tpr\"], diff_[\"tnr\"]])\n",
    "        fairmetrics[\"Statistical Parity\"] = diff_[\"sr\"]\n",
    "        fairmetrics[\"Accuracy Equality\"] = diff_[\"acc\"]\n",
    "\n",
    "        # MODIFIED: Calculate true BER Equality as Range(BER)\n",
    "        by_group = self.fairsummary.by_group\n",
    "        fpr_values = 1 - by_group[\"tnr\"]\n",
    "        fnr_values = 1 - by_group[\"tpr\"]\n",
    "        ber_values = 0.5 * (fpr_values + fnr_values)\n",
    "\n",
    "        if self.weighted:\n",
    "            # True BER Equality = Range of BER across groups\n",
    "            fairmetrics[\"BER Equality\"] = ber_values.max() - ber_values.min()\n",
    "        else:\n",
    "            fairmetrics[\"BER Equality\"] = ber_values.max() - ber_values.min()\n",
    "\n",
    "        self.fairmetrics = pd.DataFrame([fairmetrics])\n",
    "\n",
    "        # Create fairness disparity summary table\n",
    "        diff_ = self.fairsummary.difference()\n",
    "\n",
    "        # Get TPR, FPR disparities\n",
    "        tpr_diff = diff_[\"tpr\"]\n",
    "        fpr_diff = abs(diff_[\"tnr\"])  # FPR diff = TNR diff\n",
    "\n",
    "        # Equalized Odds = max of TPR and FPR differences\n",
    "        equalized_odds = max(tpr_diff, fpr_diff)\n",
    "\n",
    "        # Equal Opportunity = TPR difference\n",
    "        equal_opportunity = tpr_diff\n",
    "\n",
    "        # BER Equality (now correctly calculated)\n",
    "        ber_equality = fairmetrics[\"BER Equality\"]\n",
    "\n",
    "        # Helper function to determine if we're looking at intersectional groups\n",
    "        def is_intersectional(group_name):\n",
    "            \"\"\"Check if group contains multiple attributes (e.g., 'Male_White')\"\"\"\n",
    "            return '_' in str(group_name)\n",
    "\n",
    "        # MODIFIED: Helper function to check if reference group exists\n",
    "        def get_reference_group(groups):\n",
    "            \"\"\"Find reference group based on sensitive variable type\"\"\"\n",
    "            # For intersectional (Sex_Race)\n",
    "            for group in groups:\n",
    "                if '@Male_@White' in str(group):\n",
    "                    return group\n",
    "\n",
    "            # For race only\n",
    "            for group in groups:\n",
    "                if '@White' in str(group) and '@Male' not in str(group):\n",
    "                    return group\n",
    "\n",
    "            # For sex only\n",
    "            for group in groups:\n",
    "                if '@Male' in str(group) and '@White' not in str(group):\n",
    "                    return group\n",
    "\n",
    "            return None\n",
    "\n",
    "        # Determine min type for Equalized Odds (which metric drives the disparity)\n",
    "        if tpr_diff >= fpr_diff:\n",
    "            eq_odds_min_type = \"TPR\"\n",
    "            eq_odds_values = by_group[\"tpr\"]\n",
    "        else:\n",
    "            eq_odds_min_type = \"FPR\"\n",
    "            eq_odds_values = 1 - by_group[\"tnr\"]\n",
    "\n",
    "        # Build disparity table with 3 rows\n",
    "        disparity_data = []\n",
    "\n",
    "        # Determine the appropriate column name based on group structure\n",
    "        groups = by_group.index.tolist()\n",
    "        is_intersect = is_intersectional(groups[0]) if len(groups) > 0 else False\n",
    "        group_col_name = \"Intersection\" if is_intersect else \"Group\"\n",
    "\n",
    "        # Check if reference group exists\n",
    "        reference_group = get_reference_group(groups)\n",
    "\n",
    "        # Row 1: Equalized Odds (max of TPR/FPR)\n",
    "        row1 = {\n",
    "            \"Metric\": \"Equalized Odds (max of TPR/FPR)\",\n",
    "            \"Min Value\": eq_odds_values.min(),\n",
    "            f\"Min {group_col_name}\": eq_odds_values.idxmin(),\n",
    "            \"Min Type\": eq_odds_min_type,\n",
    "            \"Max Value\": eq_odds_values.max(),\n",
    "            f\"Max {group_col_name}\": eq_odds_values.idxmax(),\n",
    "            \"Gap\": equalized_odds\n",
    "        }\n",
    "\n",
    "        # Add reference group if it exists and is not already min/max\n",
    "        if reference_group and reference_group not in [eq_odds_values.idxmin(), eq_odds_values.idxmax()]:\n",
    "            row1[f\"Reference {group_col_name}\"] = reference_group\n",
    "            row1[\"Reference Value\"] = eq_odds_values[reference_group]\n",
    "            row1[\"Reference Gap\"] = abs(eq_odds_values[reference_group] - eq_odds_values.min())\n",
    "        else:\n",
    "            row1[f\"Reference {group_col_name}\"] = None\n",
    "            row1[\"Reference Value\"] = None\n",
    "            row1[\"Reference Gap\"] = None\n",
    "\n",
    "        disparity_data.append(row1)\n",
    "\n",
    "        # Row 2: Equal Opportunity (TPR)\n",
    "        tpr_values = by_group[\"tpr\"]\n",
    "        row2 = {\n",
    "            \"Metric\": \"Equal Opportunity (TPR)\",\n",
    "            \"Min Value\": tpr_values.min(),\n",
    "            f\"Min {group_col_name}\": tpr_values.idxmin(),\n",
    "            \"Min Type\": \"TPR\",\n",
    "            \"Max Value\": tpr_values.max(),\n",
    "            f\"Max {group_col_name}\": tpr_values.idxmax(),\n",
    "            \"Gap\": equal_opportunity\n",
    "        }\n",
    "\n",
    "        # Add reference group if it exists and is not already min/max\n",
    "        if reference_group and reference_group not in [tpr_values.idxmin(), tpr_values.idxmax()]:\n",
    "            row2[f\"Reference {group_col_name}\"] = reference_group\n",
    "            row2[\"Reference Value\"] = tpr_values[reference_group]\n",
    "            row2[\"Reference Gap\"] = abs(tpr_values[reference_group] - tpr_values.min())\n",
    "        else:\n",
    "            row2[f\"Reference {group_col_name}\"] = None\n",
    "            row2[\"Reference Value\"] = None\n",
    "            row2[\"Reference Gap\"] = None\n",
    "\n",
    "        disparity_data.append(row2)\n",
    "\n",
    "        # Row 3: BER Equality (CORRECTED)\n",
    "        row3 = {\n",
    "            \"Metric\": \"BER Equality\",\n",
    "            \"Min Value\": ber_values.min(),\n",
    "            f\"Min {group_col_name}\": ber_values.idxmin(),\n",
    "            \"Min Type\": \"BER\",\n",
    "            \"Max Value\": ber_values.max(),\n",
    "            f\"Max {group_col_name}\": ber_values.idxmax(),\n",
    "            \"Gap\": ber_equality\n",
    "        }\n",
    "\n",
    "        # Add reference group if it exists and is not already min/max\n",
    "        if reference_group and reference_group not in [ber_values.idxmin(), ber_values.idxmax()]:\n",
    "            row3[f\"Reference {group_col_name}\"] = reference_group\n",
    "            row3[\"Reference Value\"] = ber_values[reference_group]\n",
    "            row3[\"Reference Gap\"] = abs(ber_values[reference_group] - ber_values.min())\n",
    "        else:\n",
    "            row3[f\"Reference {group_col_name}\"] = None\n",
    "            row3[\"Reference Value\"] = None\n",
    "            row3[\"Reference Gap\"] = None\n",
    "\n",
    "        disparity_data.append(row3)\n",
    "\n",
    "        self.disparity_table = pd.DataFrame(disparity_data)\n",
    "\n",
    "        # Round numeric columns to 4 decimals\n",
    "        numeric_cols = [\"Min Value\", \"Max Value\", \"Gap\"]\n",
    "        if \"Reference Value\" in self.disparity_table.columns:\n",
    "            numeric_cols.append(\"Reference Value\")\n",
    "        if \"Reference Gap\" in self.disparity_table.columns:\n",
    "            numeric_cols.append(\"Reference Gap\")\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if col in self.disparity_table.columns:\n",
    "                self.disparity_table[col] = self.disparity_table[col].round(4)\n",
    "\n",
    "        self.qc = pd.DataFrame([qc])\n",
    "\n",
    "    def _clametric_generation(self):\n",
    "        clametrics = {}\n",
    "        pred = self.y_pred if self.y_pred is not None else self.y_pred_bin\n",
    "\n",
    "        # Add sensitivity (TPR) and specificity (TNR)\n",
    "        clametrics[\"sensitivity\"] = recall_score(self.y_true, self.y_pred_bin)\n",
    "        clametrics[\"specificity\"] = true_negative_rate(self.y_true, self.y_pred_bin)\n",
    "\n",
    "        if \"auc\" in self.cla_metrics:\n",
    "            # clametrics[\"auc\"] = roc_auc_score(self.y_true, self.y_pred)\n",
    "            clametrics[\"auc_low\"], clametrics[\"auc\"], clametrics[\"auc_high\"] = (\n",
    "                get_ci_auc(self.y_true, pred, alpha=0.05, type=\"auc\")\n",
    "            )\n",
    "\n",
    "        self.clametrics = pd.DataFrame([clametrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJcE3tUlpA6m"
   },
   "source": [
    "##fairness_modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djWDrJNBRvnn"
   },
   "outputs": [],
   "source": [
    "class FAIMGenerator(FairBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dat_train,\n",
    "        selected_vars,\n",
    "        selected_vars_cat,\n",
    "        y_name,\n",
    "        sen_name,\n",
    "        sen_var_ref,\n",
    "        output_dir,\n",
    "        criterion=\"loss\",\n",
    "        epsilon=0.05,\n",
    "        m=800,\n",
    "        n_final=350,\n",
    "        without_sen=False,\n",
    "        pre=False,\n",
    "        pre_method=\"Reweigh\",\n",
    "        post=False,\n",
    "        post_method=\"equalizedodds\",\n",
    "        class_weight=\"balanced\",\n",
    "    ):\n",
    "        \"\"\"Initialize the class of FAIM\n",
    "\n",
    "        Args:\n",
    "            dat_train (data frame): the training data\n",
    "            selected_vars (list): the selected variables that include sensitive variables\n",
    "            selected_vars_cat (list): the selected categorical variables that include sensitive variables\n",
    "            y_name (str): the name of the label, e.g. \"y\", \"label\", etc.\n",
    "            sen_name (list): the name of the sensitive variables\n",
    "            sen_var_ref (dict): the reference values of the sensitive variables e.g. {\"gender\": \"F\"}\n",
    "            output_dir: the output directory to store the nearly optimal models results\n",
    "            criterion (str, optional): the criterion to generate nearly optimal models. Defaults to \"loss\".\n",
    "            epsilon (float, optional): the control factor of \"nearly optimality\", i.e. the gap to the optimal model. Defaults to 0.05.\n",
    "            without_sen (bool, optional): directly exclude the sensitive variables. Defaults to False.\n",
    "            pre (bool, optional): whether to use pre-process bias mitigation methods before FAIM. Defaults to False.\n",
    "            pre_method (str, optional): specific pre-process method. Defaults to \"Reweigh\".\n",
    "            post (bool, optional): whether to use post-process bias mitigation methods after FAIM. Defaults to False.\n",
    "            post_method (str, optional): specific post-process method. Defaults to \"EqOdds\".\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(\n",
    "            dat_train,\n",
    "            selected_vars,\n",
    "            selected_vars_cat,\n",
    "            y_name,\n",
    "            sen_name,\n",
    "            sen_var_ref,\n",
    "            without_sen,\n",
    "            class_weight=class_weight,\n",
    "        )\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.output_dir = output_dir\n",
    "        self.epsilon = epsilon\n",
    "        self.m = m\n",
    "        self.n_final = n_final\n",
    "\n",
    "        self.pre = pre\n",
    "        if pre:\n",
    "            self.pre_method = pre_method\n",
    "            self.rw_model, self.rw_results, self.rw_weights = self.pre_mitigate()\n",
    "            plt.hist(self.rw_weights)\n",
    "        if post:\n",
    "            self.post = post\n",
    "            self.post_method = post_method\n",
    "\n",
    "        self.optim_obj = self.optimal_model(selected_vars, selected_vars_cat)\n",
    "        self.optim_results = self.optim_obj.model_optim\n",
    "        self.optim_model = self.optim_obj.model_optim.model\n",
    "\n",
    "        self.dat_expl = None\n",
    "        self.dat_test = None\n",
    "\n",
    "    # def __reduce__(self):\n",
    "    #     return (self.__class__, (self.coefs, self.best_coef, self.best_optim_base_obj, self.best_sen_exclusion, self.fairmetrics_df))\n",
    "\n",
    "    def pre_mitigate(self):\n",
    "        \"\"\"Pre-process bias mitigation methods\"\"\"\n",
    "        rw_model, rw_results, instance_weights = self.model(\n",
    "            method_type=\"pre\", method=self.pre_method\n",
    "        )\n",
    "\n",
    "        return rw_model, rw_results, instance_weights\n",
    "\n",
    "    def optimal_model(self, selected_vars, selected_vars_cat):\n",
    "        \"\"\"Generate the optimal model\"\"\"\n",
    "        x = self.dat_train.drop(\n",
    "            columns=[\n",
    "                c\n",
    "                for c in self.dat_train.columns\n",
    "                if c == self.y_name or c not in selected_vars\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # ===== HANDLE THREE CASES =====\n",
    "\n",
    "        # Case 1: Using pre-processing fairness intervention (Reweighing)\n",
    "        if self.pre and self.pre_method == \"Reweigh\":\n",
    "            sample_weights = self.rw_weights  # Use Reweighing weights\n",
    "            print(\" Using Reweighing pre-processing weights (fairness intervention)\")\n",
    "\n",
    "        # Case 2: Using balanced class weights (for imbalance, not fairness)\n",
    "        elif self.class_weight == \"balanced\":\n",
    "            y_train = self.dat_train[self.y_name]\n",
    "            n_samples = len(y_train)\n",
    "            classes, counts = np.unique(y_train, return_counts=True)\n",
    "            class_weight_dict = {\n",
    "                cls: n_samples / (len(classes) * cnt)\n",
    "                for cls, cnt in zip(classes, counts)\n",
    "            }\n",
    "            sample_weights = np.array([class_weight_dict[cls] for cls in y_train])\n",
    "            print(f\" Using balanced class weights (for imbalance): {class_weight_dict}\")\n",
    "\n",
    "        # Case 3: No weighting\n",
    "        else:\n",
    "            sample_weights = None\n",
    "            print(\" No sample weighting\")\n",
    "\n",
    "        model_object = model.models(\n",
    "            x=x,\n",
    "            y=self.dat_train[self.y_name],\n",
    "            x_names_cat=selected_vars_cat,\n",
    "            output_dir=self.output_dir,\n",
    "            criterion=self.criterion,\n",
    "            sample_w=sample_weights\n",
    "        )\n",
    "\n",
    "        return model_object\n",
    "\n",
    "    def nearly_optimal_model(self, optim_base_obj, m=200, n_final=50, epsilon=None):\n",
    "        \"\"\"Generate the nearly optimal models\n",
    "\n",
    "        Args:\n",
    "            optim_base_obj (object): the object of the optimal model\n",
    "            m (int, optional): the number of models to be generated. Defaults to 800.\n",
    "            n_final (int, optional): the number of nearly optimal models to be generated. Defaults to 350.\n",
    "\n",
    "        Returns:\n",
    "            coefs (data frame): the coefficients of the nearly optimal models\n",
    "            plots (plot): the plot of the status nearly optimal models\n",
    "        \"\"\"\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        u1, u2 = optim_base_obj.init_hyper_params(m=m)\n",
    "        optim_base_obj.draw_models(\n",
    "            u1=u1,\n",
    "            u2=u2,\n",
    "            m=self.m,\n",
    "            n_final=self.n_final,\n",
    "            random_state=1234\n",
    "        )\n",
    "        coefs = pd.read_csv(\n",
    "            os.path.join(self.output_dir, \"models_near_optim.csv\"), index_col=0\n",
    "        )\n",
    "        return coefs, optim_base_obj.models_plot\n",
    "\n",
    "    def fairness_compute(\n",
    "        self,\n",
    "        dat_expl,\n",
    "        optim_base_obj,\n",
    "        coefs,\n",
    "        weighted=True,\n",
    "        weights={\"tnr\": 0.5, \"tpr\": 0.5},\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Compute the fairness metrics of the nearly optimal models\n",
    "\n",
    "        Args:\n",
    "            dat_expl (data frame): the data frame of the validation data\n",
    "            optim_base_obj (object): the object of the optimal model\n",
    "            coefs (data frame): the coefficients of the nearly optimal models\n",
    "            weighted (bool, optional): whether to use weighted fairness metrics. Defaults to True.\n",
    "            weights (dict, optional): the weights of the weighted fairness metrics. Defaults to {\"tnr\": 0.5, \"tpr\": 0.5}.\n",
    "            **kwargs: the other parameters of the fairness computation\n",
    "\n",
    "        Returns:\n",
    "            fairmetrics_df (data frame): the fairness metrics of the nearly optimal models\n",
    "            qc_df (data frame): the quality control results of the nearly optimal models\n",
    "        \"\"\"\n",
    "        if weighted:\n",
    "            self.weighted = weighted\n",
    "            self.weights = weights\n",
    "        self.dat_expl = dat_expl\n",
    "\n",
    "        optim_base_results = optim_base_obj.model_optim\n",
    "        optim_base_model = optim_base_obj.model_optim.model\n",
    "\n",
    "        fairmetrics_df = []\n",
    "        qc_df = []\n",
    "        by_group_list = []\n",
    "\n",
    "        for i in range(coefs.shape[0]):\n",
    "            coef = coefs.drop(columns=[\"perf_metric\"]).iloc[i, :]\n",
    "            x_with_constant, sen_var, y_expl = self.data_process(dat_expl, **kwargs)\n",
    "\n",
    "            # sen_var = dat_expl[self.sen_name]\n",
    "            optim_base_results.params = coef\n",
    "\n",
    "            col_train = optim_base_results.params.index\n",
    "            col_test = x_with_constant.columns\n",
    "            x_with_constant = col_gap(col_train, col_test, x_with_constant)\n",
    "\n",
    "            prob_expl = optim_base_model.predict(params=coef, exog=x_with_constant)\n",
    "            thresh = find_optimal_cutoff(y_expl, prob_expl)[0]\n",
    "            pred_expl = prob_expl > thresh\n",
    "\n",
    "            fe = FAIMEvaluator(\n",
    "                y_true=y_expl,\n",
    "                y_pred=prob_expl,\n",
    "                y_pred_bin=pred_expl,\n",
    "                sen_var=sen_var,\n",
    "                fair_only=True,\n",
    "                weighted=weighted,\n",
    "                weights=weights,\n",
    "            )\n",
    "            fairmetrics = fe.fairmetrics\n",
    "            qc = fe.qc\n",
    "\n",
    "            fairmetrics_df.append(fairmetrics)\n",
    "            qc_df.append(qc)\n",
    "            by_group_list.append(fe.fairsummary)\n",
    "\n",
    "        fairmetrics_df = pd.concat(fairmetrics_df).reset_index(drop=True)\n",
    "        qc_df = pd.concat(qc_df)\n",
    "\n",
    "        return fairmetrics_df, qc_df\n",
    "        # self.thresh_list = thresh_list\n",
    "\n",
    "    def compare(self, dat_expl, optim_base_results, selected_vars, selected_vars_cat):\n",
    "        \"\"\"Compare the cases of exclusion of sensitive variables with the original optimal model i.e. no exclusion of sensitive variables\n",
    "\n",
    "        Args:\n",
    "            dat_expl (data frame): the data frame of the validation data\n",
    "            optim_base_results (object): the object of the optimal model regarding the specific case of exclusion of sensitive variables\n",
    "            selected_vars (list): the selected variables that can include sensitive variables\n",
    "            selected_vars_cat (list): the selected categorical variables that can include sensitive variables\n",
    "\n",
    "        Returns:\n",
    "            bool: whether the case of exclusion of sensitive variables will be expanded to the nearly optimal models\n",
    "\n",
    "        \"\"\"\n",
    "        x_with_constant_ori, sen_var, y_expl = self.data_process(\n",
    "            dat_expl, selected_vars=self.vars, selected_vars_cat=self.vars_cat\n",
    "        )\n",
    "        x_with_constant_base, sen_var, y_expl = self.data_process(\n",
    "            dat_expl, selected_vars=selected_vars, selected_vars_cat=selected_vars_cat\n",
    "        )\n",
    "        pred_ori = self.optim_results.predict(x_with_constant_ori)\n",
    "        pred_base = optim_base_results.predict(x_with_constant_base)\n",
    "\n",
    "        if self.criterion == \"auc\":\n",
    "            auc_ori = roc_auc_score(y_expl, pred_ori)\n",
    "            auc_base = roc_auc_score(y_expl, pred_base)\n",
    "\n",
    "            # return auc_base > auc_ori * (1-np.sqrt(self.epsilon))\n",
    "            return auc_base > auc_ori * np.sqrt(1 - self.epsilon)\n",
    "        if self.criterion == \"loss\":\n",
    "            loss_ori = self.optim_model.loglike(self.optim_results.params)\n",
    "            loss_base = optim_base_results.model.loglike(optim_base_results.params)\n",
    "            ratio = loss_base / loss_ori\n",
    "            print(f\"loss_ori: {loss_ori}, loss_base: {loss_base}, ratio: {ratio}\")\n",
    "            return ratio < np.sqrt(1 + self.epsilon)\n",
    "\n",
    "    def FAIM_model(self, dat_expl):\n",
    "        \"\"\"FAIM: Generate the nearly optimal models and compute the fairness metrics of the nearly optimal models\n",
    "\n",
    "        Args:\n",
    "            dat_expl (data frame): the data frame of the validation data\n",
    "        \"\"\"\n",
    "        self.dat_expl = dat_expl\n",
    "\n",
    "        self.coefs = {}\n",
    "        self.plots = {}\n",
    "        self.optim_base_obj_list = {}\n",
    "        self.fairmetrics_df = pd.DataFrame()\n",
    "        self.qc_df = pd.DataFrame()\n",
    "\n",
    "        if self.without_sen == \"auto\":\n",
    "            sen_senarios = generate_subsets(self.sen_name)\n",
    "            pbar = tqdm(\n",
    "                sen_senarios, desc=\"Generating nearly optimal models\", postfix=\"*Start*\"\n",
    "            )\n",
    "            for x in pbar:\n",
    "                pbar.set_postfix(postfix=f\"exclusion: {x}\")\n",
    "\n",
    "                selected_vars = [i for i in self.vars if i not in x]\n",
    "                selected_vars_cat = [i for i in self.vars_cat if i not in x]\n",
    "                optim_base_obj = self.optimal_model(selected_vars, selected_vars_cat)\n",
    "                optim_base_results = optim_base_obj.model_optim\n",
    "\n",
    "                if self.compare(\n",
    "                    dat_expl, optim_base_results, selected_vars, selected_vars_cat\n",
    "                ):\n",
    "                    self.optim_base_obj_list[\"_\".join(x)] = optim_base_obj\n",
    "\n",
    "                    if self.criterion == \"auc\":\n",
    "                        epsilon = 1 - np.sqrt(1 - self.epsilon)\n",
    "                    elif self.criterion == \"loss\":\n",
    "                        epsilon = np.sqrt(1 + self.epsilon) - 1\n",
    "\n",
    "                    coefs, plots = self.nearly_optimal_model(\n",
    "                        optim_base_obj, n_final=self.n_final, epsilon=epsilon\n",
    "                    )\n",
    "                    self.coefs[\"_\".join(x)] = coefs\n",
    "                    self.plots[\"_\".join(x)] = plots\n",
    "                    fairmetrics_df, qc_df = self.fairness_compute(\n",
    "                        dat_expl,\n",
    "                        optim_base_obj,\n",
    "                        coefs,\n",
    "                        selected_vars=selected_vars,\n",
    "                        selected_vars_cat=selected_vars_cat,\n",
    "                    )\n",
    "                    fairmetrics_df[\"auc\"] = coefs[\"perf_metric\"]\n",
    "                    qc_df[\"auc\"] = coefs[\"perf_metric\"]\n",
    "\n",
    "                    fairmetrics_df[\"sen_var_exclusion\"] = \"_\".join(x)\n",
    "                    qc_df[\"sen_var_exclusion\"] = \"_\".join(x)\n",
    "                    self.fairmetrics_df = pd.concat(\n",
    "                        [self.fairmetrics_df, fairmetrics_df]\n",
    "                    )\n",
    "                    self.qc_df = pd.concat([self.qc_df, qc_df])\n",
    "\n",
    "                else:\n",
    "                    print(f\"Exclusion of {x} degrades the discrimination performance!\")\n",
    "\n",
    "            self.fairmetrics_df = self.fairmetrics_df.reset_index(drop=True)\n",
    "            self.qc_df = self.qc_df.reset_index(drop=True)\n",
    "\n",
    "            # return fairmetrics_df, qc_df\n",
    "\n",
    "    def describe(self, selected_metrics=None):\n",
    "        \"\"\"Describe the distribution of fairness metrics for all nearly optimal models\n",
    "        Args:\n",
    "            selected_metrics (list, optional): the selected fairness metrics, e.g. [\"Statistical Parity\", \"Equalized Odds\", \"Average Accuracy\"]. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            fig: the plot of the distribution of fairness metrics for all nearly optimal models\n",
    "        \"\"\"\n",
    "        sen_var_exclusion = self.fairmetrics_df[\"sen_var_exclusion\"]\n",
    "        auc_var = self.fairmetrics_df[\"auc\"]\n",
    "        fairmetrics_df = self.fairmetrics_df.drop(columns=[\"sen_var_exclusion\"])\n",
    "        qc_df = self.qc_df.drop(columns=[\"sen_var_exclusion\", \"auc\"])\n",
    "\n",
    "        if selected_metrics is None:\n",
    "            num_metrics = fairmetrics_df.shape[1]\n",
    "        else:\n",
    "            for m in selected_metrics:\n",
    "                if m not in fairmetrics_df.columns:\n",
    "                    raise ValueError(f\"The metric {m} is not in the fairness metrics!\")\n",
    "\n",
    "            qc_df = qc_df[selected_metrics]\n",
    "            fairmetrics_df = fairmetrics_df[selected_metrics]\n",
    "            num_metrics = len(selected_metrics)\n",
    "\n",
    "        ids_after_qc = np.arange(qc_df.shape[0])[\n",
    "            (np.sum(qc_df, 1) == qc_df.shape[1]).tolist()\n",
    "        ]\n",
    "        print(f\"{len(ids_after_qc)} are qualified after quality control\")\n",
    "\n",
    "        min_ones = fairmetrics_df.iloc[ids_after_qc, :].apply(axis=0, func=np.argmin)\n",
    "\n",
    "        for m in min_ones.index:\n",
    "            id = fairmetrics_df.iloc[ids_after_qc, :].index[min_ones[m]]\n",
    "            print(\n",
    "                f\"the model with minimal {m}: No.{id} -- {fairmetrics_df.loc[id, m]:.3f}, with {sen_var_exclusion[id]} excluded from regression\"\n",
    "            )\n",
    "\n",
    "        plot_df = self.fairmetrics_df.loc[ids_after_qc, :]\n",
    "        fig = plot_distribution(plot_df)\n",
    "\n",
    "        if len(ids_after_qc) < fairmetrics_df.shape[0]:\n",
    "            return (\n",
    "                fairmetrics_df.iloc[ids_after_qc, :],\n",
    "                [sen_var_exclusion[i] for i in ids_after_qc],\n",
    "                fig,\n",
    "            )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def transmit(\n",
    "        self,\n",
    "        targeted_metrics=[\"Average Accuracy\", \"Statistical Parity\", \"Equalized Odds\"],\n",
    "        thresh_show=0.3,\n",
    "        best_id=None,\n",
    "        best_sen_exclusion=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Select the best model regarding fairness\n",
    "\n",
    "        Args:\n",
    "            targeted_metrics (list, optional): the targeted fairness metrics. Defaults to [\"Average Accuracy\", \"Statistical Parity\", \"Equalized Odds\"].\n",
    "            thresh_show (float, optional): the threshold to filter the models. Defaults to 0.3.\n",
    "\n",
    "        Returns:\n",
    "            best_coef: the coefficients of the best model\n",
    "            best_sen_exclusion: the sensitive variables excluded from the best model\n",
    "            best_optim_base_obj: the object of the best model\n",
    "            p: the radar plot of the distribution of fairness metrics for all nearly optimal models\n",
    "        \"\"\"\n",
    "        FAIM_area_list = []\n",
    "        ids = np.sum(\n",
    "            self.fairmetrics_df[targeted_metrics] < thresh_show, axis=1\n",
    "        ) == len(targeted_metrics)\n",
    "        if len(ids) == 0:\n",
    "            raise ValueError(\"The thresh is too low!\")\n",
    "        fairmetrics_df = self.fairmetrics_df.loc[ids, :]\n",
    "\n",
    "        sen_var_exclusion = fairmetrics_df[\"sen_var_exclusion\"]\n",
    "        perf = fairmetrics_df[\"auc\"]\n",
    "        df = fairmetrics_df.drop(columns=[\"sen_var_exclusion\"])\n",
    "        df = df[targeted_metrics]\n",
    "\n",
    "        print(f\"There are {df.shape[0]} models for final fairness selection.\")\n",
    "\n",
    "        if \"title\" in kwargs.keys():\n",
    "            title = kwargs[\"title\"]\n",
    "        else:\n",
    "            title = None\n",
    "        p_radar = plot_radar(df, thresh_show=thresh_show, title=title)\n",
    "        # p_radar.show()\n",
    "        p, fair_idx_df = plot_scatter(df, perf, sen_var_exclusion, title=title)\n",
    "        self.p = p\n",
    "        p.show()\n",
    "\n",
    "        for i, id in enumerate(df.index):\n",
    "            values = df.loc[id, :]\n",
    "            FAIM_area_list.append(fairarea(values))\n",
    "        ranking = np.argsort(np.argsort(FAIM_area_list))\n",
    "\n",
    "        if best_id is not None:\n",
    "            assert best_sen_exclusion is not None\n",
    "            self.best_id = best_id\n",
    "            self.best_sen_exclusion = best_sen_exclusion\n",
    "        else:\n",
    "            self.best_id = df.index[np.where(ranking == 0)][0]\n",
    "            self.best_sen_exclusion = sen_var_exclusion.iloc[np.argmin(FAIM_area_list)]\n",
    "\n",
    "        id_senario = [\n",
    "            index\n",
    "            for index, item in enumerate(list(self.coefs.keys()))\n",
    "            if item == self.best_sen_exclusion\n",
    "        ][0]\n",
    "\n",
    "        self.best_coef = (\n",
    "            self.coefs[self.best_sen_exclusion]\n",
    "            .drop(columns=[\"perf_metric\"])\n",
    "            .loc[self.best_id - self.n_final * id_senario, :]\n",
    "        )\n",
    "        self.best_optim_base_obj = self.optim_base_obj_list[self.best_sen_exclusion]\n",
    "\n",
    "        # confidence interval\n",
    "        dat_uncertainty = self.dat_train.sample(\n",
    "            n=np.min([50000, self.dat_train.shape[0]]), random_state=42\n",
    "        )\n",
    "        excluded_vars = self.best_sen_exclusion.split(\"_\")\n",
    "        selected_vars = [i for i in self.vars if i not in excluded_vars]\n",
    "        selected_vars_cat = [i for i in self.vars_cat if i not in excluded_vars]\n",
    "        x_with_constant = self.data_process(\n",
    "            dat_uncertainty,\n",
    "            selected_vars=selected_vars,\n",
    "            selected_vars_cat=selected_vars_cat,\n",
    "        )[0].values\n",
    "\n",
    "        prob_train, _, _ = self.test(dat_uncertainty)\n",
    "        best_se = None\n",
    "        fisher_information = (\n",
    "            x_with_constant.T @ np.diag(prob_train * (1 - prob_train)) @ x_with_constant\n",
    "        )\n",
    "        print(\"multiplication successed!\")\n",
    "        cov = np.linalg.pinv(fisher_information)\n",
    "        best_se = [np.sqrt(cov[i, i]) for i in range(cov.shape[0])]\n",
    "\n",
    "        # self.best_thresh = self.thresh_list[self.best_id]\n",
    "        best_results = {\n",
    "            \"best_coef\": self.best_coef,\n",
    "            \"best_sen_exclusion\": self.best_sen_exclusion,\n",
    "            \"best_se\": best_se,\n",
    "            \"best_optim_base_obj\": self.best_optim_base_obj,\n",
    "        }\n",
    "\n",
    "        return best_results, fair_idx_df\n",
    "\n",
    "    def post_mitigate(self):\n",
    "        pass\n",
    "\n",
    "    def test(self, dat_test, model=None, params=None, thresh=None):\n",
    "        \"\"\"Test the best model regarding fairness\n",
    "\n",
    "        Args:\n",
    "            dat_test (data frame): the data frame of the test data\n",
    "            model (object, optional): the object of the model to be tested. Defaults to None.\n",
    "            params (optional): the parameters of the model to be tested. Defaults to None.\n",
    "            thresh (optional): the threshold of the predictions. Defaults to None.\n",
    "\n",
    "        Methods:\n",
    "        +--------------+------------+----------------------------------------+\n",
    "        | Model        | Params     | Description                            |\n",
    "        +--------------+------------+----------------------------------------+\n",
    "        | None         | None       | Test the best model produced by FAIM.  |\n",
    "        +--------------+------------+----------------------------------------+\n",
    "        | model results| None       | Test the provided model with parameters|\n",
    "        |              |            | embedded.                              |\n",
    "        +--------------+------------+----------------------------------------+\n",
    "        | model results| as required| Test the provided model with the       |\n",
    "        |              |            | parameters additionally provided.      |\n",
    "        +--------------+------------+----------------------------------------+\n",
    "\n",
    "        Returns:\n",
    "            prob_test: the predicted probabilities of the test data\n",
    "            fairmetrics: the fairness metrics of the test data\n",
    "            fairsummary: the fairness summary of the test data for each subgroup\n",
    "\n",
    "        \"\"\"\n",
    "        self.dat_test = dat_test\n",
    "        excluded_vars = self.best_sen_exclusion.split(\"_\")\n",
    "        selected_vars = [i for i in self.vars if i not in excluded_vars]\n",
    "        selected_vars_cat = [i for i in self.vars_cat if i not in excluded_vars]\n",
    "        x_with_constant, sen_var, y_test = self.data_process(\n",
    "            dat_test, selected_vars=selected_vars, selected_vars_cat=selected_vars_cat\n",
    "        )\n",
    "\n",
    "        if model is None:\n",
    "            prob_test = self.best_optim_base_obj.model_optim.model.predict(\n",
    "                params=self.best_coef, exog=x_with_constant\n",
    "            )\n",
    "        else:\n",
    "            if isinstance(model, type(self.optim_results)) and params is None:\n",
    "                prob_test = model.predict(exog=x_with_constant)\n",
    "            elif isinstance(model, type(self.optim_model)) and params is not None:\n",
    "                prob_test = model.predict(params=params, exog=x_with_constant)\n",
    "            else:\n",
    "                raise ValueError(\"Please provide the right model!\")\n",
    "\n",
    "        thresh = find_optimal_cutoff(y_test, prob_test)[0]\n",
    "        pred_test = prob_test > thresh\n",
    "        fe = FAIMEvaluator(\n",
    "            y_true=np.array(y_test),\n",
    "            y_pred_bin=pred_test,\n",
    "            y_pred=prob_test,\n",
    "            sen_var=sen_var,\n",
    "            weighted=self.weighted,\n",
    "            weights=self.weights,\n",
    "        )\n",
    "        fairmetrics = fe.fairmetrics\n",
    "        clametrics = fe.clametrics\n",
    "\n",
    "        return prob_test, fairmetrics, clametrics\n",
    "\n",
    "\n",
    "    def explain(self, method=\"best\"):\n",
    "        \"\"\"Compute SHAP values for FAIM (best) or baseline (ori) model.\"\"\"\n",
    "\n",
    "        import shap\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import os\n",
    "\n",
    "        # Choose model + coefficients\n",
    "        if method == \"best\":\n",
    "            coef = self.best_coef\n",
    "            excluded = set(self.best_sen_exclusion.split(\"_\"))\n",
    "            used_vars = [v for v in self.vars if v not in excluded]\n",
    "            used_vars_cat = [v for v in self.vars_cat if v not in excluded]\n",
    "            model = self.best_optim_base_obj.model_optim.model  # 18 features\n",
    "        else:  # method == \"ori\"\n",
    "            coef = self.optim_results.params\n",
    "            used_vars = self.vars\n",
    "            used_vars_cat = self.vars_cat\n",
    "            model = self.optim_model  # <-- FIX: Use the ORIGINAL model (23 features)\n",
    "\n",
    "        model_cols = model.exog_names  # Get the correct columns for THIS model\n",
    "\n",
    "        # ---- Build background data ----\n",
    "        bg_full = self.data_process(\n",
    "            self.dat_train,\n",
    "            selected_vars=used_vars,\n",
    "            selected_vars_cat=used_vars_cat,\n",
    "        )[0]\n",
    "\n",
    "        # IMPORTANT: reduce to model columns BEFORE kmeans\n",
    "        bg_full = bg_full[model_cols]\n",
    "\n",
    "        # Now safe to summarize\n",
    "        bg_data = shap.kmeans(bg_full, k=50)\n",
    "\n",
    "        # ---- Build explain data ----\n",
    "        ex_full = self.data_process(\n",
    "            self.dat_expl,\n",
    "            selected_vars=used_vars,\n",
    "            selected_vars_cat=used_vars_cat,\n",
    "        )[0]\n",
    "\n",
    "        # Also reduce ex_data to model columns\n",
    "        ex_data = ex_full[model_cols].sample(n=200, random_state=42)\n",
    "\n",
    "        # ---- Model function that ALWAYS uses correct columns ----\n",
    "        def f(X):\n",
    "            if not isinstance(X, pd.DataFrame):\n",
    "                X = pd.DataFrame(X, columns=model_cols)\n",
    "            return model.predict(params=coef, exog=X)\n",
    "\n",
    "        # ---- Run SHAP ----\n",
    "        explainer = shap.KernelExplainer(f, bg_data)\n",
    "        shap_vals = explainer.shap_values(ex_data)\n",
    "\n",
    "        # KernelExplainer output may be list\n",
    "        shap_vals = shap_vals[1] if isinstance(shap_vals, list) else shap_vals\n",
    "\n",
    "        # ---- Save ----\n",
    "        output_dir = os.path.join(self.output_dir, \"explain\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        pd.DataFrame(shap_vals, columns=model_cols).to_csv(\n",
    "            os.path.join(output_dir, f\"{method}.csv\")\n",
    "        )\n",
    "\n",
    "        return shap_vals\n",
    "\n",
    "        def f(bg):\n",
    "            model = self.best_optim_base_obj.model_optim.model\n",
    "            if method == \"best\":\n",
    "                return model.predict(params=self.best_coef, exog=bg)\n",
    "            else:\n",
    "                return model.predict(params=self.optim_results.params, exog=bg)\n",
    "\n",
    "        output_dir = os.path.join(self.output_dir, \"explain\")\n",
    "        if method == \"best\":\n",
    "            excluded_vars = self.best_sen_exclusion.split(\"_\")\n",
    "            selected_vars = [i for i in self.vars if i not in excluded_vars]\n",
    "            selected_vars_cat = [i for i in self.vars_cat if i not in excluded_vars]\n",
    "\n",
    "        else:\n",
    "            selected_vars = self.vars\n",
    "            selected_vars_cat = self.vars_cat\n",
    "\n",
    "        bg_data = self.data_process(\n",
    "            self.dat_train,\n",
    "            selected_vars=selected_vars,\n",
    "            selected_vars_cat=selected_vars_cat,\n",
    "        )[0].sample(n=1000, random_state=42)\n",
    "        ex_data = self.data_process(\n",
    "            self.dat_expl,\n",
    "            selected_vars=selected_vars,\n",
    "            selected_vars_cat=selected_vars_cat,\n",
    "        )[0].sample(n=200, random_state=42)\n",
    "\n",
    "        e = shap.KernelExplainer(f, bg_data)\n",
    "        shap_values_train = e.shap_values(ex_data)\n",
    "        shap_values_train_1 = shap_values_train[1].squeeze()\n",
    "        pd.DataFrame(shap_values_train_1).to_csv(\n",
    "            os.path.join(output_dir, f\"{method}.csv\")\n",
    "        )\n",
    "\n",
    "        return shap_values_train_1\n",
    "\n",
    "\n",
    "    def compare_explain(self, overide=True, top_n=None):\n",
    "        \"\"\"Compare the SHAP values of the best model and original model\"\"\"\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        import textwrap\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        def add_bar_labels(ax, values, fmt=\"{:.4f}\", padding=3):\n",
    "            \"\"\"\n",
    "            Add numeric labels to horizontal bar plots.\n",
    "            \"\"\"\n",
    "            for i, v in enumerate(values):\n",
    "                ax.text(\n",
    "                    v,\n",
    "                    i,\n",
    "                    fmt.format(v),\n",
    "                    va=\"center\",\n",
    "                    ha=\"left\",\n",
    "                    fontsize=16  #  CHANGED from 10 to 16\n",
    "                )\n",
    "        def clean_spines(ax):\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "            ax.spines[\"left\"].set_visible(True)\n",
    "            ax.spines[\"bottom\"].set_visible(True)\n",
    "\n",
    "        output_dir = os.path.join(self.output_dir, \"explain\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # ---------- LOAD SHAP VALUES ----------\n",
    "        if (not os.path.exists(os.path.join(output_dir, \"best.csv\"))) or overide:\n",
    "            shap_best = self.explain(method=\"best\")\n",
    "        else:\n",
    "            shap_best = pd.read_csv(\n",
    "                os.path.join(output_dir, \"best.csv\"), index_col=0\n",
    "            ).values  # Don't reshape - keep as 2D\n",
    "\n",
    "        if (not os.path.exists(os.path.join(output_dir, \"ori.csv\"))) or overide:\n",
    "            shap_ori = self.explain(method=\"ori\")\n",
    "        else:\n",
    "            shap_ori = pd.read_csv(\n",
    "                os.path.join(output_dir, \"ori.csv\"), index_col=0\n",
    "            ).values  # Don't reshape - keep as 2D\n",
    "\n",
    "        # ---------- AGGREGATE SHAP VALUES (mean absolute) ----------\n",
    "        # Take mean absolute SHAP value across all samples for each feature\n",
    "        shap_best_agg = np.mean(np.abs(shap_best), axis=0)\n",
    "        shap_ori_agg = np.mean(np.abs(shap_ori), axis=0)\n",
    "\n",
    "        # ---------- SORT + SELECT TOP FEATURES ----------\n",
    "        features_best = self.best_coef.index\n",
    "        features_ori = self.optim_results.params.index\n",
    "\n",
    "        # If top_n is None, use all features\n",
    "        n_best = len(features_best) if top_n is None else top_n\n",
    "        n_ori = len(features_ori) if top_n is None else top_n\n",
    "\n",
    "        shap_best_df = (\n",
    "            pd.DataFrame({\"feature\": features_best, \"shap\": shap_best_agg})\n",
    "            .sort_values(\"shap\", ascending=False)\n",
    "            .head(n_best)\n",
    "        )\n",
    "\n",
    "        shap_ori_df = (\n",
    "            pd.DataFrame({\"feature\": features_ori, \"shap\": shap_ori_agg})\n",
    "            .sort_values(\"shap\", ascending=False)\n",
    "            .head(n_ori)\n",
    "        )\n",
    "\n",
    "        # ---------- CLEAN LABELS ----------\n",
    "        def clean_label(lbl, max_len=40):\n",
    "            lbl = lbl.replace(\"_\", \" \")\n",
    "            if len(lbl) > max_len:\n",
    "                return lbl[:max_len] + \"...\"\n",
    "            return lbl\n",
    "\n",
    "        shap_best_df[\"feature\"] = shap_best_df[\"feature\"].apply(clean_label)\n",
    "        shap_ori_df[\"feature\"] = shap_ori_df[\"feature\"].apply(clean_label)\n",
    "\n",
    "        # ---------- MAKE PLOTS WITH BIGGER SIZE ----------\n",
    "        fig, axes = plt.subplots(\n",
    "            1,\n",
    "            2,\n",
    "            figsize=(22, 12),  #  INCREASED from (18, 10) to (22, 12)\n",
    "            sharex=False\n",
    "        )\n",
    "\n",
    "        # BEST MODEL\n",
    "        axes[0].barh(\n",
    "            shap_best_df[\"feature\"],\n",
    "            shap_best_df[\"shap\"]\n",
    "        )\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].set_title(\n",
    "            \"Fairness-aware model (FAIM) - Top SHAP features\",\n",
    "            fontsize=20,  #  INCREASED from 16 to 20\n",
    "            weight=\"bold\"\n",
    "        )\n",
    "        axes[0].set_xlabel(\"Mean |SHAP value|\", fontsize=18)  #  INCREASED from 16 to 18\n",
    "        add_bar_labels(axes[0], shap_best_df[\"shap\"].values)\n",
    "        axes[0].tick_params(axis=\"y\", labelsize=16)\n",
    "        axes[0].tick_params(axis=\"x\", labelsize=14)  #  ADDED for x-axis\n",
    "        clean_spines(axes[0])\n",
    "\n",
    "        # ORIGINAL MODEL\n",
    "        axes[1].barh(\n",
    "            shap_ori_df[\"feature\"],\n",
    "            shap_ori_df[\"shap\"],\n",
    "            color='darkorange'\n",
    "        )\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].set_title(\n",
    "            \"Fairness-unaware model (Baseline) - Top SHAP features\",\n",
    "            fontsize=20,  #  INCREASED from 16 to 20\n",
    "            weight=\"bold\"\n",
    "        )\n",
    "        axes[1].set_xlabel(\"Mean |SHAP value|\", fontsize=18)  #  INCREASED from 16 to 18\n",
    "        add_bar_labels(axes[1], shap_ori_df[\"shap\"].values)\n",
    "        axes[1].tick_params(axis=\"y\", labelsize=16)\n",
    "        axes[1].tick_params(axis=\"x\", labelsize=14)  #  ADDED for x-axis\n",
    "        clean_spines(axes[1])\n",
    "\n",
    "        # CLEAN LAYOUT\n",
    "        plt.tight_layout(pad=4)\n",
    "        plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "        # SAVE TOO\n",
    "        plt.savefig(os.path.join(output_dir, \"shap_compare.png\"), dpi=300)\n",
    "\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SX-HVJjpGib"
   },
   "source": [
    "##fairness_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuI0pd2PwSBD"
   },
   "outputs": [],
   "source": [
    "def rgb01_hex(col):\n",
    "    col_hex = [round(i * 255) for i in col]\n",
    "    col_hex = \"#%02x%02x%02x\" % tuple(col_hex)\n",
    "    return col_hex\n",
    "\n",
    "\n",
    "def compute_area(fairness_metrics):\n",
    "    n_metric = len(fairness_metrics)\n",
    "    tmp = fairness_metrics.values.flatten().tolist()\n",
    "    tmp_1 = tmp[1:] + tmp[:1]\n",
    "\n",
    "    if n_metric > 2:\n",
    "        theta_c = 2 * np.pi / n_metric\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
    "    elif n_metric == 2:\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
    "    else:\n",
    "        area = np.abs(tmp[0])\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def plot_perf_metric(\n",
    "    perf_metric, eligible, x_range, select=None, plot_selected=False, x_breaks=None\n",
    "):\n",
    "    \"\"\" Plot performance metrics of sampled models\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        perf_metric : numpy.array or pandas.Series\n",
    "            Numeric vector of performance metrics for all sampled models\n",
    "        eligible : numpy.array or pandas.Series\n",
    "            Boolean vector of the same length of 'perf_metric', indicating \\\n",
    "                whether each sample is eligible.\n",
    "        x_range : list\n",
    "            Numeric vector indicating the range of eligible values for \\\n",
    "                performance metrics.\n",
    "            Will be indicated by dotted vertical lines in plots.\n",
    "        select : list or numpy.array, optional (default: None)\n",
    "            Numeric vector of indexes of 'perf_metric' to be selected\n",
    "        plot_selected : bool, optional (default: False)\n",
    "            Whether performance metrics of selected models should be plotted in \\\n",
    "                a secondary figure.\n",
    "        x_breaks : list, optional (default: None)\n",
    "            If selected models are to be plotted, the breaks to use in the \\\n",
    "                histogram\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        plot : plotnine.ggplot\n",
    "            Histogram(s) of model performance made using ggplot\n",
    "    \"\"\"\n",
    "    m = len(perf_metric)\n",
    "    perf_df = pd.DataFrame(perf_metric, columns=[\"perf_metric\"], index=None)\n",
    "    plot = (\n",
    "        pn.ggplot(perf_df, pn.aes(x=\"perf_metric\"))\n",
    "        + pn.geoms.geom_histogram(\n",
    "            breaks=np.linspace(np.min(perf_metric), np.max(perf_metric), 40)\n",
    "        )\n",
    "        + pn.geoms.geom_vline(xintercept=x_range, linetype=\"dashed\", size=0.7)\n",
    "        + pn.labels.labs(\n",
    "            x=\"Ratio of loss to minimum loss\",\n",
    "            title=\"\"\"Loss of {m:d} sampled models\n",
    "                \\n{n_elg:d} ({per_elg:.1f}%) sampled models are eligible\"\"\".format(\n",
    "                m=m, n_elg=np.sum(eligible), per_elg=np.sum(eligible) * 100 / m\n",
    "            ),\n",
    "        )\n",
    "        + pn.themes.theme_bw()\n",
    "        + pn.themes.theme(\n",
    "            title=pn.themes.element_text(ha=\"left\"),\n",
    "            axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
    "            axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
    "        )\n",
    "    )\n",
    "    if plot_selected:\n",
    "        if select is None:\n",
    "            print(\"'select' vector is not specified!\\nUsing all models instead\")\n",
    "            select = [i for i in range(len(perf_df))]\n",
    "        try:\n",
    "            perf_select = perf_df.iloc[select]\n",
    "        except:\n",
    "            print(\n",
    "                \"Invalid indexes detected in 'select' vector!\\nUsing all models instead\"\n",
    "            )\n",
    "            select = [i for i in range(len(perf_df))]\n",
    "            perf_select = perf_df.iloc[select]\n",
    "        plot2 = (\n",
    "            pn.ggplot(perf_select, pn.aes(x=\"perf_metric\"))\n",
    "            + pn.geoms.geom_histogram(breaks=x_breaks)\n",
    "            + pn.labels.labs(\n",
    "                x=\"Ratio of loss to minimum loss\",\n",
    "                title=\"{n_select:d} selected models\".format(n_select=len(select)),\n",
    "            )\n",
    "            + pn.themes.theme_bw()\n",
    "            + pn.themes.theme(\n",
    "                title=pn.themes.element_text(ha=\"left\"),\n",
    "                axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
    "                axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
    "            )\n",
    "        )\n",
    "        return (plot, plot2)\n",
    "    else:\n",
    "        return plot\n",
    "\n",
    "\n",
    "def plot_distribution(df, s=4):\n",
    "    num_metrics = df.shape[1] - 2\n",
    "    labels = df.sen_var_exclusion.unique()\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == \"\":\n",
    "            labels[i] = \"No exclusion\"\n",
    "        elif len(labels[i].split(\"_\")) == 2:\n",
    "            labels[i] = f\"Exclusion of {' and '.join(labels[i].split('_'))}\"\n",
    "        elif len(labels[i].split(\"_\")) > 2:\n",
    "            sens = labels[i].split(\"_\")\n",
    "            labels[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
    "        else:\n",
    "            labels[i] = f\"Exclusion of {labels[i]}\"\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(s * num_metrics, s))\n",
    "    for i, x in enumerate(df.columns[:-2]):\n",
    "        ax = axes[i]\n",
    "        # sns.jointplot(data=df, x=x, y=\"auc\", hue=\"sen_var_exclusion\",  ax=ax, legend=False)\n",
    "        sns.histplot(\n",
    "            data=df, x=x, hue=\"sen_var_exclusion\", bins=50, ax=ax, legend=False\n",
    "        )  # layout=(1, num_metrics), figsize=(4, 4), color=\"#595959\",\n",
    "        ax.set_title(x)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Count\" if i == 0 else \"\")\n",
    "\n",
    "    plt.legend(\n",
    "        loc=\"center left\",\n",
    "        title=\"\",\n",
    "        labels=labels[::-1],\n",
    "        ncol=1,\n",
    "        bbox_to_anchor=(1.04, 0.5),\n",
    "        borderaxespad=0,\n",
    "    )\n",
    "    # plt.tight_layout() bbox_transform=fig.transFigure,\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_scatter(df, perf, sen_var_exclusion, title, c1=20, c2=0.15, **kwargs):\n",
    "    ### basic settings ###\n",
    "    np.random.seed(0)\n",
    "    if \"figsize\" not in kwargs.keys():\n",
    "        fig_h = 400\n",
    "        figsize = [fig_h * df.shape[1] * 2.45 / 3, fig_h]\n",
    "    else:\n",
    "        figsize = kwargs[\"figsize\"]\n",
    "    caption_size = figsize[1] / c1  # control font size / figure size\n",
    "    fig_caption_ratio = 0.8\n",
    "    fig_font_size = caption_size * fig_caption_ratio\n",
    "\n",
    "    font_family = \"Arial\"\n",
    "    highlight_color = \"#D4AF37\"\n",
    "    fig_font_unit = c2  # control the relative position of elements\n",
    "    caption_font_unit = fig_font_unit * fig_caption_ratio\n",
    "    d = fig_font_unit / 8\n",
    "    legend_pos_y = 1 + fig_font_unit\n",
    "    subtitle_pos = [legend_pos_y + d, legend_pos_y + d + caption_font_unit]\n",
    "    xlab_pos_y = -fig_font_unit * 2\n",
    "\n",
    "    area_list = []\n",
    "    for i, id in enumerate(df.index):\n",
    "        values = df.loc[id, :]\n",
    "        area_list.append(1 / compute_area(values))\n",
    "    ranking = np.argsort(np.argsort(area_list)[::-1])\n",
    "\n",
    "    # map model id to index position (SAFE indexing)\n",
    "    id_to_pos = {df.index[i]: i for i in range(len(df))}\n",
    "\n",
    "    # jittering for display\n",
    "    jitter_control = np.zeros(len(ranking))\n",
    "    for idx in range(len(ranking)):\n",
    "        if ranking[idx] == 0:\n",
    "            jitter_control[idx] = 0\n",
    "        elif ranking[idx] <= 10 and ranking[idx] != 0:\n",
    "            jitter_control[idx] = 0.01 * np.random.uniform(0, 1)\n",
    "        elif ranking[idx] <= 10**2 and ranking[idx] > 10:\n",
    "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
    "        elif ranking[idx] <= 10**3 and ranking[idx] > 10**2:\n",
    "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
    "        else:\n",
    "            jitter_control[idx] = 0.02 * np.random.uniform(-1, 1)\n",
    "\n",
    "    ### plot ###\n",
    "    best_id = df.index[np.where(ranking == 0)][0]\n",
    "    worst_id = df.index[np.argmin(area_list)]\n",
    "    meduim_id = df.index[np.argsort(area_list)[int(len(area_list) / 2)]]\n",
    "\n",
    "    num_metrics = df.shape[1]\n",
    "    num_models = df.shape[0]\n",
    "\n",
    "    fig = make_subplots(cols=num_metrics, rows=1, horizontal_spacing=0.13)\n",
    "    cmap = sns.light_palette(\"steelblue\", as_cmap=False, n_colors=df.shape[0])\n",
    "    cmap = cmap[::-1]\n",
    "    colors = [rgb01_hex(cmap[x]) if x != 0 else highlight_color for x in ranking]\n",
    "    sizes = [10 if x != 0 else 20 for x in ranking]\n",
    "\n",
    "    shapes = sen_var_exclusion.copy().tolist()\n",
    "    cases = sen_var_exclusion.unique()\n",
    "    shapes_candidates = [\"square\", \"circle\", \"triangle-up\", \"star\"][: len(cases)]\n",
    "    for i, case in enumerate(cases):\n",
    "        for j, v in enumerate(sen_var_exclusion):\n",
    "            if v == case:\n",
    "                shapes[j] = shapes_candidates[i]\n",
    "\n",
    "        if cases[i] == \"\":\n",
    "            cases[i] = \"No exclusion\"\n",
    "        elif len(cases[i].split(\"_\")) == 2:\n",
    "            cases[i] = f\"Exclusion of {' and '.join(cases[i].split('_'))}\"\n",
    "        elif len(cases[i].split(\"_\")) > 2:\n",
    "            sens = cases[i].split(\"_\")\n",
    "            cases[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
    "        else:\n",
    "            cases[i] = f\"Exclusion of {cases[i]}\"\n",
    "\n",
    "    fair_index_df = pd.DataFrame(\n",
    "        {\n",
    "            \"model id\": df.index,\n",
    "            \"fair_index\": area_list,\n",
    "            \"ranking\": ranking,\n",
    "            \"eod\": df[\"Equalized Odds\"],\n",
    "            \"colors\": colors,\n",
    "            \"shapes\": shapes,\n",
    "            \"sizes\": sizes,\n",
    "            \"cases\": sen_var_exclusion,\n",
    "            \"jitter\": jitter_control,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add scatter plots to the subplots\n",
    "    for k, s in enumerate(shapes_candidates):\n",
    "        for i in range(num_metrics):\n",
    "            # index of sen_var_exclusion(shape) == s\n",
    "            s_idx = [idx for idx, x in enumerate(shapes) if x == s]\n",
    "            x = df.iloc[s_idx, i].values\n",
    "            js = fair_index_df.loc[fair_index_df.shapes == s, \"jitter\"].values\n",
    "            jittered_x = x + js\n",
    "\n",
    "            col = fair_index_df.loc[fair_index_df.shapes == s, \"colors\"]\n",
    "            size = fair_index_df.loc[fair_index_df.shapes == s, \"sizes\"]\n",
    "            fair_index = fair_index_df.loc[fair_index_df.shapes == s, \"fair_index\"]\n",
    "            ids = fair_index_df.loc[fair_index_df.shapes == s, \"model id\"]\n",
    "            rank_text = fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
    "            r = (\n",
    "                fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
    "                .apply(lambda x: math.log10(x + 1))\n",
    "                .values\n",
    "            )\n",
    "            sen_case = fair_index_df.loc[fair_index_df.shapes == s, \"cases\"]\n",
    "\n",
    "            hovertext = [\n",
    "                f\"Fairness index: {f:.3f}. Ranking: {x}. Model id: {i}\"\n",
    "                for f, x, i in zip(fair_index, rank_text, ids)\n",
    "            ]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=r,\n",
    "                    y=jittered_x,\n",
    "                    customdata=hovertext,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        color=col,\n",
    "                        symbol=s,\n",
    "                        size=size,\n",
    "                        line=dict(color=col, width=1),\n",
    "                        opacity=0.8,\n",
    "                    ),\n",
    "                    hovertemplate=\"%{customdata}.\",\n",
    "                    hoverlabel=None,\n",
    "                    hoverinfo=\"name+z\",\n",
    "                    name=cases[k],\n",
    "                ),\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "            )\n",
    "\n",
    "            if i == int((df.shape[1] + 0.5) / 2):\n",
    "                fig.update_xaxes(\n",
    "                    title_text=None,\n",
    "                    tickvals=[0, 1, 2, 3],\n",
    "                    ticktext=[1, 10, 100, 1000],\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                    tickangle=0,\n",
    "                )\n",
    "            else:\n",
    "                fig.update_xaxes(\n",
    "                    title_text=None,\n",
    "                    tickvals=[0, 1, 2, 3],\n",
    "                    ticktext=[1, 10, 100, 1000],\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                    tickangle=0,\n",
    "                )\n",
    "            fig.update_yaxes(\n",
    "                title_text=df.columns[i],\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "                showticksuffix=\"none\",\n",
    "                titlefont={\"size\": caption_size},\n",
    "            )\n",
    "\n",
    "            fig.add_vline(\n",
    "                x=0,\n",
    "                line_width=2,\n",
    "                line_dash=\"dot\",\n",
    "                line_color=highlight_color,\n",
    "                col=i + 1,\n",
    "                row=1,\n",
    "            )\n",
    "\n",
    "            min_metric = df.loc[ranking == 0, df.columns[i]].values[0]\n",
    "            max_metric = df.loc[ranking == num_models - 1, df.columns[i]].values[0]\n",
    "            meduim_metric = df.loc[\n",
    "                ranking == int(num_models / 2), df.columns[i]\n",
    "            ].values[0]\n",
    "\n",
    "            # add annotations\n",
    "            anno_size = caption_size * 0.7\n",
    "            if k == 0:\n",
    "                fig.add_hline(\n",
    "                    y=min_metric,\n",
    "                    line_width=2,\n",
    "                    line_dash=\"dot\",\n",
    "                    line_color=highlight_color,\n",
    "                    col=i + 1,\n",
    "                    row=1,\n",
    "                )\n",
    "\n",
    "                # position_y = np.mean(df.iloc[:, i])\n",
    "                min_annotation = {\n",
    "                    \"x\": 0,\n",
    "                    \"y\": min_metric,\n",
    "                    \"text\": f\"Model ID {best_id}<br> Rank No.1\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                }\n",
    "                fig.add_annotation(min_annotation, col=i + 1, row=1)\n",
    "            if meduim_id in ids:\n",
    "                medium_annotation = {\n",
    "                    \"x\": math.log10(int(num_models / 2) + 1),\n",
    "                    \"y\": meduim_metric + jitter_control[id_to_pos[meduim_id]],\n",
    "                    \"text\": f\"Model ID {meduim_id}<br> Rank No.{int(num_models/2)}\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                }\n",
    "                fig.add_annotation(medium_annotation, col=i + 1, row=1)\n",
    "            if worst_id in ids:\n",
    "                max_annotation = {\n",
    "                    \"x\": math.log10(num_models + 1),\n",
    "                    \"y\": max_metric + jitter_control[id_to_pos[worst_id]],\n",
    "                    \"text\": f\"Model ID {worst_id}<br> Rank No.{num_models}\",\n",
    "                    \"showarrow\": True,\n",
    "                    \"arrowhead\": 6,\n",
    "                    \"xanchor\": \"left\",\n",
    "                    \"yanchor\": \"bottom\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"font\": {\"size\": anno_size},\n",
    "                    \"ax\": -10,\n",
    "                    \"ay\": -10,\n",
    "                    \"xshift\": 0,\n",
    "                    \"yshift\": 0,\n",
    "                    \"align\": \"left\",\n",
    "                }\n",
    "                fig.add_annotation(max_annotation, col=i + 1, row=1)\n",
    "\n",
    "    colorbar_trace = go.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode=\"markers\",\n",
    "        hoverinfo=\"none\",\n",
    "        marker=dict(\n",
    "            colorscale=[\n",
    "                rgb01_hex(np.array((243, 244, 245)) / 255),\n",
    "                \"steelblue\",\n",
    "            ],  # \"magma\",\n",
    "            showscale=True,\n",
    "            cmin=0,\n",
    "            cmax=2,\n",
    "            colorbar=dict(\n",
    "                title=None,\n",
    "                thickness=10,\n",
    "                tickvals=[0, 2],\n",
    "                ticktext=[\"Low\", \"High\"],\n",
    "                outlinewidth=0,\n",
    "                orientation=\"v\",\n",
    "                x=1,\n",
    "                y=0.5,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    fig.add_trace(colorbar_trace)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        font=dict(family=\"Arial\", size=fig_font_size),\n",
    "        hovermode=\"closest\",\n",
    "        width=figsize[0],\n",
    "        height=figsize[1],\n",
    "        showlegend=True,\n",
    "        template=\"simple_white\",\n",
    "        legend=dict(x=0, y=legend_pos_y, orientation=\"h\"),\n",
    "    )\n",
    "\n",
    "    rectangle = {\n",
    "        \"type\": \"rect\",\n",
    "        \"x0\": -0.1,\n",
    "        \"y0\": subtitle_pos[0],\n",
    "        \"x1\": 1.1,\n",
    "        \"y1\": subtitle_pos[1],\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"fillcolor\": \"steelblue\",\n",
    "        \"opacity\": 0.1,\n",
    "    }  # 'line': {'color': 'red', 'width': 2},\n",
    "    fig.add_shape(rectangle)\n",
    "    subtitle_annotation = {\n",
    "        \"x\": -0.1,\n",
    "        \"y\": subtitle_pos[1],\n",
    "        \"text\": f\"<i> The FAIM model (i.e., fairness-aware model) is with model ID {best_id}, out of {num_models} nearly-optimal models.</i>\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": caption_size * 1.1},\n",
    "        \"align\": \"left\",\n",
    "    }\n",
    "    xaxis_annotation = {\n",
    "        \"x\": 0.5,\n",
    "        \"y\": xlab_pos_y,\n",
    "        \"text\": \"Model Rank\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": caption_size},\n",
    "    }\n",
    "    colorbar_title = {\n",
    "        \"x\": 1.05,\n",
    "        \"y\": 0.5,\n",
    "        \"text\": \"Fairness Ranking Index (FRI)\",\n",
    "        \"showarrow\": False,\n",
    "        \"xref\": \"paper\",\n",
    "        \"yref\": \"paper\",\n",
    "        \"font\": {\"size\": anno_size * 0.9},\n",
    "        \"textangle\": 90,\n",
    "    }\n",
    "    fig.add_annotation(subtitle_annotation)\n",
    "    fig.add_annotation(xaxis_annotation)\n",
    "    fig.add_annotation(colorbar_title)\n",
    "\n",
    "    for i, trace in enumerate(fig.data):\n",
    "        if i % num_metrics == 1:\n",
    "            trace.update(showlegend=True)\n",
    "        else:\n",
    "            trace.update(showlegend=False)\n",
    "    # fig.show()\n",
    "\n",
    "    return fig, fair_index_df\n",
    "\n",
    "\n",
    "def plot_radar(df, thresh_show, title, **kwargs):\n",
    "    fig = go.Figure()\n",
    "    # fig = sp.make_subplots(rows=1, cols=2)\n",
    "    cmap = sns.diverging_palette(200, 20, sep=10, s=50, as_cmap=False, n=df.shape[0])\n",
    "    theta = df.columns.tolist()\n",
    "    theta += theta[:1]\n",
    "    area_list = []\n",
    "\n",
    "    for i, id in enumerate(df.index):\n",
    "        values = df.loc[id, :]\n",
    "        area_list.append(compute_area(values))\n",
    "        values = values.values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        info = [\n",
    "            f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
    "        ]\n",
    "        fig.add_trace(\n",
    "            go.Scatterpolar(\n",
    "                r=values,\n",
    "                theta=theta,\n",
    "                fill=\"toself\" if id == \"FAIReg\" else \"none\",\n",
    "                text=\"\\n\".join(info),\n",
    "                name=f\"{id}\",\n",
    "                line=dict(color=rgb01_hex(cmap[i]), dash=\"dot\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ranking = np.argsort(np.argsort(area_list))\n",
    "    best_id = df.index[np.where(ranking == 0)][0]\n",
    "    print(\n",
    "        f\"The best model is No.{best_id} with metrics on validation set:\\n {df.loc[best_id, :]}\"\n",
    "    )\n",
    "    values = df.loc[best_id, :].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    info = [\n",
    "        f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
    "    ]\n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=theta,\n",
    "            fill=\"toself\",\n",
    "            text=\"\\n\".join(info),\n",
    "            name=f\"model {best_id}\",\n",
    "            line=dict(color=\"royalblue\", dash=\"solid\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        # title = title,\n",
    "        font=dict(family=\"Arial\", size=16),\n",
    "        polar=dict(\n",
    "            # bgcolor = \"#1e2130\",\n",
    "            radialaxis=dict(\n",
    "                showgrid=True,\n",
    "                gridwidth=1,\n",
    "                gridcolor=\"lightgray\",\n",
    "                visible=True,\n",
    "                range=[0, thresh_show],\n",
    "            )\n",
    "        ),\n",
    "        legend=dict(x=0.25, y=-0.1, orientation=\"h\"),\n",
    "        showlegend=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_bar(\n",
    "    shap_values, feature_names, original_feature_names, coef=None, title=None, **kwargs\n",
    "):\n",
    "    \"\"\"Plot the bar chart of feature importance\"\"\"\n",
    "    if \"color\" not in kwargs.keys():\n",
    "        color = \"steelblue\"\n",
    "    else:\n",
    "        color = kwargs[\"color\"]\n",
    "\n",
    "    def get_prefix(v):\n",
    "        if \"_\" in v and (v not in original_feature_names):\n",
    "            tmp = [\"_\".join(v.split(\"_\")[:i]) for i in range(len(v.split(\"_\")))]\n",
    "            return [s for s in tmp if s in original_feature_names][0]\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    if shap_values is not None:\n",
    "\n",
    "        grouped_df = pd.DataFrame({\"values\": shap_values}, index=feature_names).groupby(\n",
    "            by=get_prefix, axis=0\n",
    "        )\n",
    "        df = {k: np.mean(np.abs(g.values)) for k, g in grouped_df}\n",
    "        df = pd.DataFrame.from_dict(df, orient=\"index\").reset_index()\n",
    "        df.columns = [\"Var\", \"Value\"]\n",
    "\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in df.Value],\n",
    "                        \"order\": np.abs(df.Value),\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    elif coef is not None:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"Var\": coef.index,\n",
    "                \"Value\": coef.values,\n",
    "                \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in coef.values],\n",
    "                \"order\": np.abs(coef.values),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Either shap_value or coef should be provided\")\n",
    "\n",
    "    df = df.loc[df[\"Var\"] != \"const\", :]\n",
    "    df = df.sort_values(by=\"order\", ascending=True)\n",
    "    df[\"Var\"] = pd.Categorical(df[\"Var\"], categories=df[\"Var\"].tolist(), ordered=True)\n",
    "\n",
    "    common_theme = theme(\n",
    "        text=element_text(size=24),\n",
    "        panel_grid_major_y=element_line(colour=\"lightgrey\"),\n",
    "        panel_grid_minor=element_blank(),\n",
    "        panel_background=element_blank(),\n",
    "        axis_line_x=element_line(colour=\"black\"),\n",
    "        axis_ticks_major_y=element_blank(),\n",
    "    )\n",
    "\n",
    "    x_lab = \"Feature importance\"\n",
    "\n",
    "    p = (\n",
    "        ggplot(data=df, mapping=aes(x=\"Var\", y=\"Value\", fill=\"color\"))\n",
    "        + geom_hline(yintercept=0, color=\"grey\")\n",
    "        + geom_bar(stat=\"identity\")\n",
    "        + common_theme\n",
    "        + coord_flip()\n",
    "        + labs(x=\"\", y=x_lab, title=title)\n",
    "        + theme(legend_position=\"none\")\n",
    "        + scale_fill_manual(values=[color])\n",
    "    )\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2tWh6r0o9py"
   },
   "source": [
    "#Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN1Fbbk3_DBi"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import collections\n",
    "from pandas.io import gbq\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer\n",
    "import itertools\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmu_sO7xMRhy"
   },
   "outputs": [],
   "source": [
    "cohort = pd.read_csv(\"cohort.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZL12cvzMEnY"
   },
   "source": [
    "# Train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IjLThK9K0Kb",
    "outputId": "c74fb9ff-9610-417e-8522-ac27a855c909"
   },
   "outputs": [],
   "source": [
    "# Create 70/10/20 split\n",
    "dat_train, temp_df = train_test_split(cohort, test_size=0.3, random_state=42)\n",
    "dat_expl, dat_test = train_test_split(temp_df, test_size=2/3, random_state=42)\n",
    "\n",
    "print('Training dataset size = ', len(dat_train))\n",
    "print('Validation dataset size = ', len(dat_expl))\n",
    "print('Testing dataset size = ', len(dat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNO1FT3gwFx4",
    "outputId": "ed34fedd-0572-479b-fdfd-12e80b740268"
   },
   "outputs": [],
   "source": [
    "print(len(dat_test), dat_test[\"imv\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YESf8l3fGww0"
   },
   "source": [
    "# Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtMl6DhCMD39"
   },
   "outputs": [],
   "source": [
    "vitals_cols = ['temperature','heart_rate','resp_rate','sbp','dbp','sofa_24hours']\n",
    "imputer = SimpleImputer(strategy='median') #median for continuous and numeric\n",
    "\n",
    "dat_train[vitals_cols] = imputer.fit_transform(dat_train[vitals_cols])\n",
    "dat_expl[vitals_cols] = imputer.transform(dat_expl[vitals_cols])\n",
    "dat_test[vitals_cols] = imputer.transform(dat_test[vitals_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNbskXFeUoCX"
   },
   "source": [
    "# FAIM-specific preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARqBWrKnLSyU"
   },
   "outputs": [],
   "source": [
    "conditions = {\n",
    "    'hypertension': ['I10', 'I11', 'I12', 'I13'],\n",
    "    'congestive_heart_failure': ['I50'],\n",
    "    'copd': ['J44'],\n",
    "    'asthma': ['J45'],\n",
    "    'coronary_artery_disease': ['I25'],\n",
    "    'chronic_kidney_disease': ['N18'],\n",
    "    'diabetes': ['E10', 'E11'],\n",
    "    'connective_tissue_disease': ['M05', 'M06']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQGN5pslUzoo"
   },
   "outputs": [],
   "source": [
    "y_name = 'imv'\n",
    "# these features are chosen because ???\n",
    "colnames = ['age', 'sex', 'race', 'elective_admission',\n",
    "            'sofa_24hours', 'charlson_comorbidity_index', 'heart_rate', 'sbp', 'dbp', 'resp_rate', 'temperature'] + list(conditions.keys())\n",
    "x_names_cat = [\"sex\", \"race\"]\n",
    "sen = [\"sex\", \"race\"]\n",
    "sen_ref = {\"sex\":\"@Male\", \"race\":\"@White\"}\n",
    "\n",
    "\n",
    "output_dir = \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gbmm_6tyxLyr"
   },
   "outputs": [],
   "source": [
    "cat_cols = [\"sex\", \"race\"]\n",
    "\n",
    "for col in cat_cols:\n",
    "    dat_train[col] = dat_train[col].fillna(\"Unknown\").astype(\"category\")\n",
    "    dat_expl[col] = dat_expl[col].fillna(\"Unknown\").astype(\"category\")\n",
    "    dat_test[col] = dat_test[col].fillna(\"Unknown\").astype(\"category\")\n",
    "\n",
    "num_cols = dat_train.select_dtypes(include=[\"Int64\", \"float64\"]).columns\n",
    "\n",
    "for col in num_cols:\n",
    "    if pd.api.types.is_integer_dtype(dat_train[col]):\n",
    "        dat_train[col] = dat_train[col].astype(\"int64\")\n",
    "        dat_expl[col] = dat_expl[col].astype(\"int64\")\n",
    "        dat_test[col] = dat_test[col].astype(\"int64\")\n",
    "    else:\n",
    "        dat_train[col] = dat_train[col].astype(\"float64\")\n",
    "        dat_expl[col] = dat_expl[col].astype(\"float64\")\n",
    "        dat_test[col] = dat_test[col].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSMd3oBGo5ds",
    "outputId": "c591c60b-da79-4295-bf3b-aefa1ae5c3b3"
   },
   "outputs": [],
   "source": [
    "for col in x_names_cat:\n",
    "    print(col, dat_train[col].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CXf8nEskZug"
   },
   "source": [
    "# Step 1: Nearly-optimal model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWnnWo8ncXAf",
    "outputId": "65040ffc-d66b-40db-fd94-5370b9e76c46"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ShapleyVIC.model as model\n",
    "\n",
    "# Force Python to reload the modified file\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvxxvWW3kofj",
    "outputId": "2aa3778f-faab-4380-d6c7-553b8dcbc3dd"
   },
   "outputs": [],
   "source": [
    "faim_obj = FAIMGenerator(\n",
    "  dat_train,\n",
    "  selected_vars=colnames,\n",
    "  selected_vars_cat=x_names_cat,\n",
    "  y_name=\"imv\",\n",
    "  sen_name=sen,\n",
    "  sen_var_ref = sen_ref,\n",
    "  criterion=\"auc\", m=800, n_final=200, output_dir=output_dir, without_sen=\"auto\", pre=False, class_weight=\"balanced\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DU7Fykatk5L-"
   },
   "source": [
    "# Step 2: Fairness transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416,
     "referenced_widgets": [
      "129915dc9e9445978ffddef67e8ca210",
      "363a8f6cede74aa9919679e00252fc2c",
      "35030533e3244b7e9f570c7f3a48c5bc",
      "7a32ae6b05e54aaf8035881e62938cac",
      "0718e66025e0482ba2cca155a49aa0f0",
      "b4f775ff26e443088487ecdc1e1ae064",
      "33dbadef18904527b89dee18b8a85b06",
      "c92bd89ac3574650a66087abfb714ada",
      "bb83ceca1bad40328ab48069f077ade1",
      "67f43962e6034d3a8e89ce7b39bbd367",
      "6708e38cbbd24b51be83fb64b94dd977"
     ]
    },
    "id": "zIeCz7cvk_LH",
    "outputId": "61d3b90c-cf51-4e22-edb9-2c9a786573ce"
   },
   "outputs": [],
   "source": [
    "# takes around <20 minutes to run\n",
    "faim_obj.FAIM_model(dat_expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "bC_MFMIxlKbu",
    "outputId": "54acfa13-36c5-4e82-97c4-cc23e58ec745"
   },
   "outputs": [],
   "source": [
    "best_results, fair_idx_df = faim_obj.transmit(\n",
    "  targeted_metrics = [\"Equalized Odds\", \"Equal Opportunity\", \"BER Equality\"]\n",
    "  )\n",
    "print(best_results['best_sen_exclusion'])\n",
    "print(best_results['best_coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcDTnkcYlx67"
   },
   "outputs": [],
   "source": [
    "pred_test, fairmetrics_faim, fairsummary_faim = faim_obj.test(dat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LA5FtySGlx68",
    "outputId": "46598905-91a2-476c-92bf-b7c07b13853b"
   },
   "outputs": [],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "X-2Q-aAYlx68",
    "outputId": "bd9540ff-22d6-4604-cd90-900f0df5ab0e"
   },
   "outputs": [],
   "source": [
    "fairmetrics_faim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "UXKJ3oRElx69",
    "outputId": "1958e986-4858-4ab6-c0c5-b2910dd5acd0"
   },
   "outputs": [],
   "source": [
    "fairsummary_faim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcG8xktKl43Z"
   },
   "source": [
    "# Step 3: Shap-based model explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "78e55d7f59cd456985d7a09e5b6cd316",
      "1beecdfb5906478ba6f5d4b592560198",
      "fbf6e873c32242128b94902f23a97e18",
      "6e66ffc99c0845e1a7cc953b9116ef43",
      "2e0146833df64bcda11d0b8a16d414c2",
      "3371f0095b1d43eb9dc443121baa6771",
      "f0edb2d3856e4a929e5186582c176e0a",
      "42e2821145c84d89ba187d529773b158",
      "a38f35ea243a4eb79996e790440d0647",
      "5d137eb953f043339be6dfa5497767ec",
      "32dbc58f068b4541b38708a55882e8af",
      "cba4f068b2d2405db0a38f4213a8c41f",
      "0bbe4512a20f4d8294548dbb6f26c773",
      "13dd9b104a484dcb8788ef98d71a8a84",
      "bd7a7712c98b4211b4ac8309fb246464",
      "323af22020fb4724a480a52b161f26a2",
      "8a6dca4d0ca8480c833ebf6c5890f5b2",
      "d89fa87981ad4457ab213b076033217a",
      "c33be7e29fbf4627b95a6ef94518918b",
      "0442825e048747e59d4e834cc8021aec",
      "030200bd0ff34d88a67f2d058fdee18f",
      "ed53ffe3450b44dba2d8b353a92b59d7"
     ]
    },
    "id": "V6hlYesul43a",
    "outputId": "0a8c01a3-fe87-4771-8781-4724b09086d7"
   },
   "outputs": [],
   "source": [
    "shap_compare = faim_obj.compare_explain(overide=True, top_n=20)\n",
    "shap_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_E4lrHwl44b"
   },
   "source": [
    "# Comparison with other bias-mitigation methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcEfME8el44h"
   },
   "outputs": [],
   "source": [
    "fairbase = FairBase(\n",
    "  dat_train,\n",
    "  selected_vars=colnames,\n",
    "  selected_vars_cat=x_names_cat,\n",
    "  y_name=\"imv\",\n",
    "  sen_name=sen,\n",
    "  sen_var_ref=sen_ref,\n",
    "  weighted=True,\n",
    "  weights={\"tnr\": 0.5, \"tpr\": 0.5},\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFFVKK7wl44i"
   },
   "source": [
    "### Original LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "Z4WeVTWsl44j",
    "outputId": "f6e4d0f8-faf2-4973-97a3-2263192b3df6"
   },
   "outputs": [],
   "source": [
    "lr_results = fairbase.model(method=\"OriginalLR\")\n",
    "dat_test = dat_test.reset_index(drop=True)\n",
    "pred_ori, fairmetrics_ori, clametrics_ori = fairbase.test(dat_test, model=lr_results)\n",
    "fairmetrics_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "180ek4Yp6C2c"
   },
   "source": [
    "###Underblindness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3UwuFrV6Cb6"
   },
   "outputs": [],
   "source": [
    "m_unaw = fairbase.model(method=\"Unawareness\")\n",
    "pred_unaw, fair_unaw, cla_unaw = fairbase.test(dat_test, model=m_unaw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "dcd1b5e36b1245a2a4d643023db163eb",
      "5d6605595d3145d9a1b8a53f176303b3",
      "202e024f76cc48a1be6d24fc780e9746",
      "cb1784b1baa7455781e72fcdf3eeaead",
      "43528b3590da4a3a816c4ee912b42a08",
      "629faa10d48e49aa8dea0b765627734a",
      "e6ed1313a00f4f0f9dc32a1879af0a05",
      "db6fae4419774aa49a45f7a0c81e126d",
      "e26eff1531fd4dab8c45b9e023f444e8",
      "73723188e04442a8ab7956e0d4b986e1",
      "27bc26a99fde4274b2cbcaecc96e038d"
     ]
    },
    "id": "CaLdUE27NkvV",
    "outputId": "6efe73b9-bc09-4f6e-a98f-fe358355020e"
   },
   "outputs": [],
   "source": [
    "# Prepare Data for SHAP\n",
    "\n",
    "# Prepare the data (exclude sensitive variables for unawareness)\n",
    "X_test_for_shap = dat_test[colnames].copy()\n",
    "\n",
    "# Remove sensitive variables\n",
    "sen_vars_to_remove = sen  # ['sex', 'race']\n",
    "X_test_features = X_test_for_shap.drop(columns=sen_vars_to_remove, errors='ignore')\n",
    "\n",
    "# Get the training data (also without sensitive variables)\n",
    "X_train_for_shap = dat_train[colnames].copy()\n",
    "X_train_features = X_train_for_shap.drop(columns=sen_vars_to_remove, errors='ignore')\n",
    "\n",
    "# Handle categorical variables - convert to numeric for SHAP\n",
    "X_train_encoded = pd.get_dummies(X_train_features, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test_features, drop_first=True)\n",
    "\n",
    "# Ensure test set has same columns as training set\n",
    "missing_cols = set(X_train_encoded.columns) - set(X_test_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    X_test_encoded[col] = 0\n",
    "X_test_encoded = X_test_encoded[X_train_encoded.columns]\n",
    "\n",
    "print(f\"Training data shape: {X_train_encoded.shape}\")\n",
    "print(f\"Test data shape: {X_test_encoded.shape}\")\n",
    "print(f\"Model type: {type(m_unaw)}\")\n",
    "print(f\"Model expects {len(m_unaw.params)} parameters\")\n",
    "\n",
    "# ==========================================\n",
    "# Create SHAP Explainer (FIXED)\n",
    "# ==========================================\n",
    "\n",
    "def predict_fn(X):\n",
    "    \"\"\"\n",
    "    Fully robust prediction wrapper for statsmodels GLM + SHAP KernelExplainer\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_arr = X.values\n",
    "    else:\n",
    "        X_arr = np.asarray(X)\n",
    "\n",
    "    # FORCE 2D\n",
    "    if X_arr.ndim == 1:\n",
    "        X_arr = X_arr.reshape(1, -1)\n",
    "\n",
    "    # Rebuild DataFrame with correct columns (WITHOUT constant)\n",
    "    X_df = pd.DataFrame(X_arr, columns=X_train_encoded.columns)\n",
    "\n",
    "    # Add constant\n",
    "    X_df = sm.add_constant(X_df, has_constant=\"add\")\n",
    "\n",
    "    # ENSURE correct column order (CRITICAL!)\n",
    "    X_df = X_df[m_unaw.model.exog_names]\n",
    "\n",
    "    # FORCE numeric dtype\n",
    "    X_df = X_df.astype(float)\n",
    "\n",
    "    # Predict probabilities\n",
    "    return m_unaw.predict(X_df)\n",
    "\n",
    "# Use KernelExplainer for statsmodels\n",
    "print(\"\\nCalculating SHAP values (this may take a few minutes)...\")\n",
    "background = shap.sample(X_train_encoded, 50, random_state=42)\n",
    "explainer = shap.KernelExplainer(predict_fn, background)\n",
    "\n",
    "# Calculate SHAP values for a subset of test data\n",
    "n_samples = min(200, len(X_test_encoded))\n",
    "print(f\"Using {n_samples} test samples...\")\n",
    "shap_values = explainer.shap_values(X_test_encoded[:n_samples], nsamples=100, random_state=42)\n",
    "print(\" SHAP values calculated successfully!\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# Aggregate SHAP Values (mean absolute)\n",
    "# ==========================================\n",
    "\n",
    "# Take mean absolute SHAP value across all samples for each feature\n",
    "shap_agg = np.mean(np.abs(shap_values), axis=0)\n",
    "\n",
    "# Create DataFrame with features and their importance\n",
    "feature_names = X_test_encoded.columns\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap': shap_agg\n",
    "}).sort_values('shap', ascending=False)\n",
    "\n",
    "# ==========================================\n",
    "# Clean Feature Labels\n",
    "# ==========================================\n",
    "\n",
    "def clean_label(lbl, max_len=40):\n",
    "    \"\"\"Clean feature names for better visualization\"\"\"\n",
    "    lbl = lbl.replace(\"_\", \" \")\n",
    "    if len(lbl) > max_len:\n",
    "        return lbl[:max_len] + \"...\"\n",
    "    return lbl\n",
    "\n",
    "shap_df['feature'] = shap_df['feature'].apply(clean_label)\n",
    "\n",
    "# ==========================================\n",
    "# Create Bar Plot\n",
    "# ==========================================\n",
    "\n",
    "# Optional: Select top N features (set to None to show all)\n",
    "top_n = 15  # Change this or set to None to show all features\n",
    "\n",
    "def clean_spines(ax):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(True)\n",
    "    ax.spines[\"bottom\"].set_visible(True)\n",
    "\n",
    "def add_bar_labels(ax, values, fmt=\"{:.4f}\", padding=3):\n",
    "    \"\"\"Add numeric labels to horizontal bar plots.\"\"\"\n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(\n",
    "            v,\n",
    "            i,\n",
    "            fmt.format(v),\n",
    "            va=\"center\",\n",
    "            ha=\"left\",\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "if top_n is not None:\n",
    "    shap_df_plot = shap_df.head(top_n)\n",
    "else:\n",
    "    shap_df_plot = shap_df\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.barh(\n",
    "    shap_df_plot['feature'],\n",
    "    shap_df_plot['shap'],\n",
    "    color=\"#FFC20A\"\n",
    ")\n",
    "\n",
    "ax.invert_yaxis()  # Highest importance at top\n",
    "ax.set_title(\n",
    "    \"Unawareness Model - Feature Importance (SHAP)\",\n",
    "    fontsize=16,\n",
    "    weight=\"bold\",\n",
    "    pad=20\n",
    ")\n",
    "ax.set_xlabel(\"Mean |SHAP value|\", fontsize=12)\n",
    "ax.tick_params(axis='y', labelsize=11)\n",
    "add_bar_labels(ax, shap_df_plot['shap'].values)\n",
    "clean_spines(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"{output_dir}/shap_unawareness.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n Plot saved to: {output_dir}/shap_unawareness.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# Display Feature Importance Table\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nTop Feature Importances (Unawareness Model):\")\n",
    "print(\"=\" * 60)\n",
    "print(shap_df.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqpzW7lDl44p"
   },
   "source": [
    "### Reweigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "DoBQSt-pl44q",
    "outputId": "2bcda803-c3ef-415c-ed94-b6d1eb2c813f"
   },
   "outputs": [],
   "source": [
    "_, rw_results, _ = fairbase.model(method_type=\"pre\", method=\"Reweigh\", label_names=[\"imv\"])\n",
    "pred_rw, fairmetrics_rw, clametrics_rw = fairbase.test(dat_test, model=rw_results)\n",
    "fairmetrics_rw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8CEptT46Lm2"
   },
   "source": [
    "###In-processing: Reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBFMoO5k6P5k"
   },
   "outputs": [],
   "source": [
    "m_red = fairbase.model(method_type=\"in\", method=\"Reductions\")\n",
    "pred_red, fair_red, cla_red = fairbase.test(dat_test, model=m_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBc6ixgr6QZ9"
   },
   "source": [
    "###Post-processing: Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-eUTQoB6XPZ"
   },
   "outputs": [],
   "source": [
    "m_eq = fairbase.model(method_type=\"post\", method=\"EqOdds\", dat_expl=dat_expl)\n",
    "pred_eq, fair_eq, cla_eq = fairbase.test(dat_test, model=m_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC9G37jt6ghs"
   },
   "source": [
    "###Post-processing: Calibrated Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruQP-Xgp6hg7"
   },
   "outputs": [],
   "source": [
    "m_cal = fairbase.model(\n",
    "    method_type=\"post\",\n",
    "    method=\"CalEqOdds\",\n",
    "    dat_expl=dat_expl,\n",
    "    cost_constraint=\"weighted\",\n",
    ")\n",
    "pred_cal, fair_cal, cla_cal = fairbase.test(dat_test, model=m_cal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PbP9UEK6l6N"
   },
   "source": [
    "###Post-processing: Reject Option Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcFEqLvf6mbp"
   },
   "outputs": [],
   "source": [
    "m_roc = fairbase.model(\n",
    "    method_type=\"post\",\n",
    "    method=\"ROC\",\n",
    "    dat_expl=dat_expl,\n",
    "    metric_name=\"Equal opportunity difference\",\n",
    "    ub=0.05,\n",
    "    lb=-0.05,\n",
    ")\n",
    "pred_roc, fair_roc, cla_roc = fairbase.test(dat_test, model=m_roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4N8bMwSl44r"
   },
   "source": [
    "### Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13mzZOLG6uCA"
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Original\": (pred_ori, fairmetrics_ori, clametrics_ori),\n",
    "    \"Unawareness\": (pred_unaw, fair_unaw, cla_unaw),\n",
    "    \"Reweighing\": (pred_rw, fairmetrics_rw, clametrics_rw),\n",
    "    \"Reductions\": (pred_red, fair_red, cla_red),\n",
    "    \"EqOdds\": (pred_eq, fair_eq, cla_eq),\n",
    "    \"CalEqOdds\": (pred_cal, fair_cal, cla_cal),\n",
    "    \"ROC\": (pred_roc, fair_roc, cla_roc),\n",
    "    \"FAIM\": (pred_test, fairmetrics_faim, fairsummary_faim),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "5n5FPDn02aZW",
    "outputId": "bcbf60fe-61d5-4bd5-c901-19752e15c961"
   },
   "outputs": [],
   "source": [
    "# Read the adversarial results CSV\n",
    "adv_results = pd.read_csv(\"test_metrics_adv_imv.csv\")\n",
    "\n",
    "# Extract the fairness metrics you want (let's use the best performing row)\n",
    "# You can choose based on which metric is most important to you\n",
    "# For example, let's use the row with lowest equalized_odds_diff\n",
    "best_adv_idx = adv_results['equalized_odds_diff'].abs().idxmin()\n",
    "adv_metrics = adv_results.loc[best_adv_idx]\n",
    "\n",
    "# Create a row for the adversarial method\n",
    "# Match the column names to your fairmetrics format\n",
    "# You'll need to map the adversarial metric names to your fairness metric names\n",
    "adv_row = pd.Series({\n",
    "    'Equal Opportunity': adv_metrics['equal_opportunity_diff'],\n",
    "    'Equalized Odds': adv_metrics['equalized_odds_diff'],\n",
    "    'BER Equality': adv_metrics['ber_equality_diff'],\n",
    "    'Statistical Parity': adv_metrics['statistical_parity_diff'],\n",
    "    'Accuracy Equality': adv_metrics['accuracy_equality_diff'],\n",
    "})\n",
    "\n",
    "# Add to your comparison dataframe\n",
    "faircompare_df = pd.concat([\n",
    "    fairmetrics_ori,\n",
    "    fair_unaw,\n",
    "    fairmetrics_rw,\n",
    "    fair_red,\n",
    "    fair_eq,\n",
    "    fair_cal,\n",
    "    fair_roc,\n",
    "    fairmetrics_faim,\n",
    "    adv_row.to_frame().T  # Convert series to dataframe row\n",
    "])\n",
    "\n",
    "faircompare_df.index = [\n",
    "    \"Original\",\n",
    "    \"Unawareness\",\n",
    "    \"Reweighing\",\n",
    "    \"Reductions\",\n",
    "    \"EqOdds\",\n",
    "    \"CalEqOdds\",\n",
    "    \"ROC\",\n",
    "    \"FAIM\",\n",
    "    \"Adnet\"\n",
    "]\n",
    "\n",
    "faircompare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lggLxk34tE78"
   },
   "source": [
    "Equal Opportunity: max TPR gap between any two sensitive groups (within a sensitive attribute)\\\n",
    "Equalized Odds: worst-case fairness gap\\\n",
    "Statistical Parity: max SR gap\\\n",
    "Accuracy Equality: max accuracy diff \\\n",
    "BER Equality: weighed combination of worst TPR/TNR gaps\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "Pv1cwSbpxn42",
    "outputId": "dac01353-4440-40cc-bb48-14a99786e6e2"
   },
   "outputs": [],
   "source": [
    "adv_row = pd.Series({\n",
    "    'auc': adv_metrics['auc'],\n",
    "    'auc_low': adv_metrics['auc_low'],\n",
    "    'auc_high': adv_metrics['auc_high'],\n",
    "    'sensitivity': adv_metrics['recall'],\n",
    "    'specificity': adv_metrics['specificity'],\n",
    "})\n",
    "\n",
    "# Extract fairmetrics from each result\n",
    "faircompare_df = pd.concat([\n",
    "    clametrics_ori,\n",
    "    cla_unaw,\n",
    "    clametrics_rw,\n",
    "    cla_red,\n",
    "    cla_eq,\n",
    "    cla_cal,\n",
    "    cla_roc,\n",
    "    fairsummary_faim,\n",
    "    adv_row.to_frame().T\n",
    "])\n",
    "\n",
    "# Set descriptive index names\n",
    "faircompare_df.index = [\n",
    "    \"Original\",\n",
    "    \"Unawareness\",\n",
    "    \"Reweighing\",\n",
    "    \"Reductions\",\n",
    "    \"EqOdds\",\n",
    "    \"CalEqOdds\",\n",
    "    \"ROC\",\n",
    "    \"FAIM\",\n",
    "    \"Adnet\"\n",
    "]\n",
    "\n",
    "faircompare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPoe1fw2MHGE"
   },
   "source": [
    "Adnet has highest accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT0nu5dVHt3F"
   },
   "source": [
    "# Bias Analysis (Baseline) on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y1TuBgsP73b"
   },
   "outputs": [],
   "source": [
    "y_true = dat_test[\"imv\"]\n",
    "\n",
    "y_pred_prob_baseline = faim_obj.optim_model.predict(\n",
    "    params=faim_obj.optim_results.params,\n",
    "    exog=faim_obj.data_process(dat_test, faim_obj.vars, faim_obj.vars_cat)[0]\n",
    ")\n",
    "\n",
    "# USE OPTIMAL THRESHOLD (not 0.5!)\n",
    "threshold_baseline = find_optimal_cutoff(y_true, y_pred_prob_baseline, method=\"auc\")[0]\n",
    "y_pred_bin_baseline = (y_pred_prob_baseline > threshold_baseline).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CFtlLN7DUjo"
   },
   "outputs": [],
   "source": [
    "# Reset indices\n",
    "y_true_reset = y_true.reset_index(drop=True)\n",
    "y_pred_reset = pd.Series(y_pred_prob_baseline).reset_index(drop=True)\n",
    "y_pred_bin_reset = pd.Series(y_pred_bin_baseline).reset_index(drop=True)\n",
    "\n",
    "# Sex\n",
    "fair_sex_baseline = FAIMEvaluator(\n",
    "    y_true=y_true_reset,\n",
    "    y_pred=y_pred_reset,\n",
    "    y_pred_bin=y_pred_bin_reset,\n",
    "    sen_var=dat_test[\"sex\"],  #  Reset here\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n",
    "\n",
    "# Race\n",
    "fair_race_baseline = FAIMEvaluator(\n",
    "    y_true=y_true_reset,\n",
    "    y_pred=y_pred_reset,\n",
    "    y_pred_bin=y_pred_bin_reset,\n",
    "    sen_var=dat_test[\"race\"],  #  Reset here\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n",
    "\n",
    "# Intersectional\n",
    "dat_test_reset = dat_test.reset_index(drop=True)\n",
    "dat_test_reset[\"Sex_Race\"] = dat_test_reset[\"sex\"].astype(str) + \"_\" + dat_test_reset[\"race\"].astype(str)\n",
    "#dat_test[\"Sex_Race\"] = dat_test[\"sex\"].astype(str) + \"_\" + dat_test[\"race\"].astype(str)\n",
    "\n",
    "fair_sex_race_baseline = FAIMEvaluator(\n",
    "    y_true=y_true_reset,\n",
    "    y_pred=y_pred_reset,\n",
    "    y_pred_bin=y_pred_bin_reset,\n",
    "    sen_var=dat_test_reset[\"Sex_Race\"],  #  Already reset\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "yL8OyLePH7-m",
    "outputId": "ee57a8a2-623e-45d5-e0c2-9e79da074391"
   },
   "outputs": [],
   "source": [
    "fair_sex_baseline.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "bOqqC0PcIAEh",
    "outputId": "3910acd9-03d0-438c-c515-daa491ac7007"
   },
   "outputs": [],
   "source": [
    "fair_race_baseline.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "qTqLogwTH7Qo",
    "outputId": "d1faa28e-f666-4dc6-b52c-386dbcd4d0ac"
   },
   "outputs": [],
   "source": [
    "fair_sex_race_baseline.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "EvtC470UCRUq",
    "outputId": "7346239f-9047-4711-fe06-325b79d4d3c0"
   },
   "outputs": [],
   "source": [
    "fair_sex_race_baseline.disparity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "xgu0CHCeCb8t",
    "outputId": "2308a1ce-0ed6-4e38-e9e2-e4d2c9e11f0e"
   },
   "outputs": [],
   "source": [
    "fair_race_baseline.disparity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "lkUpkUF5E3OV",
    "outputId": "3781738f-472b-466a-f162-52d7804fee21"
   },
   "outputs": [],
   "source": [
    "fair_sex_baseline.disparity_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF6zeKdQ1GVp"
   },
   "source": [
    "#Bias Analysis (FAIM) on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Us1XjDUoTwRJ"
   },
   "outputs": [],
   "source": [
    "excluded_vars = faim_obj.best_sen_exclusion.split(\"_\")\n",
    "selected_vars = [i for i in faim_obj.vars if i not in excluded_vars]\n",
    "selected_vars_cat = [i for i in faim_obj.vars_cat if i not in excluded_vars]\n",
    "\n",
    "y_pred_prob_faim = faim_obj.best_optim_base_obj.model_optim.model.predict(\n",
    "    params=faim_obj.best_coef,\n",
    "    exog=faim_obj.data_process(dat_test, selected_vars, selected_vars_cat)[0]\n",
    ")\n",
    "\n",
    "# USE OPTIMAL THRESHOLD\n",
    "threshold_faim = find_optimal_cutoff(y_true, y_pred_prob_faim, method=\"auc\")[0]\n",
    "y_pred_bin_faim = (y_pred_prob_faim > threshold_faim).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "femMmKil7FY9"
   },
   "outputs": [],
   "source": [
    "# On test set only\n",
    "dat_test_reset = dat_test.reset_index(drop=True)\n",
    "\n",
    "# Reset indices\n",
    "y_true_reset = dat_test_reset[\"imv\"]                # target\n",
    "y_pred_reset = pd.Series(y_pred_prob_faim).reset_index(drop=True)\n",
    "y_pred_bin_reset = pd.Series(y_pred_bin_faim).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Sensitive variables from the same reset df\n",
    "sen_sex = dat_test_reset[\"sex\"]\n",
    "sen_race = dat_test_reset[\"race\"]\n",
    "dat_test_reset[\"Sex_Race\"] = sen_sex.astype(str) + \"_\" + sen_race.astype(str)\n",
    "\n",
    "fair_sex_faim = FAIMEvaluator(\n",
    "    y_true=y_true_reset,\n",
    "    y_pred=y_pred_reset,\n",
    "    y_pred_bin=y_pred_bin_reset,\n",
    "    sen_var=sen_sex,\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n",
    "\n",
    "fair_race_faim = FAIMEvaluator(\n",
    "    y_true=y_true_reset,\n",
    "    y_pred=y_pred_reset,\n",
    "    y_pred_bin=y_pred_bin_reset,\n",
    "    sen_var=sen_race,\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n",
    "\n",
    "fair_sex_race_faim = FAIMEvaluator(\n",
    "    y_true=y_true_reset,\n",
    "    y_pred=y_pred_reset,\n",
    "    y_pred_bin=y_pred_bin_reset,\n",
    "    sen_var=dat_test_reset[\"Sex_Race\"],\n",
    "    weighted=True,\n",
    "    weights={\"tpr\": 0.5, \"tnr\": 0.5}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ACxYhXjNkzVd",
    "outputId": "f3dcfebe-6081-41ce-ce14-671c097375e9"
   },
   "outputs": [],
   "source": [
    "fair_sex_race_faim.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0kor5N_y8h7",
    "outputId": "22bc2e3b-d664-486a-89f8-a478f7ec7439"
   },
   "outputs": [],
   "source": [
    "fair_race_faim.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BFmpFCgWzBig",
    "outputId": "8773886d-9cad-4054-f154-1ca50c602ca0"
   },
   "outputs": [],
   "source": [
    "fair_sex_faim.fairsummary.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4PNOWZ6DDgB",
    "outputId": "48c30e14-2b93-4fde-91e7-2a5df146aa11"
   },
   "outputs": [],
   "source": [
    "fair_sex_race_faim.disparity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CggscOo6DPk-",
    "outputId": "de34cb00-0b15-42ce-c068-811388d6899f"
   },
   "outputs": [],
   "source": [
    "fair_race_faim.disparity_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGnDEfUuEi1E",
    "outputId": "18e58197-b7c7-489c-f35a-d205cb514b29"
   },
   "outputs": [],
   "source": [
    "fair_sex_faim.disparity_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM8bllodDb_l"
   },
   "source": [
    "#Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAb5FxSzbluC",
    "outputId": "455461df-068f-499c-f36e-acfd9e21ed95"
   },
   "outputs": [],
   "source": [
    "# ===== DEFINE PREDICTIONS AND TRUE LABELS =====\n",
    "# Get FAIM's fairness-aware model predictions\n",
    "excluded_vars = faim_obj.best_sen_exclusion.split(\"_\")\n",
    "selected_vars = [i for i in faim_obj.vars if i not in excluded_vars]\n",
    "selected_vars_cat = [i for i in faim_obj.vars_cat if i not in excluded_vars]\n",
    "\n",
    "# Get predictions\n",
    "X_test_faim, _, _ = faim_obj.data_process(dat_test, selected_vars, selected_vars_cat)\n",
    "y_pred_prob_faim = faim_obj.best_optim_base_obj.model_optim.model.predict(\n",
    "    params=faim_obj.best_coef,\n",
    "    exog=X_test_faim\n",
    ")\n",
    "\n",
    "# Use optimal threshold\n",
    "optimal_threshold = find_optimal_cutoff(dat_test[\"imv\"], y_pred_prob_faim, method=\"auc\")[0]\n",
    "y_pred_bin_test = (y_pred_prob_faim > optimal_threshold).astype(int)\n",
    "\n",
    "# TRUE LABELS (this is what you're missing!)\n",
    "y_true_test = dat_test[\"imv\"].values\n",
    "\n",
    "print(f\" Optimal threshold: {optimal_threshold:.4f}\")\n",
    "print(f\" y_true_test shape: {y_true_test.shape}\")\n",
    "print(f\" y_pred_bin_test shape: {y_pred_bin_test.shape}\")\n",
    "\n",
    "# ===== CREATE SUBGROUP COLUMN =====\n",
    "dat_test[\"Sex_Race\"] = dat_test[\"sex\"].astype(str) + \"_\" + dat_test[\"race\"].astype(str)\n",
    "\n",
    "# ===== COMPUTE TPR WITH CI =====\n",
    "def compute_tpr_ci(y_true, y_pred_bin, subgroup_mask):\n",
    "    \"\"\"Compute 95% CI for TPR in a subgroup\"\"\"\n",
    "    y_true_sub = y_true[subgroup_mask.values]\n",
    "    y_pred_sub = y_pred_bin[subgroup_mask.values]\n",
    "\n",
    "    # TPR = TP / (TP + FN)\n",
    "    positives = y_true_sub == 1\n",
    "    n_pos = positives.sum()\n",
    "\n",
    "    if n_pos == 0:\n",
    "        return None, None, None, 0\n",
    "\n",
    "    tp = ((y_true_sub == 1) & (y_pred_sub == 1)).sum()\n",
    "    tpr = tp / n_pos\n",
    "\n",
    "    # Wilson score interval for binomial proportion\n",
    "    if n_pos < 5:  # Too small for reliable CI\n",
    "        return tpr, np.nan, np.nan, n_pos\n",
    "\n",
    "    ci = stats.binom.interval(0.95, n_pos, tpr)\n",
    "    ci_lower, ci_upper = ci[0] / n_pos, ci[1] / n_pos\n",
    "\n",
    "    return tpr, ci_lower, ci_upper, n_pos\n",
    "\n",
    "# Get ALL unique subgroups from test set\n",
    "all_subgroups = dat_test['Sex_Race'].unique()\n",
    "\n",
    "print(f\"\\nFound {len(all_subgroups)} unique subgroups in test set:\")\n",
    "print(sorted(all_subgroups))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test Set TPR with 95% Confidence Intervals (FAIM Model):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for subgroup in sorted(all_subgroups):\n",
    "    mask = dat_test['Sex_Race'] == subgroup\n",
    "    n_subgroup = mask.sum()\n",
    "\n",
    "    tpr, ci_low, ci_high, n_pos = compute_tpr_ci(\n",
    "        y_true_test,\n",
    "        y_pred_bin_test,\n",
    "        mask\n",
    "    )\n",
    "\n",
    "    if tpr is not None:\n",
    "        results.append({\n",
    "            'Subgroup': subgroup,\n",
    "            'N_Total': n_subgroup,\n",
    "            'N_Positive': n_pos,\n",
    "            'TPR': tpr,\n",
    "            'CI_Lower': ci_low,\n",
    "            'CI_Upper': ci_high,\n",
    "            'CI_Width': ci_high - ci_low if not np.isnan(ci_high) else np.nan\n",
    "        })\n",
    "\n",
    "        if not np.isnan(ci_low):\n",
    "            print(f\"{subgroup:25s} (n={n_subgroup:4d}, pos={n_pos:3d}): TPR={tpr:.3f} [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "        else:\n",
    "            print(f\"{subgroup:25s} (n={n_subgroup:4d}, pos={n_pos:3d}): TPR={tpr:.3f} [insufficient sample]\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('TPR', ascending=False)\n",
    "\n",
    "# ... rest of your analysis code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "f4sM5tFeer4E",
    "outputId": "8749aefb-887b-42ff-81ba-44d6962201bb"
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "results_df_sorted = results_df.sort_values('TPR', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))  # Made wider to accommodate legend\n",
    "\n",
    "# Color code by sample size\n",
    "colors = ['#E74C3C' if n < 300 else '#2E5F9E' for n in results_df_sorted['N_Total']]\n",
    "\n",
    "# Plot with error bars\n",
    "y_pos = np.arange(len(results_df_sorted))\n",
    "ax.barh(y_pos, results_df_sorted['TPR'], color=colors, alpha=0.6)\n",
    "\n",
    "# Add confidence intervals\n",
    "for i, row in enumerate(results_df_sorted.itertuples()):\n",
    "    ax.plot([row.CI_Lower, row.CI_Upper], [i, i], 'k-', linewidth=2)\n",
    "    ax.plot([row.CI_Lower, row.CI_Lower], [i-0.2, i+0.2], 'k-', linewidth=2)\n",
    "    ax.plot([row.CI_Upper, row.CI_Upper], [i-0.2, i+0.2], 'k-', linewidth=2)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(results_df_sorted['Subgroup'])\n",
    "ax.set_xlabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
    "ax.set_title('Model Performance by Demographic Subgroup with 95% Confidence Intervals', fontsize=14, weight='bold')\n",
    "ax.axvline(x=0.75, color='gray', linestyle='--', alpha=0.5, label='Target threshold')\n",
    "\n",
    "# Legend - placed outside plot area on the right\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2E5F9E', alpha=0.6, label='Large subgroup (n300)'),\n",
    "    Patch(facecolor='#E74C3C', alpha=0.6, label='Small subgroup (n<300, unreliable)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('subgroup_performance_with_ci.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Figure saved as 'subgroup_performance_with_ci.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nLyJgNdmVse",
    "outputId": "a3e1a7ce-1ff7-437b-cda6-034afd346a53"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"INTERSECTIONAL FAIRNESS: BASELINE vs FAIM COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== GET ALL INTERSECTIONAL TPRs =====\n",
    "def get_all_group_tprs(y_true_arr, y_pred_bin_arr, sensitive_var):\n",
    "    \"\"\"Get TPR for every group in a sensitive variable\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    groups = dat_test_copy[sensitive_var].unique()\n",
    "    results = []\n",
    "\n",
    "    for group in groups:\n",
    "        mask = (dat_test_copy[sensitive_var] == group).values\n",
    "        y_true_group = y_true_arr[mask]\n",
    "        y_pred_group = y_pred_bin_arr[mask]\n",
    "\n",
    "        if len(y_true_group) == 0 or y_true_group.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_group, y_pred_group, labels=[0,1]).ravel()\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "\n",
    "        if not np.isnan(tpr):\n",
    "            results.append({\n",
    "                'Intersection': group,\n",
    "                'TPR': tpr,\n",
    "                'N_Total': len(y_true_group),\n",
    "                'N_Positive': int(y_true_group.sum())\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Get TPRs for baseline and FAIM\n",
    "baseline_tprs = get_all_group_tprs(y_true_arr, y_pred_bin_baseline_arr, 'Sex_Race')\n",
    "faim_tprs = get_all_group_tprs(y_true_arr, y_pred_bin_faim_arr, 'Sex_Race')\n",
    "\n",
    "# Merge\n",
    "comparison = baseline_tprs.merge(\n",
    "    faim_tprs,\n",
    "    on='Intersection',\n",
    "    suffixes=('_Baseline', '_FAIM')\n",
    ")\n",
    "\n",
    "comparison['TPR_Change'] = comparison['TPR_FAIM'] - comparison['TPR_Baseline']\n",
    "comparison['TPR_Change_Pct'] = (comparison['TPR_Change'] / comparison['TPR_Baseline'] * 100)\n",
    "\n",
    "# Sort by baseline TPR (worst to best)\n",
    "comparison = comparison.sort_values('TPR_Baseline', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Round for cleaner output\n",
    "comparison_display = comparison.copy()\n",
    "comparison_display['TPR_Baseline'] = comparison_display['TPR_Baseline'].round(4)\n",
    "comparison_display['TPR_FAIM'] = comparison_display['TPR_FAIM'].round(4)\n",
    "comparison_display['TPR_Change'] = comparison_display['TPR_Change'].round(4)\n",
    "comparison_display['TPR_Change_Pct'] = comparison_display['TPR_Change_Pct'].round(1)\n",
    "\n",
    "# ===== CREATE COPY-PASTE FRIENDLY TABLE =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 1: INTERSECTIONAL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTab-delimited (copy-paste into Word/Excel):\\n\")\n",
    "\n",
    "# Tab-delimited version (best for Word/Excel)\n",
    "print(\"Intersection\\tN\\tBaseline TPR\\tFAIM TPR\\tChange\\tChange (%)\")\n",
    "for _, row in comparison_display.iterrows():\n",
    "    print(f\"{row['Intersection']}\\t{row['N_Total_Baseline']}\\t{row['TPR_Baseline']:.4f}\\t{row['TPR_FAIM']:.4f}\\t{row['TPR_Change']:+.4f}\\t{row['TPR_Change_Pct']:+.1f}%\")\n",
    "\n",
    "# ===== FORMATTED TABLE FOR LATEX =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LATEX FORMAT:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "\\\\begin{table}[h]\n",
    "\\\\centering\n",
    "\\\\caption{Intersectional Performance: Baseline vs FAIM}\n",
    "\\\\begin{tabular}{lrrrrc}\n",
    "\\\\hline\n",
    "Intersection & N & Baseline TPR & FAIM TPR & Change & Change (\\\\%) \\\\\\\\\n",
    "\\\\hline\n",
    "\"\"\")\n",
    "\n",
    "for _, row in comparison_display.iterrows():\n",
    "    print(f\"{row['Intersection']} & {row['N_Total_Baseline']} & {row['TPR_Baseline']:.4f} & {row['TPR_FAIM']:.4f} & {row['TPR_Change']:+.4f} & {row['TPR_Change_Pct']:+.1f}\\\\% \\\\\\\\\")\n",
    "\n",
    "print(\"\"\"\\\\hline\n",
    "\\\\end{tabular}\n",
    "\\\\end{table}\n",
    "\"\"\")\n",
    "\n",
    "# ===== MARKDOWN TABLE =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MARKDOWN FORMAT:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n| Intersection | N | Baseline TPR | FAIM TPR | Change | Change (%) |\")\n",
    "print(\"|-------------|---:|-------------:|---------:|-------:|-----------:|\")\n",
    "\n",
    "for _, row in comparison_display.iterrows():\n",
    "    print(f\"| {row['Intersection']} | {row['N_Total_Baseline']} | {row['TPR_Baseline']:.4f} | {row['TPR_FAIM']:.4f} | {row['TPR_Change']:+.4f} | {row['TPR_Change_Pct']:+.1f}% |\")\n",
    "\n",
    "# ===== SUMMARY STATISTICS TABLE =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLE 2: SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate summary stats\n",
    "baseline_min = comparison['TPR_Baseline'].min()\n",
    "baseline_max = comparison['TPR_Baseline'].max()\n",
    "baseline_gap = baseline_max - baseline_min\n",
    "baseline_mean = comparison['TPR_Baseline'].mean()\n",
    "baseline_std = comparison['TPR_Baseline'].std()\n",
    "\n",
    "faim_min = comparison['TPR_FAIM'].min()\n",
    "faim_max = comparison['TPR_FAIM'].max()\n",
    "faim_gap = faim_max - faim_min\n",
    "faim_mean = comparison['TPR_FAIM'].mean()\n",
    "faim_std = comparison['TPR_FAIM'].std()\n",
    "\n",
    "gap_reduction = baseline_gap - faim_gap\n",
    "gap_reduction_pct = (gap_reduction / baseline_gap) * 100\n",
    "\n",
    "worst_baseline = comparison.loc[comparison['TPR_Baseline'].idxmin(), 'Intersection']\n",
    "best_baseline = comparison.loc[comparison['TPR_Baseline'].idxmax(), 'Intersection']\n",
    "worst_faim = comparison.loc[comparison['TPR_FAIM'].idxmin(), 'Intersection']\n",
    "best_faim = comparison.loc[comparison['TPR_FAIM'].idxmax(), 'Intersection']\n",
    "\n",
    "print(\"\\nTab-delimited:\\n\")\n",
    "print(\"Metric\\tBaseline\\tFAIM\\tChange\")\n",
    "print(f\"Minimum TPR\\t{baseline_min:.4f}\\t{faim_min:.4f}\\t{faim_min-baseline_min:+.4f}\")\n",
    "print(f\"Maximum TPR\\t{baseline_max:.4f}\\t{faim_max:.4f}\\t{faim_max-baseline_max:+.4f}\")\n",
    "print(f\"Gap (Max-Min)\\t{baseline_gap:.4f}\\t{faim_gap:.4f}\\t{gap_reduction:+.4f} ({gap_reduction_pct:+.1f}%)\")\n",
    "print(f\"Mean TPR\\t{baseline_mean:.4f}\\t{faim_mean:.4f}\\t{faim_mean-baseline_mean:+.4f}\")\n",
    "print(f\"Std Dev\\t{baseline_std:.4f}\\t{faim_std:.4f}\\t{faim_std-baseline_std:+.4f}\")\n",
    "print(f\"Worst Group\\t{worst_baseline}\\t{worst_faim}\\t\")\n",
    "print(f\"Best Group\\t{best_baseline}\\t{best_faim}\\t\")\n",
    "\n",
    "# ===== SAVE TO CSV =====\n",
    "comparison_display.to_csv('intersectional_comparison.csv', index=False)\n",
    "print(f\"\\n Full table saved to: intersectional_comparison.csv\")\n",
    "\n",
    "# Summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': ['Minimum TPR', 'Maximum TPR', 'Gap (Max-Min)', 'Mean TPR', 'Std Dev', 'Worst Group', 'Best Group'],\n",
    "    'Baseline': [f\"{baseline_min:.4f}\", f\"{baseline_max:.4f}\", f\"{baseline_gap:.4f}\",\n",
    "                 f\"{baseline_mean:.4f}\", f\"{baseline_std:.4f}\", worst_baseline, best_baseline],\n",
    "    'FAIM': [f\"{faim_min:.4f}\", f\"{faim_max:.4f}\", f\"{faim_gap:.4f}\",\n",
    "             f\"{faim_mean:.4f}\", f\"{faim_std:.4f}\", worst_faim, best_faim],\n",
    "    'Change': [f\"{faim_min-baseline_min:+.4f}\", f\"{faim_max-baseline_max:+.4f}\",\n",
    "               f\"{gap_reduction:+.4f} ({gap_reduction_pct:+.1f}%)\",\n",
    "               f\"{faim_mean-baseline_mean:+.4f}\", f\"{faim_std-baseline_std:+.4f}\", \"\", \"\"]\n",
    "})\n",
    "\n",
    "summary_df.to_csv('intersectional_summary.csv', index=False)\n",
    "print(f\" Summary table saved to: intersectional_summary.csv\")\n",
    "\n",
    "# ===== KEY FINDINGS =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_improved = (comparison['TPR_Change'] > 0.001).sum()\n",
    "n_declined = (comparison['TPR_Change'] < -0.001).sum()\n",
    "n_unchanged = len(comparison) - n_improved - n_declined\n",
    "\n",
    "print(f\"\\n Gap reduced from {baseline_gap:.4f} to {faim_gap:.4f} (reduction: {gap_reduction:.4f}, {gap_reduction_pct:.1f}%)\")\n",
    "print(f\"\\n Group changes:\")\n",
    "print(f\"   - Improved: {n_improved}/{len(comparison)} groups\")\n",
    "print(f\"   - Declined: {n_declined}/{len(comparison)} groups\")\n",
    "print(f\"   - Unchanged: {n_unchanged}/{len(comparison)} groups\")\n",
    "\n",
    "if worst_baseline == worst_faim:\n",
    "    print(f\"\\n Same worst-performing group: {worst_baseline}\")\n",
    "    print(f\"   TPR improved: {baseline_min:.4f}  {faim_min:.4f} ({((faim_min/baseline_min - 1)*100):+.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n  Worst-performing group changed:\")\n",
    "    print(f\"   Baseline: {worst_baseline} (TPR={baseline_min:.4f})\")\n",
    "    print(f\"   FAIM: {worst_faim} (TPR={faim_min:.4f})\")\n",
    "\n",
    "if best_baseline == best_faim:\n",
    "    print(f\"\\n Same best-performing group: {best_baseline}\")\n",
    "    print(f\"   TPR changed: {baseline_max:.4f}  {faim_max:.4f} ({((faim_max/baseline_max - 1)*100):+.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n  Best-performing group changed:\")\n",
    "    print(f\"   Baseline: {best_baseline} (TPR={baseline_max:.4f})\")\n",
    "    print(f\"   FAIM: {best_faim} (TPR={faim_max:.4f})\")\n",
    "    print(f\"    Performance was redistributed more equitably\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "HD9VqjEZXeEk",
    "outputId": "725a4589-305a-4138-87b5-8c7b2c401db1"
   },
   "outputs": [],
   "source": [
    "# Create table showing exclusion scenarios and their fairness rankings\n",
    "\n",
    "# Access the fairness metrics dataframe stored in faim_obj\n",
    "# This was created during FAIM_model() execution\n",
    "fairmetrics_df = faim_obj.fairmetrics_df.copy()\n",
    "\n",
    "print(f\"Found fairmetrics_df with shape: {fairmetrics_df.shape}\")\n",
    "print(f\"Columns: {fairmetrics_df.columns.tolist()}\")\n",
    "\n",
    "# Calculate fairness ranking for each model\n",
    "def compute_area(row):\n",
    "    \"\"\"Compute fairness area metric (smaller = more fair)\"\"\"\n",
    "    metrics = ['Equal Opportunity', 'Equalized Odds', 'BER Equality']\n",
    "    values = [row[m] for m in metrics if m in row]\n",
    "\n",
    "    n_metric = len(values)\n",
    "    if n_metric == 0:\n",
    "        return np.inf\n",
    "\n",
    "    tmp = values\n",
    "    tmp_1 = values[1:] + values[:1]\n",
    "\n",
    "    if n_metric > 2:\n",
    "        theta_c = 2 * np.pi / n_metric\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
    "    elif n_metric == 2:\n",
    "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
    "    else:\n",
    "        area = np.abs(tmp[0])\n",
    "\n",
    "    return area\n",
    "\n",
    "# Calculate fairness area for each model\n",
    "fairmetrics_df['fairness_area'] = fairmetrics_df.apply(compute_area, axis=1)\n",
    "\n",
    "# Rank models (lower area = better fairness = lower rank number)\n",
    "fairmetrics_df['ranking'] = fairmetrics_df['fairness_area'].rank(method='min').astype(int)\n",
    "\n",
    "# Create ranking bins\n",
    "def create_exclusion_ranking_table(df):\n",
    "    \"\"\"\n",
    "    Create a table showing how different exclusion scenarios\n",
    "    perform across fairness ranking bins\n",
    "    \"\"\"\n",
    "\n",
    "    # Define ranking bins\n",
    "    bins = [\n",
    "        (1, 10, \"1-10\\n(\\\"most fair\\\")\"),\n",
    "        (11, 700, \"11-700\"),\n",
    "        (701, 800, \"701-800\\n(\\\"least fair\\\")\"),\n",
    "    ]\n",
    "\n",
    "    # Get unique exclusion cases\n",
    "    exclusion_cases = df['sen_var_exclusion'].unique()\n",
    "\n",
    "    # Map cases to readable labels\n",
    "    case_labels = {}\n",
    "    for case in exclusion_cases:\n",
    "        if case == \"\":\n",
    "            case_labels[case] = \"No exclusion\"\n",
    "        elif len(case.split(\"_\")) == 2:\n",
    "            case_labels[case] = f\"Exclusion of\\n{' and '.join(case.split('_'))}\"\n",
    "        elif len(case.split(\"_\")) > 2:\n",
    "            sens = case.split(\"_\")\n",
    "            case_labels[case] = f\"Exclusion of\\n{', '.join(sens[:-1])} and {sens[-1]}\"\n",
    "        else:\n",
    "            case_labels[case] = f\"Exclusion of\\n{case}\"\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    results = []\n",
    "\n",
    "    for case in exclusion_cases:\n",
    "        case_data = df[df['sen_var_exclusion'] == case]\n",
    "\n",
    "        row = {'Exclusion cases': case_labels[case]}\n",
    "\n",
    "        # Count models in each bin\n",
    "        for bin_start, bin_end, bin_label in bins:\n",
    "            count = ((case_data['ranking'] >= bin_start) &\n",
    "                    (case_data['ranking'] <= bin_end)).sum()\n",
    "            row[bin_label] = count\n",
    "\n",
    "        # Find the best (lowest) ranking for this case\n",
    "        best_ranking = case_data['ranking'].min()\n",
    "        row['Highest\\nranking'] = f\"No.{int(best_ranking)}\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Sort by highest ranking (extract number from \"No.X\")\n",
    "    results_df['_sort_key'] = results_df['Highest\\nranking'].str.extract(r'(\\d+)').astype(int)\n",
    "    results_df = results_df.sort_values('_sort_key').drop('_sort_key', axis=1)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Generate table\n",
    "exclusion_table = create_exclusion_ranking_table(fairmetrics_df)\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXCLUSION OF SENSITIVE VARIABLES ON FAIRNESS RANKINGS\")\n",
    "print(\"=\"*80)\n",
    "print(exclusion_table.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgadkYBaDHTK"
   },
   "outputs": [],
   "source": [
    "def safe_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    if cm.shape == (1,1):\n",
    "        if y_true.unique()[0] == 0:\n",
    "            cm = np.array([[cm[0,0],0],[0,0]])\n",
    "        else:\n",
    "            cm = np.array([[0,0],[0,cm[0,0]]])\n",
    "    return cm.ravel()\n",
    "\n",
    "def intersectional_metrics(df, cat_vars, y_true_col, y_pred_col, min_group_size=20, max_intersection=3):\n",
    "    results = []\n",
    "\n",
    "    for r in range(1, max_intersection + 1):\n",
    "        for comb in itertools.combinations(cat_vars, r):\n",
    "            grouped = df.groupby(list(comb))\n",
    "            for vals, g in grouped:\n",
    "                if len(g) < min_group_size:\n",
    "                    continue\n",
    "                y_true = g[y_true_col]\n",
    "                y_pred = g[y_pred_col]\n",
    "                tn, fp, fn, tp = safe_confusion_matrix(y_true, y_pred)\n",
    "                sr = y_pred.mean()\n",
    "                acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "                fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
    "                fnr = fn / (fn + tp) if (fn + tp) > 0 else np.nan\n",
    "                results.append({\n",
    "                    'intersection': \"  \".join(f\"{v}={val}\" for v,val in zip(comb, vals)) if r>1 else f\"{comb[0]}={vals}\",\n",
    "                    'n_samples': len(g),\n",
    "                    'TPR': tp / (tp + fn) if (tp + fn) > 0 else np.nan,\n",
    "                    'FPR': fp / (fp + tn) if (fp + tn) > 0 else np.nan,\n",
    "                    'TNR': tn / (tn + fp) if (tn + fp) > 0 else np.nan,\n",
    "                    'FNR': fn / (fn + tp) if (fn + tp) > 0 else np.nan,\n",
    "                    'SR': sr,\n",
    "                    'Accuracy': acc,\n",
    "                    'BER': 0.5 * (fpr + fnr)\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results).round(4).sort_values('n_samples', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uz5KbkNYeJD8",
    "outputId": "39e1de2b-19d0-49ed-c099-3db39cbd0d32"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FAIRNESS GAP REDUCTION: BASELINE vs FAIM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== GET BASELINE PREDICTIONS =====\n",
    "X_test_baseline, _, y_true = faim_obj.data_process(dat_test)\n",
    "y_pred_prob_baseline = faim_obj.optim_model.predict(\n",
    "    params=faim_obj.optim_results.params,\n",
    "    exog=X_test_baseline\n",
    ")\n",
    "threshold_baseline = find_optimal_cutoff(y_true, y_pred_prob_baseline, method=\"auc\")[0]\n",
    "y_pred_bin_baseline = (y_pred_prob_baseline > threshold_baseline).astype(int)\n",
    "\n",
    "# ===== GET FAIM PREDICTIONS =====\n",
    "excluded_vars = faim_obj.best_sen_exclusion.split(\"_\")\n",
    "selected_vars = [i for i in faim_obj.vars if i not in excluded_vars]\n",
    "selected_vars_cat = [i for i in faim_obj.vars_cat if i not in excluded_vars]\n",
    "\n",
    "X_test_faim, _, _ = faim_obj.data_process(dat_test, selected_vars, selected_vars_cat)\n",
    "y_pred_prob_faim = faim_obj.best_optim_base_obj.model_optim.model.predict(\n",
    "    params=faim_obj.best_coef,\n",
    "    exog=X_test_faim\n",
    ")\n",
    "threshold_faim = find_optimal_cutoff(y_true, y_pred_prob_faim, method=\"auc\")[0]\n",
    "y_pred_bin_faim = (y_pred_prob_faim > threshold_faim).astype(int)\n",
    "\n",
    "print(f\" Baseline threshold: {threshold_baseline:.4f}\")\n",
    "print(f\" FAIM threshold: {threshold_faim:.4f}\")\n",
    "\n",
    "# ===== CONVERT TO NUMPY ARRAYS (FIX INDEX ISSUE) =====\n",
    "y_true_arr = y_true.values if hasattr(y_true, 'values') else np.array(y_true)\n",
    "y_pred_bin_baseline_arr = y_pred_bin_baseline.values if hasattr(y_pred_bin_baseline, 'values') else np.array(y_pred_bin_baseline)\n",
    "y_pred_bin_faim_arr = y_pred_bin_faim.values if hasattr(y_pred_bin_faim, 'values') else np.array(y_pred_bin_faim)\n",
    "\n",
    "# ===== PREPARE DATA =====\n",
    "dat_test_copy = dat_test.copy().reset_index(drop=True)\n",
    "dat_test_copy[\"Sex_Race\"] = dat_test_copy[\"sex\"].astype(str) + \"_\" + dat_test_copy[\"race\"].astype(str)\n",
    "\n",
    "def compute_gaps(y_true_arr, y_pred_bin_arr, sensitive_var, var_name):\n",
    "    \"\"\"\n",
    "    Compute min/max TPR gap for a sensitive variable\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    groups = dat_test_copy[sensitive_var].unique()\n",
    "    tprs = []\n",
    "\n",
    "    for group in groups:\n",
    "        mask = (dat_test_copy[sensitive_var] == group).values  # Convert to numpy array\n",
    "        y_true_group = y_true_arr[mask]\n",
    "        y_pred_group = y_pred_bin_arr[mask]\n",
    "\n",
    "        if len(y_true_group) == 0 or y_true_group.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Compute TPR\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_group, y_pred_group, labels=[0,1]).ravel()\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "\n",
    "        if not np.isnan(tpr):\n",
    "            tprs.append({\n",
    "                'group': group,\n",
    "                'tpr': tpr,\n",
    "                'n': len(y_true_group),\n",
    "                'n_pos': y_true_group.sum()\n",
    "            })\n",
    "\n",
    "    if len(tprs) == 0:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    tprs_df = pd.DataFrame(tprs)\n",
    "\n",
    "    min_tpr = tprs_df['tpr'].min()\n",
    "    max_tpr = tprs_df['tpr'].max()\n",
    "    gap = max_tpr - min_tpr\n",
    "\n",
    "    min_group = tprs_df.loc[tprs_df['tpr'].idxmin(), 'group']\n",
    "    max_group = tprs_df.loc[tprs_df['tpr'].idxmax(), 'group']\n",
    "\n",
    "    return gap, min_tpr, max_tpr, min_group, max_group\n",
    "\n",
    "# ===== COMPUTE GAPS FOR EACH ATTRIBUTE =====\n",
    "results = []\n",
    "\n",
    "# SEX\n",
    "gap_base_sex, min_base_sex, max_base_sex, min_group_sex_base, max_group_sex_base = compute_gaps(\n",
    "    y_true_arr, y_pred_bin_baseline_arr, 'sex', 'Sex'\n",
    ")\n",
    "gap_faim_sex, min_faim_sex, max_faim_sex, min_group_sex_faim, max_group_sex_faim = compute_gaps(\n",
    "    y_true_arr, y_pred_bin_faim_arr, 'sex', 'Sex'\n",
    ")\n",
    "\n",
    "if gap_base_sex is not None and gap_faim_sex is not None:\n",
    "    results.append({\n",
    "        'Sensitive Attribute': 'Sex',\n",
    "        'Baseline Gap': gap_base_sex,\n",
    "        'Baseline Min Group': min_group_sex_base,\n",
    "        'Baseline Min TPR': min_base_sex,\n",
    "        'Baseline Max Group': max_group_sex_base,\n",
    "        'Baseline Max TPR': max_base_sex,\n",
    "        'FAIM Gap': gap_faim_sex,\n",
    "        'FAIM Min Group': min_group_sex_faim,\n",
    "        'FAIM Min TPR': min_faim_sex,\n",
    "        'FAIM Max Group': max_group_sex_faim,\n",
    "        'FAIM Max TPR': max_faim_sex,\n",
    "        'Gap Reduction': gap_base_sex - gap_faim_sex,\n",
    "        'Gap Reduction %': (gap_base_sex - gap_faim_sex) / gap_base_sex * 100\n",
    "    })\n",
    "\n",
    "# RACE\n",
    "gap_base_race, min_base_race, max_base_race, min_group_race_base, max_group_race_base = compute_gaps(\n",
    "    y_true_arr, y_pred_bin_baseline_arr, 'race', 'Race'\n",
    ")\n",
    "gap_faim_race, min_faim_race, max_faim_race, min_group_race_faim, max_group_race_faim = compute_gaps(\n",
    "    y_true_arr, y_pred_bin_faim_arr, 'race', 'Race'\n",
    ")\n",
    "\n",
    "if gap_base_race is not None and gap_faim_race is not None:\n",
    "    results.append({\n",
    "        'Sensitive Attribute': 'Race',\n",
    "        'Baseline Gap': gap_base_race,\n",
    "        'Baseline Min Group': min_group_race_base,\n",
    "        'Baseline Min TPR': min_base_race,\n",
    "        'Baseline Max Group': max_group_race_base,\n",
    "        'Baseline Max TPR': max_base_race,\n",
    "        'FAIM Gap': gap_faim_race,\n",
    "        'FAIM Min Group': min_group_race_faim,\n",
    "        'FAIM Min TPR': min_faim_race,\n",
    "        'FAIM Max Group': max_group_race_faim,\n",
    "        'FAIM Max TPR': max_faim_race,\n",
    "        'Gap Reduction': gap_base_race - gap_faim_race,\n",
    "        'Gap Reduction %': (gap_base_race - gap_faim_race) / gap_base_race * 100\n",
    "    })\n",
    "\n",
    "# SEX  RACE INTERSECTIONS\n",
    "gap_base_inter, min_base_inter, max_base_inter, min_group_inter_base, max_group_inter_base = compute_gaps(\n",
    "    y_true_arr, y_pred_bin_baseline_arr, 'Sex_Race', 'Sex  Race'\n",
    ")\n",
    "gap_faim_inter, min_faim_inter, max_faim_inter, min_group_inter_faim, max_group_inter_faim = compute_gaps(\n",
    "    y_true_arr, y_pred_bin_faim_arr, 'Sex_Race', 'Sex  Race'\n",
    ")\n",
    "\n",
    "if gap_base_inter is not None and gap_faim_inter is not None:\n",
    "    results.append({\n",
    "        'Sensitive Attribute': 'Sex  Race',\n",
    "        'Baseline Gap': gap_base_inter,\n",
    "        'Baseline Min Group': min_group_inter_base,\n",
    "        'Baseline Min TPR': min_base_inter,\n",
    "        'Baseline Max Group': max_group_inter_base,\n",
    "        'Baseline Max TPR': max_base_inter,\n",
    "        'FAIM Gap': gap_faim_inter,\n",
    "        'FAIM Min Group': min_group_inter_faim,\n",
    "        'FAIM Min TPR': min_faim_inter,\n",
    "        'FAIM Max Group': max_group_inter_faim,\n",
    "        'FAIM Max TPR': max_faim_inter,\n",
    "        'Gap Reduction': gap_base_inter - gap_faim_inter,\n",
    "        'Gap Reduction %': (gap_base_inter - gap_faim_inter) / gap_base_inter * 100\n",
    "    })\n",
    "\n",
    "# ===== REST OF YOUR CODE (unchanged) =====\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "summary_table = results_df[[\n",
    "    'Sensitive Attribute',\n",
    "    'Baseline Gap',\n",
    "    'FAIM Gap',\n",
    "    'Gap Reduction',\n",
    "    'Gap Reduction %'\n",
    "]].copy()\n",
    "\n",
    "summary_table = summary_table.round({\n",
    "    'Baseline Gap': 4,\n",
    "    'FAIM Gap': 4,\n",
    "    'Gap Reduction': 4,\n",
    "    'Gap Reduction %': 1\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: TPR GAP REDUCTION\")\n",
    "print(\"=\"*80)\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# ... rest of your visualization and output code ...\n",
    "\n",
    "# ===== DETAILED TABLE =====\n",
    "detailed_table = results_df[[\n",
    "    'Sensitive Attribute',\n",
    "    'Baseline Min Group',\n",
    "    'Baseline Min TPR',\n",
    "    'Baseline Max Group',\n",
    "    'Baseline Max TPR',\n",
    "    'Baseline Gap',\n",
    "    'FAIM Min Group',\n",
    "    'FAIM Min TPR',\n",
    "    'FAIM Max Group',\n",
    "    'FAIM Max TPR',\n",
    "    'FAIM Gap',\n",
    "    'Gap Reduction',\n",
    "    'Gap Reduction %'\n",
    "]].copy()\n",
    "\n",
    "detailed_table = detailed_table.round({\n",
    "    'Baseline Min TPR': 4,\n",
    "    'Baseline Max TPR': 4,\n",
    "    'Baseline Gap': 4,\n",
    "    'FAIM Min TPR': 4,\n",
    "    'FAIM Max TPR': 4,\n",
    "    'FAIM Gap': 4,\n",
    "    'Gap Reduction': 4,\n",
    "    'Gap Reduction %': 1\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED: MIN/MAX GROUPS AND TPR VALUES\")\n",
    "print(\"=\"*80)\n",
    "print(detailed_table.to_string(index=False))\n",
    "\n",
    "# ===== VISUALIZATION =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Gap comparison\n",
    "attributes = results_df['Sensitive Attribute'].values\n",
    "baseline_gaps = results_df['Baseline Gap'].values\n",
    "faim_gaps = results_df['FAIM Gap'].values\n",
    "\n",
    "x = np.arange(len(attributes))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.barh(x - width/2, baseline_gaps, width, label='Baseline', color='darkorange', alpha=0.7)\n",
    "bars2 = ax1.barh(x + width/2, faim_gaps, width, label='FAIM', color='steelblue', alpha=0.7)\n",
    "\n",
    "ax1.set_yticks(x)\n",
    "ax1.set_yticklabels(attributes)\n",
    "ax1.set_frame_on(False)\n",
    "ax1.set_xlabel('TPR Gap (Max - Min)', fontsize=12)\n",
    "ax1.set_title('Fairness Gaps: Baseline vs FAIM', fontsize=14, weight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add gap values on bars\n",
    "for i, (base, faim) in enumerate(zip(baseline_gaps, faim_gaps)):\n",
    "    ax1.text(base, i - width/2, f'{base:.3f}', va='center', ha='left', fontsize=9)\n",
    "    ax1.text(faim, i + width/2, f'{faim:.3f}', va='center', ha='left', fontsize=9)\n",
    "\n",
    "# Plot 2: Gap reduction percentage\n",
    "reductions = results_df['Gap Reduction %'].values\n",
    "colors = ['#E74C3C' if r > 0 else '#2ECC71' for r in reductions]\n",
    "\n",
    "bars = ax2.barh(x, reductions, color=colors, alpha=0.7)\n",
    "ax2.set_yticks(x)\n",
    "ax2.set_frame_on(False)\n",
    "ax2.set_yticklabels(attributes)\n",
    "ax2.set_xlabel('Gap Reduction (%)', fontsize=12)\n",
    "ax2.set_title('Percentage Gap Reduction', fontsize=14, weight='bold')\n",
    "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add percentage values\n",
    "for i, r in enumerate(reductions):\n",
    "    ha = 'left' if r > 0 else 'right'\n",
    "    ax2.text(r, i, f'{r:+.1f}%', va='center', ha=ha, fontsize=10, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fairness_gap_reduction.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Figure saved as 'fairness_gap_reduction.png'\")\n",
    "\n",
    "# ===== SAVE TO CSV =====\n",
    "detailed_table.to_csv('fairness_gap_reduction_detailed.csv', index=False)\n",
    "summary_table.to_csv('fairness_gap_reduction_summary.csv', index=False)\n",
    "\n",
    "print(\" Tables saved to CSV files\")\n",
    "\n",
    "# ===== KEY FINDINGS =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    attr = row['Sensitive Attribute']\n",
    "    reduction = row['Gap Reduction']\n",
    "    pct = row['Gap Reduction %']\n",
    "\n",
    "    if reduction > 0:\n",
    "        print(f\"\\n {attr}:\")\n",
    "        print(f\"  Gap reduced from {row['Baseline Gap']:.4f} to {row['FAIM Gap']:.4f}\")\n",
    "        print(f\"  Reduction: {reduction:.4f} ({pct:.1f}%)\")\n",
    "        print(f\"  Worst group improved: {row['Baseline Min Group']} (TPR: {row['Baseline Min TPR']:.3f}  {row['FAIM Min TPR']:.3f})\")\n",
    "    elif reduction < 0:\n",
    "        print(f\"\\n {attr}:\")\n",
    "        print(f\"  Gap INCREASED from {row['Baseline Gap']:.4f} to {row['FAIM Gap']:.4f}\")\n",
    "        print(f\"  Change: {reduction:.4f} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n {attr}: No change in gap\")\n",
    "\n",
    "# Overall assessment\n",
    "avg_reduction = results_df['Gap Reduction %'].mean()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OVERALL: Average gap reduction = {avg_reduction:.1f}%\")\n",
    "if avg_reduction > 10:\n",
    "    print(\" FAIM successfully reduced fairness gaps across sensitive attributes\")\n",
    "elif avg_reduction > 0:\n",
    "    print(\" FAIM provided modest improvement in fairness\")\n",
    "else:\n",
    "    print(\" FAIM did not improve fairness - consider adjusting parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "QLFku8swfRS-",
    "outputId": "2b5d85cb-1055-4f9a-99a9-afc8277e26e7"
   },
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q3sqb3chB6j"
   },
   "source": [
    "#Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt-NZ0Zygfv3"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CORRECTED SIGNIFICANCE TESTS\n",
    "# ==========================================\n",
    "\n",
    "\"\"\"\n",
    "FIXES:\n",
    "1. Use FIXED threshold (not recalculating in each bootstrap)\n",
    "2. Test sex, race, and intersections SEPARATELY\n",
    "3. Match the exact gaps from disparity_table\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# FIXED BOOTSTRAP WITH SINGLE THRESHOLD\n",
    "# ==========================================\n",
    "\n",
    "def bootstrap_gap_fixed_threshold(y_true, y_pred_prob, sen_var, threshold,\n",
    "                                   metric='tpr', n_bootstrap=1000, alpha=0.05, seed=42):\n",
    "    \"\"\"\n",
    "    Bootstrap with FIXED threshold (matches your disparity_table calculation)\n",
    "\n",
    "    This is the correct way to match your reported gaps!\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n = len(y_true)\n",
    "    groups = sen_var.unique()\n",
    "\n",
    "    # Use FIXED threshold to get predictions\n",
    "    y_pred_bin = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "    gaps = []\n",
    "\n",
    "    # Pre-convert to numpy\n",
    "    y_true_arr = y_true.values if hasattr(y_true, 'values') else np.array(y_true)\n",
    "    y_pred_arr = y_pred_bin.values if hasattr(y_pred_bin, 'values') else np.array(y_pred_bin)\n",
    "    sen_var_arr = sen_var.values if hasattr(sen_var, 'values') else np.array(sen_var)\n",
    "\n",
    "    print(f\"Running {n_bootstrap} bootstrap iterations with fixed threshold={threshold:.4f}...\")\n",
    "\n",
    "    for _ in tqdm(range(n_bootstrap), desc=\"Bootstrap\"):\n",
    "        # Resample\n",
    "        indices = np.random.choice(n, size=n, replace=True)\n",
    "\n",
    "        y_true_boot = y_true_arr[indices]\n",
    "        y_pred_boot = y_pred_arr[indices]\n",
    "        sen_var_boot = sen_var_arr[indices]\n",
    "\n",
    "        # Calculate metric for each group\n",
    "        group_metrics = []\n",
    "\n",
    "        for group in groups:\n",
    "            mask = sen_var_boot == group\n",
    "            if mask.sum() < 10:\n",
    "                continue\n",
    "\n",
    "            y_true_g = y_true_boot[mask]\n",
    "            y_pred_g = y_pred_boot[mask]\n",
    "\n",
    "            if metric == 'tpr':\n",
    "                n_pos = (y_true_g == 1).sum()\n",
    "                if n_pos == 0:\n",
    "                    continue\n",
    "                tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "                val = tp / n_pos\n",
    "            elif metric == 'fpr':\n",
    "                n_neg = (y_true_g == 0).sum()\n",
    "                if n_neg == 0:\n",
    "                    continue\n",
    "                fp = ((y_true_g == 0) & (y_pred_g == 1)).sum()\n",
    "                val = fp / n_neg\n",
    "            elif metric == 'ber':\n",
    "                n_pos = (y_true_g == 1).sum()\n",
    "                n_neg = (y_true_g == 0).sum()\n",
    "                if n_pos == 0 or n_neg == 0:\n",
    "                    continue\n",
    "                tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "                fp = ((y_true_g == 0) & (y_pred_g == 1)).sum()\n",
    "                tpr = tp / n_pos\n",
    "                fpr = fp / n_neg\n",
    "                val = 0.5 * (fpr + (1 - tpr))\n",
    "\n",
    "            group_metrics.append(val)\n",
    "\n",
    "        if len(group_metrics) >= 2:\n",
    "            gaps.append(max(group_metrics) - min(group_metrics))\n",
    "\n",
    "    gaps = np.array(gaps)\n",
    "\n",
    "    # Calculate CI\n",
    "    ci_lower = np.percentile(gaps, alpha/2 * 100)\n",
    "    ci_upper = np.percentile(gaps, (1 - alpha/2) * 100)\n",
    "    gap_mean = np.mean(gaps)\n",
    "\n",
    "    # Also calculate the OBSERVED gap (should match your table!)\n",
    "    y_pred_bin_full = (y_pred_prob > threshold).astype(int)\n",
    "    observed_metrics = []\n",
    "    for group in groups:\n",
    "        mask = sen_var == group\n",
    "        if mask.sum() < 10:\n",
    "            continue\n",
    "\n",
    "        y_true_g = y_true[mask]\n",
    "        y_pred_g = y_pred_bin_full[mask]\n",
    "\n",
    "        if metric == 'tpr':\n",
    "            n_pos = (y_true_g == 1).sum()\n",
    "            if n_pos == 0:\n",
    "                continue\n",
    "            tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "            val = tp / n_pos\n",
    "        elif metric == 'ber':\n",
    "            n_pos = (y_true_g == 1).sum()\n",
    "            n_neg = (y_true_g == 0).sum()\n",
    "            if n_pos == 0 or n_neg == 0:\n",
    "                continue\n",
    "            tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "            fp = ((y_true_g == 0) & (y_pred_g == 1)).sum()\n",
    "            tpr = tp / n_pos\n",
    "            fpr = fp / n_neg\n",
    "            val = 0.5 * (fpr + (1 - tpr))\n",
    "\n",
    "        observed_metrics.append(val)\n",
    "\n",
    "    observed_gap = max(observed_metrics) - min(observed_metrics)\n",
    "\n",
    "    return {\n",
    "        'gap': gap_mean,\n",
    "        'observed_gap': observed_gap,  # This should match your table!\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'bootstrap_dist': gaps\n",
    "    }\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# COMPREHENSIVE SIGNIFICANCE TESTS\n",
    "# ==========================================\n",
    "\n",
    "def comprehensive_significance_test(y_true,\n",
    "                                    y_pred_prob_baseline, threshold_baseline,\n",
    "                                    y_pred_prob_faim, threshold_faim,\n",
    "                                    dat_test,\n",
    "                                    metric='tpr',\n",
    "                                    n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Test significance for SEX, RACE, and INTERSECTIONS separately\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Reset indices\n",
    "    y_true = y_true.reset_index(drop=True)\n",
    "    dat_test_reset = dat_test.reset_index(drop=True)\n",
    "    y_pred_prob_baseline = pd.Series(y_pred_prob_baseline).reset_index(drop=True)\n",
    "    y_pred_prob_faim = pd.Series(y_pred_prob_faim).reset_index(drop=True)\n",
    "\n",
    "    # ===== TEST 1: SEX =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 1: SEX\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sen_sex = dat_test_reset[\"sex\"]\n",
    "\n",
    "    baseline_sex = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_baseline, sen_sex, threshold_baseline,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=42\n",
    "    )\n",
    "\n",
    "    faim_sex = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_faim, sen_sex, threshold_faim,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=43\n",
    "    )\n",
    "\n",
    "    # Calculate p-value\n",
    "    diff_sex = baseline_sex['bootstrap_dist'] - faim_sex['bootstrap_dist']\n",
    "    p_value_sex = np.mean(diff_sex <= 0)\n",
    "\n",
    "    print(f\"\\nBaseline {metric.upper()} gap (sex): {baseline_sex['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {baseline_sex['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{baseline_sex['ci_lower']:.4f}, {baseline_sex['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nFAIM {metric.upper()} gap (sex): {faim_sex['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {faim_sex['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{faim_sex['ci_lower']:.4f}, {faim_sex['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nGap reduction: {baseline_sex['observed_gap'] - faim_sex['observed_gap']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {p_value_sex:.4f}\")\n",
    "\n",
    "    if p_value_sex < 0.05:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['sex'] = {\n",
    "        'baseline_gap': baseline_sex['observed_gap'],\n",
    "        'faim_gap': faim_sex['observed_gap'],\n",
    "        'reduction': baseline_sex['observed_gap'] - faim_sex['observed_gap'],\n",
    "        'p_value': p_value_sex,\n",
    "        'significant': p_value_sex < 0.05\n",
    "    }\n",
    "\n",
    "    # ===== TEST 2: RACE =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 2: RACE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sen_race = dat_test_reset[\"race\"]\n",
    "\n",
    "    baseline_race = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_baseline, sen_race, threshold_baseline,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=44\n",
    "    )\n",
    "\n",
    "    faim_race = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_faim, sen_race, threshold_faim,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=45\n",
    "    )\n",
    "\n",
    "    diff_race = baseline_race['bootstrap_dist'] - faim_race['bootstrap_dist']\n",
    "    p_value_race = np.mean(diff_race <= 0)\n",
    "\n",
    "    print(f\"\\nBaseline {metric.upper()} gap (race): {baseline_race['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {baseline_race['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{baseline_race['ci_lower']:.4f}, {baseline_race['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nFAIM {metric.upper()} gap (race): {faim_race['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {faim_race['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{faim_race['ci_lower']:.4f}, {faim_race['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nGap reduction: {baseline_race['observed_gap'] - faim_race['observed_gap']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {p_value_race:.4f}\")\n",
    "\n",
    "    if p_value_race < 0.05:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['race'] = {\n",
    "        'baseline_gap': baseline_race['observed_gap'],\n",
    "        'faim_gap': faim_race['observed_gap'],\n",
    "        'reduction': baseline_race['observed_gap'] - faim_race['observed_gap'],\n",
    "        'p_value': p_value_race,\n",
    "        'significant': p_value_race < 0.05\n",
    "    }\n",
    "\n",
    "    # ===== TEST 3: INTERSECTIONS =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 3: INTERSECTIONS (SEX  RACE)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sen_intersect = dat_test_reset[\"sex\"].astype(str) + \"_\" + dat_test_reset[\"race\"].astype(str)\n",
    "    sen_intersect = pd.Series(sen_intersect).reset_index(drop=True)\n",
    "\n",
    "    baseline_intersect = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_baseline, sen_intersect, threshold_baseline,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=46\n",
    "    )\n",
    "\n",
    "    faim_intersect = bootstrap_gap_fixed_threshold(\n",
    "        y_true, y_pred_prob_faim, sen_intersect, threshold_faim,\n",
    "        metric=metric, n_bootstrap=n_bootstrap, seed=47\n",
    "    )\n",
    "\n",
    "    diff_intersect = baseline_intersect['bootstrap_dist'] - faim_intersect['bootstrap_dist']\n",
    "    p_value_intersect = np.mean(diff_intersect <= 0)\n",
    "\n",
    "    print(f\"\\nBaseline {metric.upper()} gap (intersections): {baseline_intersect['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {baseline_intersect['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{baseline_intersect['ci_lower']:.4f}, {baseline_intersect['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nFAIM {metric.upper()} gap (intersections): {faim_intersect['observed_gap']:.4f}\")\n",
    "    print(f\"  Bootstrap mean: {faim_intersect['gap']:.4f}\")\n",
    "    print(f\"  95% CI: [{faim_intersect['ci_lower']:.4f}, {faim_intersect['ci_upper']:.4f}]\")\n",
    "\n",
    "    print(f\"\\nGap reduction: {baseline_intersect['observed_gap'] - faim_intersect['observed_gap']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {p_value_intersect:.4f}\")\n",
    "\n",
    "    if p_value_intersect < 0.05:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['intersections'] = {\n",
    "        'baseline_gap': baseline_intersect['observed_gap'],\n",
    "        'faim_gap': faim_intersect['observed_gap'],\n",
    "        'reduction': baseline_intersect['observed_gap'] - faim_intersect['observed_gap'],\n",
    "        'p_value': p_value_intersect,\n",
    "        'significant': p_value_intersect < 0.05\n",
    "    }\n",
    "\n",
    "    # ===== SUMMARY =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY OF SIGNIFICANCE TESTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Grouping': ['Sex', 'Race', 'Sex  Race'],\n",
    "        'Baseline Gap': [results['sex']['baseline_gap'],\n",
    "                        results['race']['baseline_gap'],\n",
    "                        results['intersections']['baseline_gap']],\n",
    "        'FAIM Gap': [results['sex']['faim_gap'],\n",
    "                    results['race']['faim_gap'],\n",
    "                    results['intersections']['faim_gap']],\n",
    "        'Reduction': [results['sex']['reduction'],\n",
    "                     results['race']['reduction'],\n",
    "                     results['intersections']['reduction']],\n",
    "        'P-value': [results['sex']['p_value'],\n",
    "                   results['race']['p_value'],\n",
    "                   results['intersections']['p_value']],\n",
    "        'Significant': [results['sex']['significant'],\n",
    "                       results['race']['significant'],\n",
    "                       results['intersections']['significant']]\n",
    "    })\n",
    "\n",
    "    print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "    n_significant = summary_df['Significant'].sum()\n",
    "    print(f\"\\n{n_significant}/3 tests show significant improvement\")\n",
    "\n",
    "    return results, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWi-Z64Wgl9-",
    "outputId": "513703b6-cfbe-48ee-bf6f-a2f32f359ff8"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# HOW TO USE\n",
    "# ==========================================\n",
    "\n",
    "# 1. Get predictions with FIXED thresholds\n",
    "y_true = dat_test[\"imv\"].reset_index(drop=True)\n",
    "\n",
    "# Baseline\n",
    "X_baseline, _, _ = fairbase.data_process(dat_test)\n",
    "y_pred_prob_baseline = faim_obj.optim_model.predict(\n",
    "    params=faim_obj.optim_results.params,\n",
    "    exog=X_baseline\n",
    ")\n",
    "threshold_baseline = find_optimal_cutoff(y_true, y_pred_prob_baseline, method=\"auc\")[0]\n",
    "\n",
    "# FAIM\n",
    "excluded = faim_obj.best_sen_exclusion.split(\"_\")\n",
    "sel_vars = [v for v in faim_obj.vars if v not in excluded]\n",
    "sel_vars_cat = [v for v in faim_obj.vars_cat if v not in excluded]\n",
    "X_faim, _, _ = faim_obj.data_process(dat_test, sel_vars, sel_vars_cat)\n",
    "y_pred_prob_faim = faim_obj.best_optim_base_obj.model_optim.model.predict(\n",
    "    params=faim_obj.best_coef,\n",
    "    exog=X_faim\n",
    ")\n",
    "threshold_faim = find_optimal_cutoff(y_true, y_pred_prob_faim, method=\"auc\")[0]\n",
    "\n",
    "print(f\"Baseline threshold: {threshold_baseline:.4f}\")\n",
    "print(f\"FAIM threshold: {threshold_faim:.4f}\")\n",
    "\n",
    "# 2. Run comprehensive tests\n",
    "results, summary = comprehensive_significance_test(\n",
    "    y_true,\n",
    "    y_pred_prob_baseline, threshold_baseline,\n",
    "    y_pred_prob_faim, threshold_faim,\n",
    "    dat_test,\n",
    "    metric='tpr',  # or 'ber'\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "# 3. Check results\n",
    "print(\"\\\\nYour disparity table shows:\")\n",
    "print(\"  Baseline TPR gap (intersections): 0.2118\")\n",
    "print(\"  FAIM TPR gap (intersections): 0.1902\")\n",
    "\n",
    "print(\"\\\\nOur calculated gaps:\")\n",
    "print(f\"  Baseline: {results['intersections']['baseline_gap']:.4f}\")\n",
    "print(f\"  FAIM: {results['intersections']['faim_gap']:.4f}\")\n",
    "\n",
    "# They should MATCH now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j9A0fX4ziNHZ",
    "outputId": "0d79212d-80c8-4d5d-bede-9422376e9050"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PERMUTATION TESTING + VISUALIZATIONS\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# PERMUTATION TEST FUNCTION\n",
    "# ==========================================\n",
    "\n",
    "def permutation_test_fairness(y_true,\n",
    "                              y_pred_prob_baseline, threshold_baseline,\n",
    "                              y_pred_prob_faim, threshold_faim,\n",
    "                              sen_var,\n",
    "                              metric='tpr',\n",
    "                              n_permutations=1000,\n",
    "                              seed=42):\n",
    "    \"\"\"\n",
    "    Permutation test to compare fairness gaps between baseline and FAIM\n",
    "\n",
    "    H0: The two models have the same fairness gap\n",
    "    H1: FAIM has smaller gap than baseline\n",
    "\n",
    "    Returns p-value and permutation distribution\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    print(f\"Running {n_permutations} permutations with fixed thresholds...\")\n",
    "\n",
    "    # Convert to binary predictions with FIXED thresholds\n",
    "    y_pred_baseline = (y_pred_prob_baseline > threshold_baseline).astype(int)\n",
    "    y_pred_faim = (y_pred_prob_faim > threshold_faim).astype(int)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_true_arr = y_true.values if hasattr(y_true, 'values') else np.array(y_true)\n",
    "    y_pred_baseline_arr = y_pred_baseline.values if hasattr(y_pred_baseline, 'values') else np.array(y_pred_baseline)\n",
    "    y_pred_faim_arr = y_pred_faim.values if hasattr(y_pred_faim, 'values') else np.array(y_pred_faim)\n",
    "    sen_var_arr = sen_var.values if hasattr(sen_var, 'values') else np.array(sen_var)\n",
    "\n",
    "    # Function to calculate gap\n",
    "    def calculate_gap(y_true, y_pred, sen_var, metric):\n",
    "        groups = np.unique(sen_var)\n",
    "        group_metrics = []\n",
    "\n",
    "        for group in groups:\n",
    "            mask = sen_var == group\n",
    "            if mask.sum() < 10:\n",
    "                continue\n",
    "\n",
    "            y_true_g = y_true[mask]\n",
    "            y_pred_g = y_pred[mask]\n",
    "\n",
    "            if metric == 'tpr':\n",
    "                n_pos = (y_true_g == 1).sum()\n",
    "                if n_pos == 0:\n",
    "                    continue\n",
    "                tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "                val = tp / n_pos\n",
    "            elif metric == 'ber':\n",
    "                n_pos = (y_true_g == 1).sum()\n",
    "                n_neg = (y_true_g == 0).sum()\n",
    "                if n_pos == 0 or n_neg == 0:\n",
    "                    continue\n",
    "                tp = ((y_true_g == 1) & (y_pred_g == 1)).sum()\n",
    "                fp = ((y_true_g == 0) & (y_pred_g == 1)).sum()\n",
    "                tpr = tp / n_pos\n",
    "                fpr = fp / n_neg\n",
    "                val = 0.5 * (fpr + (1 - tpr))\n",
    "\n",
    "            group_metrics.append(val)\n",
    "\n",
    "        return max(group_metrics) - min(group_metrics) if len(group_metrics) >= 2 else 0\n",
    "\n",
    "    # Calculate observed difference\n",
    "    gap_baseline = calculate_gap(y_true_arr, y_pred_baseline_arr, sen_var_arr, metric)\n",
    "    gap_faim = calculate_gap(y_true_arr, y_pred_faim_arr, sen_var_arr, metric)\n",
    "    observed_diff = gap_baseline - gap_faim\n",
    "\n",
    "    # Permutation test\n",
    "    permuted_diffs = []\n",
    "\n",
    "    for _ in tqdm(range(n_permutations), desc=\"Permutations\"):\n",
    "        # Randomly swap model labels for each observation\n",
    "        swap = np.random.rand(len(y_true_arr)) > 0.5\n",
    "\n",
    "        y_pred_perm1 = y_pred_baseline_arr.copy()\n",
    "        y_pred_perm2 = y_pred_faim_arr.copy()\n",
    "\n",
    "        # Swap predictions where swap is True\n",
    "        y_pred_perm1[swap] = y_pred_faim_arr[swap]\n",
    "        y_pred_perm2[swap] = y_pred_baseline_arr[swap]\n",
    "\n",
    "        # Calculate gaps for permuted predictions\n",
    "        gap1 = calculate_gap(y_true_arr, y_pred_perm1, sen_var_arr, metric)\n",
    "        gap2 = calculate_gap(y_true_arr, y_pred_perm2, sen_var_arr, metric)\n",
    "\n",
    "        permuted_diffs.append(gap1 - gap2)\n",
    "\n",
    "    permuted_diffs = np.array(permuted_diffs)\n",
    "\n",
    "    # Calculate p-value (one-sided: H1: baseline > FAIM)\n",
    "    p_value_one_sided = np.mean(permuted_diffs >= observed_diff)\n",
    "\n",
    "    # Two-sided p-value\n",
    "    p_value_two_sided = np.mean(np.abs(permuted_diffs) >= np.abs(observed_diff))\n",
    "\n",
    "    return {\n",
    "        'observed_diff': observed_diff,\n",
    "        'gap_baseline': gap_baseline,\n",
    "        'gap_faim': gap_faim,\n",
    "        'p_value_one_sided': p_value_one_sided,\n",
    "        'p_value_two_sided': p_value_two_sided,\n",
    "        'permuted_dist': permuted_diffs,\n",
    "        'significant': p_value_one_sided < 0.05\n",
    "    }\n",
    "\n",
    "\n",
    "def comprehensive_permutation_test(y_true,\n",
    "                                   y_pred_prob_baseline, threshold_baseline,\n",
    "                                   y_pred_prob_faim, threshold_faim,\n",
    "                                   dat_test,\n",
    "                                   metric='tpr',\n",
    "                                   n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Run permutation tests for sex, race, and intersections\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Reset indices\n",
    "    y_true = y_true.reset_index(drop=True)\n",
    "    dat_test_reset = dat_test.reset_index(drop=True)\n",
    "    y_pred_prob_baseline = pd.Series(y_pred_prob_baseline).reset_index(drop=True)\n",
    "    y_pred_prob_faim = pd.Series(y_pred_prob_faim).reset_index(drop=True)\n",
    "\n",
    "    # Test 1: Sex\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERMUTATION TEST 1: SEX\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    perm_sex = permutation_test_fairness(\n",
    "        y_true, y_pred_prob_baseline, threshold_baseline,\n",
    "        y_pred_prob_faim, threshold_faim,\n",
    "        dat_test_reset[\"sex\"],\n",
    "        metric=metric, n_permutations=n_permutations, seed=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\nObserved baseline gap: {perm_sex['gap_baseline']:.4f}\")\n",
    "    print(f\"Observed FAIM gap: {perm_sex['gap_faim']:.4f}\")\n",
    "    print(f\"Observed difference: {perm_sex['observed_diff']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {perm_sex['p_value_one_sided']:.4f}\")\n",
    "    print(f\"P-value (two-sided): {perm_sex['p_value_two_sided']:.4f}\")\n",
    "\n",
    "    if perm_sex['significant']:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['sex'] = perm_sex\n",
    "\n",
    "    # Test 2: Race\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERMUTATION TEST 2: RACE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    perm_race = permutation_test_fairness(\n",
    "        y_true, y_pred_prob_baseline, threshold_baseline,\n",
    "        y_pred_prob_faim, threshold_faim,\n",
    "        dat_test_reset[\"race\"],\n",
    "        metric=metric, n_permutations=n_permutations, seed=43\n",
    "    )\n",
    "\n",
    "    print(f\"\\nObserved baseline gap: {perm_race['gap_baseline']:.4f}\")\n",
    "    print(f\"Observed FAIM gap: {perm_race['gap_faim']:.4f}\")\n",
    "    print(f\"Observed difference: {perm_race['observed_diff']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {perm_race['p_value_one_sided']:.4f}\")\n",
    "    print(f\"P-value (two-sided): {perm_race['p_value_two_sided']:.4f}\")\n",
    "\n",
    "    if perm_race['significant']:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['race'] = perm_race\n",
    "\n",
    "    # Test 3: Intersections\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERMUTATION TEST 3: INTERSECTIONS (SEX  RACE)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sen_intersect = dat_test_reset[\"sex\"].astype(str) + \"_\" + dat_test_reset[\"race\"].astype(str)\n",
    "\n",
    "    perm_intersect = permutation_test_fairness(\n",
    "        y_true, y_pred_prob_baseline, threshold_baseline,\n",
    "        y_pred_prob_faim, threshold_faim,\n",
    "        sen_intersect,\n",
    "        metric=metric, n_permutations=n_permutations, seed=44\n",
    "    )\n",
    "\n",
    "    print(f\"\\nObserved baseline gap: {perm_intersect['gap_baseline']:.4f}\")\n",
    "    print(f\"Observed FAIM gap: {perm_intersect['gap_faim']:.4f}\")\n",
    "    print(f\"Observed difference: {perm_intersect['observed_diff']:.4f}\")\n",
    "    print(f\"P-value (one-sided): {perm_intersect['p_value_one_sided']:.4f}\")\n",
    "    print(f\"P-value (two-sided): {perm_intersect['p_value_two_sided']:.4f}\")\n",
    "\n",
    "    if perm_intersect['significant']:\n",
    "        print(\" SIGNIFICANT at =0.05\")\n",
    "    else:\n",
    "        print(\" NOT significant at =0.05\")\n",
    "\n",
    "    results['intersections'] = perm_intersect\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERMUTATION TEST SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Grouping': ['Sex', 'Race', 'Sex  Race'],\n",
    "        'Baseline Gap': [perm_sex['gap_baseline'], perm_race['gap_baseline'], perm_intersect['gap_baseline']],\n",
    "        'FAIM Gap': [perm_sex['gap_faim'], perm_race['gap_faim'], perm_intersect['gap_faim']],\n",
    "        'Difference': [perm_sex['observed_diff'], perm_race['observed_diff'], perm_intersect['observed_diff']],\n",
    "        'P-value': [perm_sex['p_value_one_sided'], perm_race['p_value_one_sided'], perm_intersect['p_value_one_sided']],\n",
    "        'Significant': [perm_sex['significant'], perm_race['significant'], perm_intersect['significant']]\n",
    "    })\n",
    "\n",
    "    print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "    n_significant = summary_df['Significant'].sum()\n",
    "    print(f\"\\n{n_significant}/3 tests show significant improvement\")\n",
    "\n",
    "    return results, summary_df\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# COMPREHENSIVE VISUALIZATIONS\n",
    "# ==========================================\n",
    "\n",
    "def create_comprehensive_visualizations(bootstrap_results, permutation_results,\n",
    "                                        metric='TPR', output_dir='output'):\n",
    "    \"\"\"\n",
    "    Create publication-quality visualizations\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_palette(\"colorblind\")\n",
    "\n",
    "    groupings = ['sex', 'race', 'intersections']\n",
    "    titles = ['Sex', 'Race', 'Sex  Race']\n",
    "\n",
    "    # ==========================================\n",
    "    # FIGURE 1: Bootstrap Distributions (3x2 grid)\n",
    "    # ==========================================\n",
    "\n",
    "    fig1, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig1.suptitle(f'Bootstrap Distributions: {metric} Gap Comparison',\n",
    "                  fontsize=22, weight='bold', y=0.995)\n",
    "\n",
    "    for i, (grouping, title) in enumerate(zip(groupings, titles)):\n",
    "        # Get bootstrap distributions from results dict\n",
    "        baseline_dist = bootstrap_results[grouping]['baseline']['bootstrap_dist']\n",
    "        faim_dist = bootstrap_results[grouping]['faim']['bootstrap_dist']\n",
    "\n",
    "        baseline_gap = bootstrap_results[grouping]['baseline']['observed_gap']\n",
    "        faim_gap = bootstrap_results[grouping]['faim']['observed_gap']\n",
    "\n",
    "        # Left: Histograms\n",
    "        ax = axes[i, 0]\n",
    "        ax.hist(baseline_dist, bins=50, alpha=0.65, label='Baseline', color='darkorange', edgecolor='black', linewidth=0.5)\n",
    "        ax.hist(faim_dist, bins=50, alpha=0.65, label='FAIM', color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "        ax.axvline(baseline_gap, color='darkorange', linestyle='--', linewidth=3, label=f'Baseline: {baseline_gap:.3f}')\n",
    "        ax.axvline(faim_gap, color='steelblue', linestyle='--', linewidth=3, label=f'FAIM: {faim_gap:.3f}')\n",
    "\n",
    "        ax.set_xlabel(f'{metric} Gap', fontsize=14, weight='bold')\n",
    "        ax.set_ylabel('Frequency', fontsize=14, weight='bold')\n",
    "        ax.set_title(f'{title}: Bootstrap Distributions', fontsize=16, weight='bold', pad=10)\n",
    "        ax.legend(fontsize=11, loc='upper right', framealpha=0.9)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.tick_params(labelsize=11)\n",
    "\n",
    "        # Right: Box plots with CIs\n",
    "        ax = axes[i, 1]\n",
    "\n",
    "        data = [baseline_dist, faim_dist]\n",
    "        bp = ax.boxplot(data, labels=['Baseline', 'FAIM'], patch_artist=True, widths=0.6)\n",
    "\n",
    "        bp['boxes'][0].set_facecolor('darkorange')\n",
    "        bp['boxes'][0].set_alpha(0.7)\n",
    "        bp['boxes'][1].set_facecolor('steelblue')\n",
    "        bp['boxes'][1].set_alpha(0.7)\n",
    "\n",
    "        # Add CI markers\n",
    "        baseline_ci_lower = bootstrap_results[grouping]['baseline']['ci_lower']\n",
    "        baseline_ci_upper = bootstrap_results[grouping]['baseline']['ci_upper']\n",
    "        faim_ci_lower = bootstrap_results[grouping]['faim']['ci_lower']\n",
    "        faim_ci_upper = bootstrap_results[grouping]['faim']['ci_upper']\n",
    "\n",
    "        ax.plot([1, 1], [baseline_ci_lower, baseline_ci_upper], 'k-', linewidth=4, label='95% CI')\n",
    "        ax.plot([2, 2], [faim_ci_lower, faim_ci_upper], 'k-', linewidth=4)\n",
    "\n",
    "        # Add significance annotation\n",
    "        p_val = bootstrap_results[grouping]['p_value']\n",
    "        reduction = baseline_gap - faim_gap\n",
    "        sig_text = f\"p={p_val:.3f}{'*' if p_val < 0.05 else ''}\\n={reduction:.3f}\"\n",
    "        ax.text(1.5, ax.get_ylim()[1]*0.95, sig_text,\n",
    "               ha='center', fontsize=13, weight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen' if p_val < 0.05 else 'lightcoral', alpha=0.8))\n",
    "\n",
    "        ax.set_ylabel(f'{metric} Gap', fontsize=14, weight='bold')\n",
    "        ax.set_title(f'{title}: Comparison with 95% CI', fontsize=16, weight='bold', pad=10)\n",
    "        ax.grid(alpha=0.3, axis='y')\n",
    "        ax.tick_params(labelsize=11)\n",
    "        ax.legend(fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/bootstrap_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_dir}/bootstrap_distributions.png\")\n",
    "\n",
    "    # ==========================================\n",
    "    # FIGURE 2: Permutation Test Distributions\n",
    "    # ==========================================\n",
    "\n",
    "    fig2, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    fig2.suptitle(f'Permutation Test: {metric} Gap Differences',\n",
    "                  fontsize=22, weight='bold', y=1.02)\n",
    "\n",
    "    for i, (grouping, title) in enumerate(zip(groupings, titles)):\n",
    "        ax = axes[i]\n",
    "\n",
    "        perm_dist = permutation_results[grouping]['permuted_dist']\n",
    "        observed_diff = permutation_results[grouping]['observed_diff']\n",
    "        p_val = permutation_results[grouping]['p_value_one_sided']\n",
    "\n",
    "        # Histogram of permuted differences\n",
    "        ax.hist(perm_dist, bins=50, color='gray', alpha=0.7, edgecolor='black', linewidth=0.5, label='Null distribution')\n",
    "\n",
    "        # Observed difference\n",
    "        ax.axvline(observed_diff, color='red', linestyle='--', linewidth=3,\n",
    "                  label=f'Observed: {observed_diff:.3f}')\n",
    "\n",
    "        # Shade significant region\n",
    "        if observed_diff > 0:\n",
    "            x_sig = perm_dist[perm_dist >= observed_diff]\n",
    "            if len(x_sig) > 0:\n",
    "                ax.axvspan(observed_diff, perm_dist.max(), alpha=0.3, color='red', label=f'p={p_val:.3f}')\n",
    "\n",
    "        ax.set_xlabel('Gap Difference (Baseline - FAIM)', fontsize=14, weight='bold')\n",
    "        ax.set_ylabel('Frequency', fontsize=14, weight='bold')\n",
    "        ax.set_title(f'{title}', fontsize=17, weight='bold', pad=10)\n",
    "        ax.legend(fontsize=12, framealpha=0.9)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.tick_params(labelsize=11)\n",
    "\n",
    "        # Add significance annotation\n",
    "        sig_marker = ' Significant' if p_val < 0.05 else ' Not significant'\n",
    "        ax.text(0.05, 0.95, f\"{sig_marker}\\np = {p_val:.3f}\",\n",
    "               transform=ax.transAxes, fontsize=13, weight='bold',\n",
    "               verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen' if p_val < 0.05 else 'lightcoral', alpha=0.9))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/permutation_tests.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_dir}/permutation_tests.png\")\n",
    "\n",
    "    # ==========================================\n",
    "    # FIGURE 3: Combined Summary (2x2 grid)\n",
    "    # ==========================================\n",
    "\n",
    "    fig3, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    fig3.suptitle(f'Significance Testing Summary: {metric} Gap',\n",
    "                  fontsize=22, weight='bold', y=0.995)\n",
    "\n",
    "    # Subplot 1: Gap comparison bar chart\n",
    "    ax = axes[0, 0]\n",
    "\n",
    "    x = np.arange(len(groupings))\n",
    "    width = 0.35\n",
    "\n",
    "    baseline_gaps = [bootstrap_results[g]['baseline']['observed_gap'] for g in groupings]\n",
    "    faim_gaps = [bootstrap_results[g]['faim']['observed_gap'] for g in groupings]\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, baseline_gaps, width, label='Baseline', color='darkorange', alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, faim_gaps, width, label='FAIM', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=12, weight='bold')\n",
    "\n",
    "    ax.set_ylabel(f'{metric} Gap', fontsize=15, weight='bold')\n",
    "    ax.set_title('Gap Comparison by Grouping', fontsize=17, weight='bold', pad=10)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(titles, fontsize=13)\n",
    "    ax.legend(fontsize=13, framealpha=0.9)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    # Subplot 2: Reduction percentages\n",
    "    ax = axes[0, 1]\n",
    "\n",
    "    reductions = [(b - f) / b * 100 for b, f in zip(baseline_gaps, faim_gaps)]\n",
    "    colors = ['green' if r > 0 else 'red' for r in reductions]\n",
    "\n",
    "    bars = ax.barh(titles, reductions, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "    for i, (bar, val) in enumerate(zip(bars, reductions)):\n",
    "        ax.text(val, i, f' {val:.1f}%', va='center', ha='left' if val > 0 else 'right',\n",
    "               fontsize=13, weight='bold')\n",
    "\n",
    "    ax.set_xlabel('Gap Reduction (%)', fontsize=15, weight='bold')\n",
    "    ax.set_title('Percentage Gap Reduction', fontsize=17, weight='bold', pad=10)\n",
    "    ax.axvline(0, color='black', linewidth=2)\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    # Subplot 3: P-values comparison\n",
    "    ax = axes[1, 0]\n",
    "\n",
    "    boot_pvals = [bootstrap_results[g]['p_value'] for g in groupings]\n",
    "    perm_pvals = [permutation_results[g]['p_value_one_sided'] for g in groupings]\n",
    "\n",
    "    x = np.arange(len(groupings))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, boot_pvals, width, label='Bootstrap', color='purple', alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, perm_pvals, width, label='Permutation', color='teal', alpha=0.7, edgecolor='black')\n",
    "\n",
    "    # Add significance line\n",
    "    ax.axhline(0.05, color='red', linestyle='--', linewidth=2.5, label='=0.05')\n",
    "\n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=11, weight='bold')\n",
    "\n",
    "    ax.set_ylabel('P-value', fontsize=15, weight='bold')\n",
    "    ax.set_title('Statistical Significance (p-values)', fontsize=17, weight='bold', pad=10)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(titles, fontsize=13)\n",
    "    ax.set_ylim(0, max(max(boot_pvals), max(perm_pvals)) * 1.3)\n",
    "    ax.legend(fontsize=12, framealpha=0.9)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    # Subplot 4: Summary table\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create summary table\n",
    "    table_data = []\n",
    "    for i, (grouping, title) in enumerate(zip(groupings, titles)):\n",
    "        boot_sig = '' if bootstrap_results[grouping]['significant'] else ''\n",
    "        perm_sig = '' if permutation_results[grouping]['significant'] else ''\n",
    "\n",
    "        row = [\n",
    "            title,\n",
    "            f\"{baseline_gaps[i]:.3f}\",\n",
    "            f\"{faim_gaps[i]:.3f}\",\n",
    "            f\"{baseline_gaps[i] - faim_gaps[i]:.3f}\",\n",
    "            f\"{boot_pvals[i]:.3f} {boot_sig}\",\n",
    "            f\"{perm_pvals[i]:.3f} {perm_sig}\"\n",
    "        ]\n",
    "        table_data.append(row)\n",
    "\n",
    "    table = ax.table(cellText=table_data,\n",
    "                    colLabels=['Grouping', 'Baseline\\nGap', 'FAIM\\nGap', 'Reduction',\n",
    "                              'Bootstrap\\np-value', 'Permutation\\np-value'],\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    bbox=[0, 0, 1, 1])\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1, 3.2)\n",
    "\n",
    "    # Color code header\n",
    "    for i in range(6):\n",
    "        table[(0, i)].set_facecolor('#4472C4')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white', fontsize=13)\n",
    "\n",
    "    # Color code significance\n",
    "    for i in range(1, 4):\n",
    "        # Bootstrap\n",
    "        if '' in table_data[i-1][4]:\n",
    "            table[(i, 4)].set_facecolor('lightgreen')\n",
    "        else:\n",
    "            table[(i, 4)].set_facecolor('lightcoral')\n",
    "\n",
    "        # Permutation\n",
    "        if '' in table_data[i-1][5]:\n",
    "            table[(i, 5)].set_facecolor('lightgreen')\n",
    "        else:\n",
    "            table[(i, 5)].set_facecolor('lightcoral')\n",
    "\n",
    "    ax.set_title('Comprehensive Summary Table', fontsize=17, weight='bold', pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/significance_summary.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\" Saved: {output_dir}/significance_summary.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VISUALIZATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nGenerated 3 figures:\")\n",
    "    print(f\"  1. {output_dir}/bootstrap_distributions.png\")\n",
    "    print(f\"  2. {output_dir}/permutation_tests.png\")\n",
    "    print(f\"  3. {output_dir}/significance_summary.png\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# RUN PERMUTATION TESTS\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING PERMUTATION TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run permutation tests (using same data as bootstrap)\n",
    "permutation_results, permutation_summary = comprehensive_permutation_test(\n",
    "    y_true,\n",
    "    y_pred_prob_baseline, threshold_baseline,\n",
    "    y_pred_prob_faim, threshold_faim,\n",
    "    dat_test,\n",
    "    metric='tpr',\n",
    "    n_permutations=1000\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# CREATE VISUALIZATIONS\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reorganize bootstrap results for visualization\n",
    "# (assuming you've already run the bootstrap tests from the previous cell)\n",
    "\n",
    "bootstrap_vis = {\n",
    "    'sex': {\n",
    "        'baseline': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_baseline, dat_test.reset_index(drop=True)[\"sex\"],\n",
    "            threshold_baseline, metric='tpr', n_bootstrap=1000, seed=42\n",
    "        ),\n",
    "        'faim': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_faim, dat_test.reset_index(drop=True)[\"sex\"],\n",
    "            threshold_faim, metric='tpr', n_bootstrap=1000, seed=43\n",
    "        ),\n",
    "        'p_value': results['sex']['p_value'],\n",
    "        'significant': results['sex']['significant']\n",
    "    },\n",
    "    'race': {\n",
    "        'baseline': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_baseline, dat_test.reset_index(drop=True)[\"race\"],\n",
    "            threshold_baseline, metric='tpr', n_bootstrap=1000, seed=44\n",
    "        ),\n",
    "        'faim': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_faim, dat_test.reset_index(drop=True)[\"race\"],\n",
    "            threshold_faim, metric='tpr', n_bootstrap=1000, seed=45\n",
    "        ),\n",
    "        'p_value': results['race']['p_value'],\n",
    "        'significant': results['race']['significant']\n",
    "    },\n",
    "    'intersections': {\n",
    "        'baseline': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_baseline,\n",
    "            (dat_test.reset_index(drop=True)[\"sex\"].astype(str) + \"_\" + dat_test.reset_index(drop=True)[\"race\"].astype(str)),\n",
    "            threshold_baseline, metric='tpr', n_bootstrap=1000, seed=46\n",
    "        ),\n",
    "        'faim': bootstrap_gap_fixed_threshold(\n",
    "            y_true, y_pred_prob_faim,\n",
    "            (dat_test.reset_index(drop=True)[\"sex\"].astype(str) + \"_\" + dat_test.reset_index(drop=True)[\"race\"].astype(str)),\n",
    "            threshold_faim, metric='tpr', n_bootstrap=1000, seed=47\n",
    "        ),\n",
    "        'p_value': results['intersections']['p_value'],\n",
    "        'significant': results['intersections']['significant']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "create_comprehensive_visualizations(\n",
    "    bootstrap_vis,\n",
    "    permutation_results,\n",
    "    metric='TPR',\n",
    "    output_dir='output'\n",
    ")\n",
    "\n",
    "print(\"\\n All significance testing complete!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TeVk96X1ofrn",
    "Ai3rD4aG7N7b",
    "5prZZ2x9pWAP",
    "JHDw9mDnovxk",
    "zmuQmuFto1c1",
    "-SX-HVJjpGib",
    "v2tWh6r0o9py",
    "bZL12cvzMEnY",
    "YESf8l3fGww0"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
